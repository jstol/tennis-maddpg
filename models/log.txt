/usr/local/anaconda3/envs/drlnd/bin/python /Users/jake/code/tennis-maddpg/main.py train
Mono path[0] = '/Users/jake/code/tennis-maddpg/Tennis.app/Contents/Resources/Data/Managed'
Mono config path = '/Users/jake/code/tennis-maddpg/Tennis.app/Contents/MonoBleedingEdge/etc'
INFO:unityagents:
'Academy' started successfully!
Unity Academy name: Academy
        Number of Brains: 1
        Number of External Brains : 1
        Lesson number : 0
        Reset Parameters :

Unity brain name: TennisBrain
        Number of Visual Observations (per agent): 0
        Vector Observation space type: continuous
        Vector Observation space size (per agent): 8
        Number of stacked Vector Observation: 3
        Vector Action space type: continuous
        Vector Action space size (per agent): 2
        Vector Action descriptions: ,
Episode 0/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 1/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 2/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 3/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 4/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 5/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 6/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 7/5000 (17 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 8/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 9/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 10/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 11/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 12/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 13/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 14/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 15/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: inf, Agent 1: inf] ; E(V) Avg: [Agent 0: -inf, Agent 1: -inf]) [Noise stdev: 0.25]
Episode 16/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0059 ; (Q Loss Avg: [Agent 0: 1.48014918, Agent 1: 1.38223882] ; E(V) Avg: [Agent 0: 0.20548230, Agent 1: 0.54559204]) [Noise stdev: 0.25]
Episode 17/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0056 ; (Q Loss Avg: [Agent 0: 0.15281480, Agent 1: 0.34027364] ; E(V) Avg: [Agent 0: 0.13482698, Agent 1: 0.56201078]) [Noise stdev: 0.25]
Episode 18/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0053 ; (Q Loss Avg: [Agent 0: 0.08760210, Agent 1: 0.24181262] ; E(V) Avg: [Agent 0: 0.16175030, Agent 1: 0.55875361]) [Noise stdev: 0.25]
Episode 19/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.05121363, Agent 1: 0.17514711] ; E(V) Avg: [Agent 0: 0.18159853, Agent 1: 0.57573176]) [Noise stdev: 0.25]
Episode 20/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0095 ; (Q Loss Avg: [Agent 0: 0.03048771, Agent 1: 0.09891790] ; E(V) Avg: [Agent 0: 0.15771485, Agent 1: 0.62306567]) [Noise stdev: 0.25]
Episode 21/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0091 ; (Q Loss Avg: [Agent 0: 0.02124928, Agent 1: 0.05152912] ; E(V) Avg: [Agent 0: 0.14270832, Agent 1: 0.71507387]) [Noise stdev: 0.25]
Episode 22/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0087 ; (Q Loss Avg: [Agent 0: 0.02040376, Agent 1: 0.03862218] ; E(V) Avg: [Agent 0: 0.13403721, Agent 1: 0.76922776]) [Noise stdev: 0.25]
Episode 23/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0083 ; (Q Loss Avg: [Agent 0: 0.02179087, Agent 1: 0.03754284] ; E(V) Avg: [Agent 0: 0.13601371, Agent 1: 0.81399534]) [Noise stdev: 0.25]
Episode 24/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0080 ; (Q Loss Avg: [Agent 0: 0.01795500, Agent 1: 0.02579734] ; E(V) Avg: [Agent 0: 0.14861332, Agent 1: 0.82194542]) [Noise stdev: 0.25]
Episode 25/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0077 ; (Q Loss Avg: [Agent 0: 0.01763760, Agent 1: 0.01810582] ; E(V) Avg: [Agent 0: 0.11372382, Agent 1: 0.87919565]) [Noise stdev: 0.25]
Episode 26/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0074 ; (Q Loss Avg: [Agent 0: 0.01669691, Agent 1: 0.01393392] ; E(V) Avg: [Agent 0: 0.11194278, Agent 1: 0.89912488]) [Noise stdev: 0.25]
Episode 27/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0071 ; (Q Loss Avg: [Agent 0: 0.01801047, Agent 1: 0.00983909] ; E(V) Avg: [Agent 0: 0.11322670, Agent 1: 0.91743766]) [Noise stdev: 0.25]
Episode 28/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0069 ; (Q Loss Avg: [Agent 0: 0.01740815, Agent 1: 0.00971965] ; E(V) Avg: [Agent 0: 0.09412810, Agent 1: 0.91964129]) [Noise stdev: 0.25]
Episode 29/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0067 ; (Q Loss Avg: [Agent 0: 0.01500680, Agent 1: 0.00995153] ; E(V) Avg: [Agent 0: 0.09974705, Agent 1: 0.92884849]) [Noise stdev: 0.25]
Episode 30/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0065 ; (Q Loss Avg: [Agent 0: 0.01464215, Agent 1: 0.00921065] ; E(V) Avg: [Agent 0: 0.07871114, Agent 1: 0.93784777]) [Noise stdev: 0.25]
Episode 31/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0063 ; (Q Loss Avg: [Agent 0: 0.01436248, Agent 1: 0.00723603] ; E(V) Avg: [Agent 0: 0.08413524, Agent 1: 0.91079814]) [Noise stdev: 0.25]
Episode 32/5000 (33 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0088 ; (Q Loss Avg: [Agent 0: 0.01176760, Agent 1: 0.00653432] ; E(V) Avg: [Agent 0: 0.07799665, Agent 1: 0.78695395]) [Noise stdev: 0.25]
Episode 33/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0085 ; (Q Loss Avg: [Agent 0: 0.01120337, Agent 1: 0.00634995] ; E(V) Avg: [Agent 0: 0.07940478, Agent 1: 0.56591501]) [Noise stdev: 0.25]
Episode 34/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0083 ; (Q Loss Avg: [Agent 0: 0.01156513, Agent 1: 0.00438224] ; E(V) Avg: [Agent 0: 0.08528464, Agent 1: 0.50663268]) [Noise stdev: 0.25]
Episode 35/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0081 ; (Q Loss Avg: [Agent 0: 0.00993984, Agent 1: 0.00519911] ; E(V) Avg: [Agent 0: 0.09114504, Agent 1: 0.47447298]) [Noise stdev: 0.25]
Episode 36/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0078 ; (Q Loss Avg: [Agent 0: 0.00904208, Agent 1: 0.00490651] ; E(V) Avg: [Agent 0: 0.08748354, Agent 1: 0.47167165]) [Noise stdev: 0.25]
Episode 37/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0076 ; (Q Loss Avg: [Agent 0: 0.00985521, Agent 1: 0.00593413] ; E(V) Avg: [Agent 0: 0.09863371, Agent 1: 0.45734064]) [Noise stdev: 0.25]
Episode 38/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0074 ; (Q Loss Avg: [Agent 0: 0.00986488, Agent 1: 0.00599877] ; E(V) Avg: [Agent 0: 0.10065682, Agent 1: 0.45466212]) [Noise stdev: 0.25]
Episode 39/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0073 ; (Q Loss Avg: [Agent 0: 0.00816376, Agent 1: 0.00564084] ; E(V) Avg: [Agent 0: 0.11264030, Agent 1: 0.44754615]) [Noise stdev: 0.25]
Episode 40/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0071 ; (Q Loss Avg: [Agent 0: 0.00778401, Agent 1: 0.00465127] ; E(V) Avg: [Agent 0: 0.11766728, Agent 1: 0.41925312]) [Noise stdev: 0.25]
Episode 41/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0069 ; (Q Loss Avg: [Agent 0: 0.00769096, Agent 1: 0.00412850] ; E(V) Avg: [Agent 0: 0.11323115, Agent 1: 0.43568108]) [Noise stdev: 0.25]
Episode 42/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0067 ; (Q Loss Avg: [Agent 0: 0.00735079, Agent 1: 0.00310816] ; E(V) Avg: [Agent 0: 0.13915554, Agent 1: 0.44575543]) [Noise stdev: 0.25]
Episode 43/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0066 ; (Q Loss Avg: [Agent 0: 0.00696955, Agent 1: 0.00397531] ; E(V) Avg: [Agent 0: 0.14659740, Agent 1: 0.44479862]) [Noise stdev: 0.25]
Episode 44/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0064 ; (Q Loss Avg: [Agent 0: 0.00687425, Agent 1: 0.00382877] ; E(V) Avg: [Agent 0: 0.13792677, Agent 1: 0.44423417]) [Noise stdev: 0.25]
Episode 45/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0063 ; (Q Loss Avg: [Agent 0: 0.00686607, Agent 1: 0.00430972] ; E(V) Avg: [Agent 0: 0.15266413, Agent 1: 0.44890042]) [Noise stdev: 0.25]
Episode 46/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0083 ; (Q Loss Avg: [Agent 0: 0.00715881, Agent 1: 0.00395222] ; E(V) Avg: [Agent 0: 0.16194377, Agent 1: 0.44382670]) [Noise stdev: 0.25]
Episode 47/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0081 ; (Q Loss Avg: [Agent 0: 0.00556030, Agent 1: 0.00387932] ; E(V) Avg: [Agent 0: 0.17473593, Agent 1: 0.46066241]) [Noise stdev: 0.25]
Episode 48/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0098 ; (Q Loss Avg: [Agent 0: 0.00525526, Agent 1: 0.00399728] ; E(V) Avg: [Agent 0: 0.19479585, Agent 1: 0.45419517]) [Noise stdev: 0.25]
Episode 49/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0096 ; (Q Loss Avg: [Agent 0: 0.00493544, Agent 1: 0.00336244] ; E(V) Avg: [Agent 0: 0.22261539, Agent 1: 0.44267459]) [Noise stdev: 0.25]
Episode 50/5000 (28 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0114 ; (Q Loss Avg: [Agent 0: 0.00423745, Agent 1: 0.00343503] ; E(V) Avg: [Agent 0: 0.23458263, Agent 1: 0.45358468]) [Noise stdev: 0.25]
Episode 51/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0112 ; (Q Loss Avg: [Agent 0: 0.00403824, Agent 1: 0.00361694] ; E(V) Avg: [Agent 0: 0.24088080, Agent 1: 0.44504442]) [Noise stdev: 0.25]
Episode 52/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0109 ; (Q Loss Avg: [Agent 0: 0.00431928, Agent 1: 0.00367804] ; E(V) Avg: [Agent 0: 0.24259021, Agent 1: 0.44544386]) [Noise stdev: 0.25]
Episode 53/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0107 ; (Q Loss Avg: [Agent 0: 0.00398930, Agent 1: 0.00335448] ; E(V) Avg: [Agent 0: 0.23751400, Agent 1: 0.45634250]) [Noise stdev: 0.25]
Episode 54/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0124 ; (Q Loss Avg: [Agent 0: 0.00391567, Agent 1: 0.00368245] ; E(V) Avg: [Agent 0: 0.26274155, Agent 1: 0.45805819]) [Noise stdev: 0.25]
Episode 55/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0121 ; (Q Loss Avg: [Agent 0: 0.00491306, Agent 1: 0.00479108] ; E(V) Avg: [Agent 0: 0.26352213, Agent 1: 0.46752687]) [Noise stdev: 0.25]
Episode 56/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0119 ; (Q Loss Avg: [Agent 0: 0.00387399, Agent 1: 0.00513256] ; E(V) Avg: [Agent 0: 0.26767992, Agent 1: 0.47087357]) [Noise stdev: 0.25]
Episode 57/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0117 ; (Q Loss Avg: [Agent 0: 0.00365961, Agent 1: 0.00495999] ; E(V) Avg: [Agent 0: 0.26417172, Agent 1: 0.46198141]) [Noise stdev: 0.25]
Episode 58/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0115 ; (Q Loss Avg: [Agent 0: 0.00364668, Agent 1: 0.00459803] ; E(V) Avg: [Agent 0: 0.27636218, Agent 1: 0.47904784]) [Noise stdev: 0.25]
Episode 59/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0113 ; (Q Loss Avg: [Agent 0: 0.00308514, Agent 1: 0.00474096] ; E(V) Avg: [Agent 0: 0.28654725, Agent 1: 0.45743853]) [Noise stdev: 0.25]
Episode 60/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0111 ; (Q Loss Avg: [Agent 0: 0.00391192, Agent 1: 0.00429333] ; E(V) Avg: [Agent 0: 0.29424017, Agent 1: 0.46238469]) [Noise stdev: 0.25]
Episode 61/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0110 ; (Q Loss Avg: [Agent 0: 0.00341490, Agent 1: 0.00380521] ; E(V) Avg: [Agent 0: 0.28134518, Agent 1: 0.46512284]) [Noise stdev: 0.25]
Episode 62/5000 (36 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0124 ; (Q Loss Avg: [Agent 0: 0.00281920, Agent 1: 0.00347641] ; E(V) Avg: [Agent 0: 0.30493944, Agent 1: 0.46097966]) [Noise stdev: 0.25]
Episode 63/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0122 ; (Q Loss Avg: [Agent 0: 0.00288267, Agent 1: 0.00384490] ; E(V) Avg: [Agent 0: 0.30332300, Agent 1: 0.46161543]) [Noise stdev: 0.25]
Episode 64/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0120 ; (Q Loss Avg: [Agent 0: 0.00287071, Agent 1: 0.00380023] ; E(V) Avg: [Agent 0: 0.31974325, Agent 1: 0.45585251]) [Noise stdev: 0.25]
Episode 65/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0118 ; (Q Loss Avg: [Agent 0: 0.00306784, Agent 1: 0.00432125] ; E(V) Avg: [Agent 0: 0.32101242, Agent 1: 0.46421300]) [Noise stdev: 0.25]
Episode 66/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0116 ; (Q Loss Avg: [Agent 0: 0.00301294, Agent 1: 0.00388299] ; E(V) Avg: [Agent 0: 0.33373560, Agent 1: 0.45376617]) [Noise stdev: 0.25]
Episode 67/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0128 ; (Q Loss Avg: [Agent 0: 0.00263103, Agent 1: 0.00372360] ; E(V) Avg: [Agent 0: 0.32684948, Agent 1: 0.46213655]) [Noise stdev: 0.25]
Episode 68/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0141 ; (Q Loss Avg: [Agent 0: 0.00265152, Agent 1: 0.00291400] ; E(V) Avg: [Agent 0: 0.35304538, Agent 1: 0.45737722]) [Noise stdev: 0.25]
Episode 69/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0139 ; (Q Loss Avg: [Agent 0: 0.00311233, Agent 1: 0.00272663] ; E(V) Avg: [Agent 0: 0.37297542, Agent 1: 0.45307859]) [Noise stdev: 0.25]
Episode 70/5000 (49 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0151 ; (Q Loss Avg: [Agent 0: 0.00243487, Agent 1: 0.00256422] ; E(V) Avg: [Agent 0: 0.37746405, Agent 1: 0.44553908]) [Noise stdev: 0.25]
Episode 71/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00270321, Agent 1: 0.00275685] ; E(V) Avg: [Agent 0: 0.38722685, Agent 1: 0.42680754]) [Noise stdev: 0.25]
Episode 72/5000 (23 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0160 ; (Q Loss Avg: [Agent 0: 0.00231956, Agent 1: 0.00264866] ; E(V) Avg: [Agent 0: 0.39353318, Agent 1: 0.42075964]) [Noise stdev: 0.25]
Episode 73/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0158 ; (Q Loss Avg: [Agent 0: 0.00275168, Agent 1: 0.00266839] ; E(V) Avg: [Agent 0: 0.39674343, Agent 1: 0.40968554]) [Noise stdev: 0.25]
Episode 74/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0169 ; (Q Loss Avg: [Agent 0: 0.00328757, Agent 1: 0.00320472] ; E(V) Avg: [Agent 0: 0.39229402, Agent 1: 0.41232776]) [Noise stdev: 0.25]
Episode 75/5000 (32 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0179 ; (Q Loss Avg: [Agent 0: 0.00372795, Agent 1: 0.00289146] ; E(V) Avg: [Agent 0: 0.32237324, Agent 1: 0.41242426]) [Noise stdev: 0.25]
Episode 76/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0177 ; (Q Loss Avg: [Agent 0: 0.00327343, Agent 1: 0.00278626] ; E(V) Avg: [Agent 0: 0.30895889, Agent 1: 0.40090504]) [Noise stdev: 0.25]
Episode 77/5000 (38 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0187 ; (Q Loss Avg: [Agent 0: 0.00346169, Agent 1: 0.00273940] ; E(V) Avg: [Agent 0: 0.29941579, Agent 1: 0.40162913]) [Noise stdev: 0.25]
Episode 78/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0185 ; (Q Loss Avg: [Agent 0: 0.00267293, Agent 1: 0.00292427] ; E(V) Avg: [Agent 0: 0.27479048, Agent 1: 0.40732969]) [Noise stdev: 0.25]
Episode 79/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0183 ; (Q Loss Avg: [Agent 0: 0.00267277, Agent 1: 0.00271213] ; E(V) Avg: [Agent 0: 0.25542046, Agent 1: 0.40216787]) [Noise stdev: 0.25]
Episode 80/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0180 ; (Q Loss Avg: [Agent 0: 0.00272725, Agent 1: 0.00276622] ; E(V) Avg: [Agent 0: 0.23610318, Agent 1: 0.38328706]) [Noise stdev: 0.25]
Episode 81/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0178 ; (Q Loss Avg: [Agent 0: 0.00253662, Agent 1: 0.00304881] ; E(V) Avg: [Agent 0: 0.22115930, Agent 1: 0.38426891]) [Noise stdev: 0.25]
Episode 82/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0187 ; (Q Loss Avg: [Agent 0: 0.00279361, Agent 1: 0.00276273] ; E(V) Avg: [Agent 0: 0.20330882, Agent 1: 0.38629384]) [Noise stdev: 0.25]
Episode 83/5000 (34 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0195 ; (Q Loss Avg: [Agent 0: 0.00337013, Agent 1: 0.00254816] ; E(V) Avg: [Agent 0: 0.18769605, Agent 1: 0.38568101]) [Noise stdev: 0.25]
Episode 84/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0193 ; (Q Loss Avg: [Agent 0: 0.00326964, Agent 1: 0.00257915] ; E(V) Avg: [Agent 0: 0.17048943, Agent 1: 0.38864296]) [Noise stdev: 0.25]
Episode 85/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0191 ; (Q Loss Avg: [Agent 0: 0.00283999, Agent 1: 0.00314547] ; E(V) Avg: [Agent 0: 0.16374436, Agent 1: 0.38301703]) [Noise stdev: 0.25]
Episode 86/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00289046, Agent 1: 0.00300404] ; E(V) Avg: [Agent 0: 0.16045745, Agent 1: 0.39803658]) [Noise stdev: 0.25]
Episode 87/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0186 ; (Q Loss Avg: [Agent 0: 0.00292333, Agent 1: 0.00316540] ; E(V) Avg: [Agent 0: 0.15357828, Agent 1: 0.39542854]) [Noise stdev: 0.25]
Episode 88/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0184 ; (Q Loss Avg: [Agent 0: 0.00246936, Agent 1: 0.00276759] ; E(V) Avg: [Agent 0: 0.15714169, Agent 1: 0.39825729]) [Noise stdev: 0.25]
Episode 89/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0182 ; (Q Loss Avg: [Agent 0: 0.00262259, Agent 1: 0.00229939] ; E(V) Avg: [Agent 0: 0.15840376, Agent 1: 0.38708253]) [Noise stdev: 0.25]
Episode 90/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0180 ; (Q Loss Avg: [Agent 0: 0.00259538, Agent 1: 0.00289977] ; E(V) Avg: [Agent 0: 0.15154686, Agent 1: 0.40072684]) [Noise stdev: 0.25]
Episode 91/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0178 ; (Q Loss Avg: [Agent 0: 0.00302330, Agent 1: 0.00384577] ; E(V) Avg: [Agent 0: 0.14236614, Agent 1: 0.38611658]) [Noise stdev: 0.25]
Episode 92/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0176 ; (Q Loss Avg: [Agent 0: 0.00291950, Agent 1: 0.00347665] ; E(V) Avg: [Agent 0: 0.14430189, Agent 1: 0.40521623]) [Noise stdev: 0.25]
Episode 93/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0174 ; (Q Loss Avg: [Agent 0: 0.00301291, Agent 1: 0.00279131] ; E(V) Avg: [Agent 0: 0.15063874, Agent 1: 0.40175497]) [Noise stdev: 0.25]
Episode 94/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0183 ; (Q Loss Avg: [Agent 0: 0.00329316, Agent 1: 0.00284046] ; E(V) Avg: [Agent 0: 0.14136012, Agent 1: 0.39740372]) [Noise stdev: 0.25]
Episode 95/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0191 ; (Q Loss Avg: [Agent 0: 0.00306956, Agent 1: 0.00216996] ; E(V) Avg: [Agent 0: 0.14362618, Agent 1: 0.40496958]) [Noise stdev: 0.25]
Episode 96/5000 (21 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00255389, Agent 1: 0.00203869] ; E(V) Avg: [Agent 0: 0.14768192, Agent 1: 0.38982811]) [Noise stdev: 0.25]
Episode 97/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0196 ; (Q Loss Avg: [Agent 0: 0.00328703, Agent 1: 0.00213648] ; E(V) Avg: [Agent 0: 0.14722428, Agent 1: 0.39826015]) [Noise stdev: 0.25]
Episode 98/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0194 ; (Q Loss Avg: [Agent 0: 0.00655628, Agent 1: 0.00213831] ; E(V) Avg: [Agent 0: 0.15320130, Agent 1: 0.39310729]) [Noise stdev: 0.25]
Episode 99/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0192 ; (Q Loss Avg: [Agent 0: 0.00562680, Agent 1: 0.00199045] ; E(V) Avg: [Agent 0: 0.15534453, Agent 1: 0.39463779]) [Noise stdev: 0.25]
Episode 100/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0192 ; (Q Loss Avg: [Agent 0: 0.00495308, Agent 1: 0.00213487] ; E(V) Avg: [Agent 0: 0.14827052, Agent 1: 0.40018294]) [Noise stdev: 0.25]
Episode 101/5000 (26 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0202 ; (Q Loss Avg: [Agent 0: 0.00358401, Agent 1: 0.00228849] ; E(V) Avg: [Agent 0: 0.15173035, Agent 1: 0.40616264]) [Noise stdev: 0.25]
Episode 102/5000 (38 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0202 ; (Q Loss Avg: [Agent 0: 0.00268588, Agent 1: 0.00184739] ; E(V) Avg: [Agent 0: 0.15551516, Agent 1: 0.40018560]) [Noise stdev: 0.25]
Episode 103/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0202 ; (Q Loss Avg: [Agent 0: 0.00277084, Agent 1: 0.00197016] ; E(V) Avg: [Agent 0: 0.16412491, Agent 1: 0.39082302]) [Noise stdev: 0.25]
Episode 104/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0202 ; (Q Loss Avg: [Agent 0: 0.00290372, Agent 1: 0.00231545] ; E(V) Avg: [Agent 0: 0.15938928, Agent 1: 0.39765486]) [Noise stdev: 0.25]
Episode 105/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0222 ; (Q Loss Avg: [Agent 0: 0.00309703, Agent 1: 0.00192618] ; E(V) Avg: [Agent 0: 0.16214539, Agent 1: 0.40205055]) [Noise stdev: 0.25]
Episode 106/5000 (46 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00362406, Agent 1: 0.00185059] ; E(V) Avg: [Agent 0: 0.17130605, Agent 1: 0.39326656]) [Noise stdev: 0.25]
Episode 107/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00306238, Agent 1: 0.00231797] ; E(V) Avg: [Agent 0: 0.15943020, Agent 1: 0.39027359]) [Noise stdev: 0.25]
Episode 108/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00329428, Agent 1: 0.00164664] ; E(V) Avg: [Agent 0: 0.17834693, Agent 1: 0.39198304]) [Noise stdev: 0.25]
Episode 109/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00367832, Agent 1: 0.00185839] ; E(V) Avg: [Agent 0: 0.17663217, Agent 1: 0.39126993]) [Noise stdev: 0.25]
Episode 110/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00406121, Agent 1: 0.00168148] ; E(V) Avg: [Agent 0: 0.16024643, Agent 1: 0.38910837]) [Noise stdev: 0.25]
Episode 111/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00464505, Agent 1: 0.00152267] ; E(V) Avg: [Agent 0: 0.16494723, Agent 1: 0.38678439]) [Noise stdev: 0.25]
Episode 112/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00489913, Agent 1: 0.00158204] ; E(V) Avg: [Agent 0: 0.16828012, Agent 1: 0.38778660]) [Noise stdev: 0.25]
Episode 113/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00524536, Agent 1: 0.00160909] ; E(V) Avg: [Agent 0: 0.16782935, Agent 1: 0.39161457]) [Noise stdev: 0.25]
Episode 114/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00522159, Agent 1: 0.00159578] ; E(V) Avg: [Agent 0: 0.15907364, Agent 1: 0.37914867]) [Noise stdev: 0.25]
Episode 115/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00475326, Agent 1: 0.00171682] ; E(V) Avg: [Agent 0: 0.16359969, Agent 1: 0.38747497]) [Noise stdev: 0.25]
Episode 116/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0222 ; (Q Loss Avg: [Agent 0: 0.00393887, Agent 1: 0.00187347] ; E(V) Avg: [Agent 0: 0.16845300, Agent 1: 0.38965177]) [Noise stdev: 0.25]
Episode 117/5000 (18 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0222 ; (Q Loss Avg: [Agent 0: 0.00376112, Agent 1: 0.00169236] ; E(V) Avg: [Agent 0: 0.15175174, Agent 1: 0.38287598]) [Noise stdev: 0.25]
Episode 118/5000 (25 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00428982, Agent 1: 0.00149768] ; E(V) Avg: [Agent 0: 0.16547530, Agent 1: 0.37406569]) [Noise stdev: 0.25]
Episode 119/5000 (17 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00396057, Agent 1: 0.00172262] ; E(V) Avg: [Agent 0: 0.14963612, Agent 1: 0.37732949]) [Noise stdev: 0.25]
Episode 120/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00328527, Agent 1: 0.00165515] ; E(V) Avg: [Agent 0: 0.15646737, Agent 1: 0.37593659]) [Noise stdev: 0.25]
Episode 121/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0232 ; (Q Loss Avg: [Agent 0: 0.00340317, Agent 1: 0.00167387] ; E(V) Avg: [Agent 0: 0.15211699, Agent 1: 0.37768289]) [Noise stdev: 0.25]
Episode 122/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0242 ; (Q Loss Avg: [Agent 0: 0.00377431, Agent 1: 0.00194208] ; E(V) Avg: [Agent 0: 0.14942399, Agent 1: 0.36914249]) [Noise stdev: 0.25]
Episode 123/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0242 ; (Q Loss Avg: [Agent 0: 0.00376136, Agent 1: 0.00170358] ; E(V) Avg: [Agent 0: 0.14417197, Agent 1: 0.36187536]) [Noise stdev: 0.25]
Episode 124/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0242 ; (Q Loss Avg: [Agent 0: 0.00397510, Agent 1: 0.00144312] ; E(V) Avg: [Agent 0: 0.14850764, Agent 1: 0.35991950]) [Noise stdev: 0.25]
Episode 125/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0252 ; (Q Loss Avg: [Agent 0: 0.00359371, Agent 1: 0.00149198] ; E(V) Avg: [Agent 0: 0.13108352, Agent 1: 0.36185132]) [Noise stdev: 0.25]
Episode 126/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0252 ; (Q Loss Avg: [Agent 0: 0.00296758, Agent 1: 0.00123754] ; E(V) Avg: [Agent 0: 0.14534232, Agent 1: 0.36094342]) [Noise stdev: 0.25]
Episode 127/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0262 ; (Q Loss Avg: [Agent 0: 0.00354136, Agent 1: 0.00146061] ; E(V) Avg: [Agent 0: 0.13805028, Agent 1: 0.35932638]) [Noise stdev: 0.25]
Episode 128/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0262 ; (Q Loss Avg: [Agent 0: 0.00333974, Agent 1: 0.00174707] ; E(V) Avg: [Agent 0: 0.12943161, Agent 1: 0.36480364]) [Noise stdev: 0.25]
Episode 129/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0262 ; (Q Loss Avg: [Agent 0: 0.00278606, Agent 1: 0.00156060] ; E(V) Avg: [Agent 0: 0.13154132, Agent 1: 0.36101558]) [Noise stdev: 0.25]
Episode 130/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0262 ; (Q Loss Avg: [Agent 0: 0.00300963, Agent 1: 0.00173525] ; E(V) Avg: [Agent 0: 0.12873239, Agent 1: 0.36668577]) [Noise stdev: 0.25]
Episode 131/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0262 ; (Q Loss Avg: [Agent 0: 0.00296000, Agent 1: 0.00142670] ; E(V) Avg: [Agent 0: 0.12784285, Agent 1: 0.35204765]) [Noise stdev: 0.25]
Episode 132/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00314916, Agent 1: 0.00146338] ; E(V) Avg: [Agent 0: 0.12976723, Agent 1: 0.35141909]) [Noise stdev: 0.25]
Episode 133/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00326032, Agent 1: 0.00146589] ; E(V) Avg: [Agent 0: 0.12992462, Agent 1: 0.34908463]) [Noise stdev: 0.25]
Episode 134/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00275016, Agent 1: 0.00153021] ; E(V) Avg: [Agent 0: 0.13444275, Agent 1: 0.34458835]) [Noise stdev: 0.25]
Episode 135/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00324422, Agent 1: 0.00159712] ; E(V) Avg: [Agent 0: 0.13351620, Agent 1: 0.33820800]) [Noise stdev: 0.25]
Episode 136/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00303792, Agent 1: 0.00172140] ; E(V) Avg: [Agent 0: 0.14792986, Agent 1: 0.34864845]) [Noise stdev: 0.25]
Episode 137/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00314765, Agent 1: 0.00151034] ; E(V) Avg: [Agent 0: 0.12676113, Agent 1: 0.34061612]) [Noise stdev: 0.25]
Episode 138/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00410395, Agent 1: 0.00139707] ; E(V) Avg: [Agent 0: 0.13080692, Agent 1: 0.34005022]) [Noise stdev: 0.25]
Episode 139/5000 (21 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00315860, Agent 1: 0.00134944] ; E(V) Avg: [Agent 0: 0.13238863, Agent 1: 0.33685616]) [Noise stdev: 0.25]
Episode 140/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00309942, Agent 1: 0.00125446] ; E(V) Avg: [Agent 0: 0.13667273, Agent 1: 0.33641053]) [Noise stdev: 0.25]
Episode 141/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00333289, Agent 1: 0.00120912] ; E(V) Avg: [Agent 0: 0.13869815, Agent 1: 0.33328450]) [Noise stdev: 0.25]
Episode 142/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00394118, Agent 1: 0.00125723] ; E(V) Avg: [Agent 0: 0.14338104, Agent 1: 0.33265152]) [Noise stdev: 0.25]
Episode 143/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00324482, Agent 1: 0.00136784] ; E(V) Avg: [Agent 0: 0.13843057, Agent 1: 0.33967125]) [Noise stdev: 0.25]
Episode 144/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0263 ; (Q Loss Avg: [Agent 0: 0.00333927, Agent 1: 0.00144625] ; E(V) Avg: [Agent 0: 0.12712541, Agent 1: 0.32831677]) [Noise stdev: 0.25]
Episode 145/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0263 ; (Q Loss Avg: [Agent 0: 0.00299888, Agent 1: 0.00139814] ; E(V) Avg: [Agent 0: 0.11755171, Agent 1: 0.33636980]) [Noise stdev: 0.25]
Episode 146/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00264617, Agent 1: 0.00130936] ; E(V) Avg: [Agent 0: 0.11517772, Agent 1: 0.33221833]) [Noise stdev: 0.25]
Episode 147/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0253 ; (Q Loss Avg: [Agent 0: 0.00255899, Agent 1: 0.00121068] ; E(V) Avg: [Agent 0: 0.12501409, Agent 1: 0.32955841]) [Noise stdev: 0.25]
Episode 148/5000 (27 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0254 ; (Q Loss Avg: [Agent 0: 0.00279217, Agent 1: 0.00128828] ; E(V) Avg: [Agent 0: 0.12212233, Agent 1: 0.32386581]) [Noise stdev: 0.25]
Episode 149/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0254 ; (Q Loss Avg: [Agent 0: 0.00266904, Agent 1: 0.00138596] ; E(V) Avg: [Agent 0: 0.12893898, Agent 1: 0.32822574]) [Noise stdev: 0.25]
Disabling random exploration...
Episode 150/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0244 ; (Q Loss Avg: [Agent 0: 0.00279925, Agent 1: 0.00120869] ; E(V) Avg: [Agent 0: 0.12074954, Agent 1: 0.32221678]) [Noise stdev: 0.25]
Episode 151/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0244 ; (Q Loss Avg: [Agent 0: 0.00279472, Agent 1: 0.00106487] ; E(V) Avg: [Agent 0: 0.11897793, Agent 1: 0.31115150]) [Noise stdev: 0.25]
Episode 152/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0244 ; (Q Loss Avg: [Agent 0: 0.00353538, Agent 1: 0.00120189] ; E(V) Avg: [Agent 0: 0.12340409, Agent 1: 0.31072504]) [Noise stdev: 0.2495]
Episode 153/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0244 ; (Q Loss Avg: [Agent 0: 0.00329876, Agent 1: 0.00132921] ; E(V) Avg: [Agent 0: 0.12511320, Agent 1: 0.31451473]) [Noise stdev: 0.249001]
Episode 154/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0234 ; (Q Loss Avg: [Agent 0: 0.00317837, Agent 1: 0.00145894] ; E(V) Avg: [Agent 0: 0.11324023, Agent 1: 0.31388503]) [Noise stdev: 0.248502998]
Episode 155/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0234 ; (Q Loss Avg: [Agent 0: 0.00319521, Agent 1: 0.00127339] ; E(V) Avg: [Agent 0: 0.11283767, Agent 1: 0.30656005]) [Noise stdev: 0.248005992004]
Episode 156/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0234 ; (Q Loss Avg: [Agent 0: 0.00364240, Agent 1: 0.00134434] ; E(V) Avg: [Agent 0: 0.12020497, Agent 1: 0.30797653]) [Noise stdev: 0.247509980019992]
Episode 157/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0234 ; (Q Loss Avg: [Agent 0: 0.00312032, Agent 1: 0.00140648] ; E(V) Avg: [Agent 0: 0.11502944, Agent 1: 0.30545752]) [Noise stdev: 0.24701496005995202]
Episode 158/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0234 ; (Q Loss Avg: [Agent 0: 0.00299615, Agent 1: 0.00264990] ; E(V) Avg: [Agent 0: 0.11555483, Agent 1: 0.29978408]) [Noise stdev: 0.24652093013983212]
Episode 159/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0234 ; (Q Loss Avg: [Agent 0: 0.00257531, Agent 1: 0.00226362] ; E(V) Avg: [Agent 0: 0.10958012, Agent 1: 0.29949087]) [Noise stdev: 0.24602788827955246]
Episode 160/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0234 ; (Q Loss Avg: [Agent 0: 0.00325066, Agent 1: 0.00172354] ; E(V) Avg: [Agent 0: 0.11123963, Agent 1: 0.28610466]) [Noise stdev: 0.24553583250299335]
Episode 161/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0234 ; (Q Loss Avg: [Agent 0: 0.00240607, Agent 1: 0.00179635] ; E(V) Avg: [Agent 0: 0.10878485, Agent 1: 0.28777160]) [Noise stdev: 0.24504476083798737]
Episode 162/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0224 ; (Q Loss Avg: [Agent 0: 0.00269000, Agent 1: 0.00151420] ; E(V) Avg: [Agent 0: 0.11607035, Agent 1: 0.29730458]) [Noise stdev: 0.2445546713163114]
Episode 163/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0224 ; (Q Loss Avg: [Agent 0: 0.00257364, Agent 1: 0.00127442] ; E(V) Avg: [Agent 0: 0.12299442, Agent 1: 0.29135547]) [Noise stdev: 0.24406556197367876]
Episode 164/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0224 ; (Q Loss Avg: [Agent 0: 0.00253191, Agent 1: 0.00116298] ; E(V) Avg: [Agent 0: 0.11319830, Agent 1: 0.28799115]) [Noise stdev: 0.2435774308497314]
Episode 165/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0224 ; (Q Loss Avg: [Agent 0: 0.00316125, Agent 1: 0.00161750] ; E(V) Avg: [Agent 0: 0.11382664, Agent 1: 0.28296740]) [Noise stdev: 0.24309027598803196]
Episode 166/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0224 ; (Q Loss Avg: [Agent 0: 0.00257445, Agent 1: 0.00169688] ; E(V) Avg: [Agent 0: 0.10521024, Agent 1: 0.27705028]) [Noise stdev: 0.2426040954360559]
Episode 167/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0215 ; (Q Loss Avg: [Agent 0: 0.00220627, Agent 1: 0.00179773] ; E(V) Avg: [Agent 0: 0.10675437, Agent 1: 0.28316395]) [Noise stdev: 0.2421188872451838]
Episode 168/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0205 ; (Q Loss Avg: [Agent 0: 0.00205223, Agent 1: 0.00170000] ; E(V) Avg: [Agent 0: 0.09761202, Agent 1: 0.28041136]) [Noise stdev: 0.24163464947069344]
Episode 169/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0205 ; (Q Loss Avg: [Agent 0: 0.00196806, Agent 1: 0.00129869] ; E(V) Avg: [Agent 0: 0.10322995, Agent 1: 0.27870836]) [Noise stdev: 0.24115138017175206]
Episode 170/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0195 ; (Q Loss Avg: [Agent 0: 0.00239250, Agent 1: 0.00107618] ; E(V) Avg: [Agent 0: 0.11121493, Agent 1: 0.27395400]) [Noise stdev: 0.24066907741140856]
Episode 171/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0195 ; (Q Loss Avg: [Agent 0: 0.00212038, Agent 1: 0.00102549] ; E(V) Avg: [Agent 0: 0.12009024, Agent 1: 0.27902751]) [Noise stdev: 0.24018773925658574]
Episode 172/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0185 ; (Q Loss Avg: [Agent 0: 0.00264622, Agent 1: 0.00108357] ; E(V) Avg: [Agent 0: 0.10422769, Agent 1: 0.27727698]) [Noise stdev: 0.23970736377807256]
Episode 173/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0185 ; (Q Loss Avg: [Agent 0: 0.00229446, Agent 1: 0.00101989] ; E(V) Avg: [Agent 0: 0.11087574, Agent 1: 0.27029149]) [Noise stdev: 0.23922794905051642]
Episode 174/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0175 ; (Q Loss Avg: [Agent 0: 0.00199281, Agent 1: 0.00098411] ; E(V) Avg: [Agent 0: 0.10400770, Agent 1: 0.27671416]) [Noise stdev: 0.2387494931524154]
Episode 175/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0166 ; (Q Loss Avg: [Agent 0: 0.00176883, Agent 1: 0.00129256] ; E(V) Avg: [Agent 0: 0.11067745, Agent 1: 0.26420886]) [Noise stdev: 0.23827199416611056]
Episode 176/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0166 ; (Q Loss Avg: [Agent 0: 0.00175767, Agent 1: 0.00125757] ; E(V) Avg: [Agent 0: 0.10634725, Agent 1: 0.26842256]) [Noise stdev: 0.23779545017777834]
Episode 177/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0156 ; (Q Loss Avg: [Agent 0: 0.00171717, Agent 1: 0.00108169] ; E(V) Avg: [Agent 0: 0.10945942, Agent 1: 0.26199295]) [Noise stdev: 0.23731985927742277]
Episode 178/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0156 ; (Q Loss Avg: [Agent 0: 0.00214387, Agent 1: 0.00105759] ; E(V) Avg: [Agent 0: 0.10796378, Agent 1: 0.26137919]) [Noise stdev: 0.23684521955886792]
Episode 179/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0156 ; (Q Loss Avg: [Agent 0: 0.00210602, Agent 1: 0.00102387] ; E(V) Avg: [Agent 0: 0.10129344, Agent 1: 0.24898615]) [Noise stdev: 0.23637152911975018]
Episode 180/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0156 ; (Q Loss Avg: [Agent 0: 0.00211682, Agent 1: 0.00098658] ; E(V) Avg: [Agent 0: 0.10306819, Agent 1: 0.26434469]) [Noise stdev: 0.23589878606151068]
Episode 181/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0156 ; (Q Loss Avg: [Agent 0: 0.00151365, Agent 1: 0.00085712] ; E(V) Avg: [Agent 0: 0.10209932, Agent 1: 0.26287921]) [Noise stdev: 0.23542698848938767]
Episode 182/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0147 ; (Q Loss Avg: [Agent 0: 0.00145368, Agent 1: 0.00097605] ; E(V) Avg: [Agent 0: 0.10201911, Agent 1: 0.26045468]) [Noise stdev: 0.2349561345124089]
Episode 183/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0138 ; (Q Loss Avg: [Agent 0: 0.00144897, Agent 1: 0.00084029] ; E(V) Avg: [Agent 0: 0.10896273, Agent 1: 0.25857390]) [Noise stdev: 0.23448622224338408]
Episode 184/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0138 ; (Q Loss Avg: [Agent 0: 0.00125318, Agent 1: 0.00091700] ; E(V) Avg: [Agent 0: 0.09750423, Agent 1: 0.25022592]) [Noise stdev: 0.23401724979889732]
Episode 185/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0138 ; (Q Loss Avg: [Agent 0: 0.00122006, Agent 1: 0.00101040] ; E(V) Avg: [Agent 0: 0.10583153, Agent 1: 0.24458248]) [Noise stdev: 0.23354921529929953]
Episode 186/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0138 ; (Q Loss Avg: [Agent 0: 0.00132350, Agent 1: 0.00085819] ; E(V) Avg: [Agent 0: 0.10852530, Agent 1: 0.25513729]) [Noise stdev: 0.23308211686870092]
Episode 187/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0138 ; (Q Loss Avg: [Agent 0: 0.00153091, Agent 1: 0.00080678] ; E(V) Avg: [Agent 0: 0.10054236, Agent 1: 0.25664897]) [Noise stdev: 0.2326159526349635]
Episode 188/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0138 ; (Q Loss Avg: [Agent 0: 0.00129333, Agent 1: 0.00091962] ; E(V) Avg: [Agent 0: 0.09393714, Agent 1: 0.25176970]) [Noise stdev: 0.2321507207296936]
Episode 189/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0138 ; (Q Loss Avg: [Agent 0: 0.00107061, Agent 1: 0.00084163] ; E(V) Avg: [Agent 0: 0.10432324, Agent 1: 0.25308477]) [Noise stdev: 0.2316864192882342]
Episode 190/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0138 ; (Q Loss Avg: [Agent 0: 0.00111606, Agent 1: 0.00141362] ; E(V) Avg: [Agent 0: 0.09250879, Agent 1: 0.25749864]) [Noise stdev: 0.23122304644965772]
Episode 191/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0138 ; (Q Loss Avg: [Agent 0: 0.00130876, Agent 1: 0.00093585] ; E(V) Avg: [Agent 0: 0.10645695, Agent 1: 0.24957912]) [Noise stdev: 0.2307606003567584]
Episode 192/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0138 ; (Q Loss Avg: [Agent 0: 0.00131628, Agent 1: 0.00083050] ; E(V) Avg: [Agent 0: 0.10391166, Agent 1: 0.24726003]) [Noise stdev: 0.2302990791560449]
Episode 193/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0138 ; (Q Loss Avg: [Agent 0: 0.00105286, Agent 1: 0.00097097] ; E(V) Avg: [Agent 0: 0.10432445, Agent 1: 0.24356466]) [Noise stdev: 0.2298384809977328]
Episode 194/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0128 ; (Q Loss Avg: [Agent 0: 0.00093962, Agent 1: 0.00084831] ; E(V) Avg: [Agent 0: 0.09865117, Agent 1: 0.24466951]) [Noise stdev: 0.22937880403573732]
Episode 195/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0119 ; (Q Loss Avg: [Agent 0: 0.00093030, Agent 1: 0.00117599] ; E(V) Avg: [Agent 0: 0.09878961, Agent 1: 0.23945220]) [Noise stdev: 0.22892004642766584]
Episode 196/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0119 ; (Q Loss Avg: [Agent 0: 0.00099777, Agent 1: 0.00119564] ; E(V) Avg: [Agent 0: 0.09798508, Agent 1: 0.23835604]) [Noise stdev: 0.2284622063348105]
Episode 197/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0110 ; (Q Loss Avg: [Agent 0: 0.00086057, Agent 1: 0.00129346] ; E(V) Avg: [Agent 0: 0.11385775, Agent 1: 0.23566896]) [Noise stdev: 0.2280052819221409]
Episode 198/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0110 ; (Q Loss Avg: [Agent 0: 0.00091136, Agent 1: 0.00112356] ; E(V) Avg: [Agent 0: 0.11376230, Agent 1: 0.24711432]) [Noise stdev: 0.2275492713582966]
Episode 199/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0110 ; (Q Loss Avg: [Agent 0: 0.00096179, Agent 1: 0.00093422] ; E(V) Avg: [Agent 0: 0.09814573, Agent 1: 0.24630964]) [Noise stdev: 0.22709417281558003]
Episode 200/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0110 ; (Q Loss Avg: [Agent 0: 0.00086255, Agent 1: 0.00089727] ; E(V) Avg: [Agent 0: 0.10480081, Agent 1: 0.23860110]) [Noise stdev: 0.22663998446994887]
Episode 201/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0100 ; (Q Loss Avg: [Agent 0: 0.00083702, Agent 1: 0.00086381] ; E(V) Avg: [Agent 0: 0.10970732, Agent 1: 0.23434307]) [Noise stdev: 0.22618670450100897]
Episode 202/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0100 ; (Q Loss Avg: [Agent 0: 0.00084730, Agent 1: 0.00082073] ; E(V) Avg: [Agent 0: 0.09946412, Agent 1: 0.22689208]) [Noise stdev: 0.22573433109200697]
Episode 203/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0100 ; (Q Loss Avg: [Agent 0: 0.00087188, Agent 1: 0.00081030] ; E(V) Avg: [Agent 0: 0.10019548, Agent 1: 0.24277282]) [Noise stdev: 0.22528286242982296]
Episode 204/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0100 ; (Q Loss Avg: [Agent 0: 0.00088497, Agent 1: 0.00085860] ; E(V) Avg: [Agent 0: 0.10764662, Agent 1: 0.23448433]) [Noise stdev: 0.2248322967049633]
Episode 205/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0080 ; (Q Loss Avg: [Agent 0: 0.00083191, Agent 1: 0.00077934] ; E(V) Avg: [Agent 0: 0.10877175, Agent 1: 0.22995214]) [Noise stdev: 0.22438263211155338]
Episode 206/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00079820, Agent 1: 0.00073039] ; E(V) Avg: [Agent 0: 0.11988655, Agent 1: 0.22338430]) [Noise stdev: 0.22393386684733027]
Episode 207/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00076381, Agent 1: 0.00085397] ; E(V) Avg: [Agent 0: 0.10505798, Agent 1: 0.23437695]) [Noise stdev: 0.22348599911363562]
Episode 208/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00076604, Agent 1: 0.00080017] ; E(V) Avg: [Agent 0: 0.11082677, Agent 1: 0.22902834]) [Noise stdev: 0.22303902711540835]
Episode 209/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00074704, Agent 1: 0.00086571] ; E(V) Avg: [Agent 0: 0.11665024, Agent 1: 0.22192112]) [Noise stdev: 0.22259294906117752]
Episode 210/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00076390, Agent 1: 0.00072755] ; E(V) Avg: [Agent 0: 0.10869715, Agent 1: 0.22031915]) [Noise stdev: 0.22214776316305515]
Episode 211/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00072170, Agent 1: 0.00070531] ; E(V) Avg: [Agent 0: 0.10601812, Agent 1: 0.22201167]) [Noise stdev: 0.22170346763672905]
Episode 212/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00080029, Agent 1: 0.00078264] ; E(V) Avg: [Agent 0: 0.11445378, Agent 1: 0.22310144]) [Noise stdev: 0.2212600607014556]
Episode 213/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00067940, Agent 1: 0.00087602] ; E(V) Avg: [Agent 0: 0.11897853, Agent 1: 0.22089082]) [Noise stdev: 0.22081754058005268]
Episode 214/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00074275, Agent 1: 0.00071332] ; E(V) Avg: [Agent 0: 0.11125872, Agent 1: 0.22305216]) [Noise stdev: 0.22037590549889258]
Episode 215/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00101212, Agent 1: 0.00070684] ; E(V) Avg: [Agent 0: 0.11237580, Agent 1: 0.21551317]) [Noise stdev: 0.2199351536878948]
Episode 216/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00130046, Agent 1: 0.00090376] ; E(V) Avg: [Agent 0: 0.11255670, Agent 1: 0.21664018]) [Noise stdev: 0.219495283380519]
Episode 217/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00123054, Agent 1: 0.00125029] ; E(V) Avg: [Agent 0: 0.10452134, Agent 1: 0.20997010]) [Noise stdev: 0.21905629281375796]
Episode 218/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0060 ; (Q Loss Avg: [Agent 0: 0.00109064, Agent 1: 0.00091091] ; E(V) Avg: [Agent 0: 0.11361068, Agent 1: 0.22053085]) [Noise stdev: 0.21861818022813045]
Episode 219/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0060 ; (Q Loss Avg: [Agent 0: 0.00092063, Agent 1: 0.00089312] ; E(V) Avg: [Agent 0: 0.10397316, Agent 1: 0.21779850]) [Noise stdev: 0.2181809438676742]
Episode 220/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00092828, Agent 1: 0.00092771] ; E(V) Avg: [Agent 0: 0.11054337, Agent 1: 0.21161713]) [Noise stdev: 0.21774458197993884]
Episode 221/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00075938, Agent 1: 0.00070571] ; E(V) Avg: [Agent 0: 0.11168957, Agent 1: 0.21256306]) [Noise stdev: 0.21730909281597896]
Episode 222/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00098127, Agent 1: 0.00073946] ; E(V) Avg: [Agent 0: 0.11766177, Agent 1: 0.21555505]) [Noise stdev: 0.216874474630347]
Episode 223/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00101618, Agent 1: 0.00059171] ; E(V) Avg: [Agent 0: 0.11043596, Agent 1: 0.21896828]) [Noise stdev: 0.21644072568108633]
Episode 224/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00075237, Agent 1: 0.00067215] ; E(V) Avg: [Agent 0: 0.11368159, Agent 1: 0.20587376]) [Noise stdev: 0.21600784422972416]
Episode 225/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0030 ; (Q Loss Avg: [Agent 0: 0.00086939, Agent 1: 0.00061757] ; E(V) Avg: [Agent 0: 0.10756341, Agent 1: 0.21011827]) [Noise stdev: 0.21557582854126472]
Episode 226/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0030 ; (Q Loss Avg: [Agent 0: 0.00076282, Agent 1: 0.00063844] ; E(V) Avg: [Agent 0: 0.12253111, Agent 1: 0.20622970]) [Noise stdev: 0.2151446768841822]
Episode 227/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00077551, Agent 1: 0.00060426] ; E(V) Avg: [Agent 0: 0.11671850, Agent 1: 0.20038175]) [Noise stdev: 0.21471438753041383]
Episode 228/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00077998, Agent 1: 0.00058994] ; E(V) Avg: [Agent 0: 0.11729821, Agent 1: 0.21196133]) [Noise stdev: 0.214284958755353]
Episode 229/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00066805, Agent 1: 0.00067903] ; E(V) Avg: [Agent 0: 0.12554051, Agent 1: 0.19770057]) [Noise stdev: 0.2138563888378423]
Episode 230/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00086648, Agent 1: 0.00068535] ; E(V) Avg: [Agent 0: 0.11888110, Agent 1: 0.19998028]) [Noise stdev: 0.2134286760601666]
Episode 231/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00071341, Agent 1: 0.00071673] ; E(V) Avg: [Agent 0: 0.11696750, Agent 1: 0.19909454]) [Noise stdev: 0.21300181870804627]
Episode 232/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00062591, Agent 1: 0.00068781] ; E(V) Avg: [Agent 0: 0.11718945, Agent 1: 0.20572431]) [Noise stdev: 0.21257581507063017]
Episode 233/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00066051, Agent 1: 0.00073484] ; E(V) Avg: [Agent 0: 0.11425589, Agent 1: 0.20204636]) [Noise stdev: 0.2121506634404889]
Episode 234/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00070830, Agent 1: 0.00057557] ; E(V) Avg: [Agent 0: 0.11684772, Agent 1: 0.18738830]) [Noise stdev: 0.21172636211360793]
Episode 235/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00063940, Agent 1: 0.00067659] ; E(V) Avg: [Agent 0: 0.11308493, Agent 1: 0.19811905]) [Noise stdev: 0.2113029093893807]
Episode 236/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00070633, Agent 1: 0.00064730] ; E(V) Avg: [Agent 0: 0.11781338, Agent 1: 0.19634539]) [Noise stdev: 0.21088030357060195]
Episode 237/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00075844, Agent 1: 0.00067341] ; E(V) Avg: [Agent 0: 0.11919551, Agent 1: 0.19656483]) [Noise stdev: 0.21045854296346075]
Episode 238/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00077109, Agent 1: 0.00068774] ; E(V) Avg: [Agent 0: 0.11286213, Agent 1: 0.19288932]) [Noise stdev: 0.21003762587753383]
Episode 239/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00068233, Agent 1: 0.00058149] ; E(V) Avg: [Agent 0: 0.10962487, Agent 1: 0.19276400]) [Noise stdev: 0.20961755062577875]
Episode 240/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00060908, Agent 1: 0.00065283] ; E(V) Avg: [Agent 0: 0.12205290, Agent 1: 0.18841249]) [Noise stdev: 0.2091983155245272]
Episode 241/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00064394, Agent 1: 0.00086779] ; E(V) Avg: [Agent 0: 0.11162219, Agent 1: 0.18821594]) [Noise stdev: 0.20877991889347813]
Episode 242/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00063685, Agent 1: 0.00081044] ; E(V) Avg: [Agent 0: 0.11481328, Agent 1: 0.19533611]) [Noise stdev: 0.20836235905569117]
Episode 243/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00066467, Agent 1: 0.00068599] ; E(V) Avg: [Agent 0: 0.11437731, Agent 1: 0.18923128]) [Noise stdev: 0.2079456343375798]
Episode 244/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00061469, Agent 1: 0.00054193] ; E(V) Avg: [Agent 0: 0.11614694, Agent 1: 0.18733194]) [Noise stdev: 0.20752974306890462]
Episode 245/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00053463, Agent 1: 0.00049704] ; E(V) Avg: [Agent 0: 0.11699807, Agent 1: 0.18042348]) [Noise stdev: 0.2071146835827668]
Episode 246/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00072817, Agent 1: 0.00057807] ; E(V) Avg: [Agent 0: 0.11846638, Agent 1: 0.17747248]) [Noise stdev: 0.20670045421560126]
Episode 247/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00063986, Agent 1: 0.00065400] ; E(V) Avg: [Agent 0: 0.11226455, Agent 1: 0.17987845]) [Noise stdev: 0.20628705330717007]
Episode 248/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00070887, Agent 1: 0.00060946] ; E(V) Avg: [Agent 0: 0.11763307, Agent 1: 0.17831170]) [Noise stdev: 0.20587447920055574]
Episode 249/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00082247, Agent 1: 0.00070111] ; E(V) Avg: [Agent 0: 0.11142917, Agent 1: 0.17769962]) [Noise stdev: 0.20546273024215464]
Episode 250/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00069176, Agent 1: 0.00063124] ; E(V) Avg: [Agent 0: 0.10952662, Agent 1: 0.17038223]) [Noise stdev: 0.20505180478167034]
Episode 251/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00072104, Agent 1: 0.00054984] ; E(V) Avg: [Agent 0: 0.12081383, Agent 1: 0.17886332]) [Noise stdev: 0.204641701172107]
Episode 252/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00089168, Agent 1: 0.00056115] ; E(V) Avg: [Agent 0: 0.11714091, Agent 1: 0.17550015]) [Noise stdev: 0.20423241776976278]
Episode 253/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00095277, Agent 1: 0.00054703] ; E(V) Avg: [Agent 0: 0.11953002, Agent 1: 0.18514377]) [Noise stdev: 0.20382395293422326]
Episode 254/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00074062, Agent 1: 0.00053196] ; E(V) Avg: [Agent 0: 0.12155507, Agent 1: 0.17935932]) [Noise stdev: 0.2034163050283548]
Episode 255/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00073311, Agent 1: 0.00061989] ; E(V) Avg: [Agent 0: 0.11943766, Agent 1: 0.17322997]) [Noise stdev: 0.2030094724182981]
Episode 256/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00069497, Agent 1: 0.00056496] ; E(V) Avg: [Agent 0: 0.11446818, Agent 1: 0.16995297]) [Noise stdev: 0.20260345347346148]
Episode 257/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00067455, Agent 1: 0.00075099] ; E(V) Avg: [Agent 0: 0.11019747, Agent 1: 0.17982868]) [Noise stdev: 0.20219824656651456]
Episode 258/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00061692, Agent 1: 0.00086455] ; E(V) Avg: [Agent 0: 0.11359706, Agent 1: 0.16553778]) [Noise stdev: 0.20179385007338152]
Episode 259/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00054903, Agent 1: 0.00077589] ; E(V) Avg: [Agent 0: 0.11665961, Agent 1: 0.17538877]) [Noise stdev: 0.20139026237323476]
Episode 260/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00063959, Agent 1: 0.00070400] ; E(V) Avg: [Agent 0: 0.11937107, Agent 1: 0.16879881]) [Noise stdev: 0.2009874818484883]
Episode 261/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00058757, Agent 1: 0.00054086] ; E(V) Avg: [Agent 0: 0.11878147, Agent 1: 0.16803400]) [Noise stdev: 0.20058550688479132]
Episode 262/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00068898, Agent 1: 0.00051720] ; E(V) Avg: [Agent 0: 0.11767112, Agent 1: 0.17004709]) [Noise stdev: 0.20018433587102175]
Episode 263/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00063798, Agent 1: 0.00054494] ; E(V) Avg: [Agent 0: 0.11235854, Agent 1: 0.16623300]) [Noise stdev: 0.1997839671992797]
Episode 264/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00060548, Agent 1: 0.00046550] ; E(V) Avg: [Agent 0: 0.11464060, Agent 1: 0.16664733]) [Noise stdev: 0.19938439926488116]
Episode 265/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00064288, Agent 1: 0.00054639] ; E(V) Avg: [Agent 0: 0.11326219, Agent 1: 0.16661624]) [Noise stdev: 0.1989856304663514]
Episode 266/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00068336, Agent 1: 0.00057204] ; E(V) Avg: [Agent 0: 0.11738124, Agent 1: 0.16672371]) [Noise stdev: 0.19858765920541868]
Episode 267/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00067496, Agent 1: 0.00060470] ; E(V) Avg: [Agent 0: 0.11254885, Agent 1: 0.17137449]) [Noise stdev: 0.19819048388700786]
Episode 268/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00063752, Agent 1: 0.00060676] ; E(V) Avg: [Agent 0: 0.11825215, Agent 1: 0.16700963]) [Noise stdev: 0.19779410291923383]
Episode 269/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00061058, Agent 1: 0.00073313] ; E(V) Avg: [Agent 0: 0.11006666, Agent 1: 0.16545841]) [Noise stdev: 0.19739851471339537]
Episode 270/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051204, Agent 1: 0.00056128] ; E(V) Avg: [Agent 0: 0.11534348, Agent 1: 0.16535284]) [Noise stdev: 0.19700371768396857]
Episode 271/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045915, Agent 1: 0.00056928] ; E(V) Avg: [Agent 0: 0.12214575, Agent 1: 0.16719220]) [Noise stdev: 0.19660971024860063]
Episode 272/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00047156, Agent 1: 0.00049539] ; E(V) Avg: [Agent 0: 0.12830693, Agent 1: 0.16173240]) [Noise stdev: 0.19621649082810344]
Episode 273/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00046519, Agent 1: 0.00047768] ; E(V) Avg: [Agent 0: 0.12121026, Agent 1: 0.16479757]) [Noise stdev: 0.19582405784644724]
Episode 274/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00061595, Agent 1: 0.00051383] ; E(V) Avg: [Agent 0: 0.12065754, Agent 1: 0.16161480]) [Noise stdev: 0.19543240973075435]
Episode 275/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00050687, Agent 1: 0.00063048] ; E(V) Avg: [Agent 0: 0.11789861, Agent 1: 0.16443285]) [Noise stdev: 0.19504154491129283]
Episode 276/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00055532, Agent 1: 0.00051536] ; E(V) Avg: [Agent 0: 0.12625474, Agent 1: 0.15768746]) [Noise stdev: 0.19465146182147025]
Episode 277/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00049379, Agent 1: 0.00059280] ; E(V) Avg: [Agent 0: 0.11660168, Agent 1: 0.15517416]) [Noise stdev: 0.1942621588978273]
Episode 278/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00046714, Agent 1: 0.00066774] ; E(V) Avg: [Agent 0: 0.11209270, Agent 1: 0.15930011]) [Noise stdev: 0.19387363458003165]
Episode 279/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051217, Agent 1: 0.00064205] ; E(V) Avg: [Agent 0: 0.11775822, Agent 1: 0.15264911]) [Noise stdev: 0.19348588731087157]
Episode 280/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00056851, Agent 1: 0.00059966] ; E(V) Avg: [Agent 0: 0.12579034, Agent 1: 0.15450667]) [Noise stdev: 0.19309891553624983]
Episode 281/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00052028, Agent 1: 0.00052208] ; E(V) Avg: [Agent 0: 0.12430171, Agent 1: 0.15644687]) [Noise stdev: 0.19271271770517734]
Episode 282/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00047568, Agent 1: 0.00057724] ; E(V) Avg: [Agent 0: 0.12529124, Agent 1: 0.15602764]) [Noise stdev: 0.192327292269767]
Episode 283/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051744, Agent 1: 0.00056323] ; E(V) Avg: [Agent 0: 0.11926360, Agent 1: 0.16060984]) [Noise stdev: 0.19194263768522746]
Episode 284/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00053570, Agent 1: 0.00055828] ; E(V) Avg: [Agent 0: 0.12564470, Agent 1: 0.16374704]) [Noise stdev: 0.191558752409857]
Episode 285/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00043033, Agent 1: 0.00059042] ; E(V) Avg: [Agent 0: 0.11520835, Agent 1: 0.15476024]) [Noise stdev: 0.1911756349050373]
Episode 286/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051977, Agent 1: 0.00057421] ; E(V) Avg: [Agent 0: 0.12166443, Agent 1: 0.14526284]) [Noise stdev: 0.19079328363522724]
Episode 287/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00059144, Agent 1: 0.00050010] ; E(V) Avg: [Agent 0: 0.12303192, Agent 1: 0.15068552]) [Noise stdev: 0.1904116970679568]
Episode 288/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00053264, Agent 1: 0.00044984] ; E(V) Avg: [Agent 0: 0.12521509, Agent 1: 0.14678723]) [Noise stdev: 0.19003087367382088]
Episode 289/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00047726, Agent 1: 0.00049724] ; E(V) Avg: [Agent 0: 0.12387711, Agent 1: 0.14828595]) [Noise stdev: 0.18965081192647323]
Episode 290/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041739, Agent 1: 0.00064442] ; E(V) Avg: [Agent 0: 0.11747504, Agent 1: 0.15000585]) [Noise stdev: 0.1892715103026203]
Episode 291/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00042125, Agent 1: 0.00068893] ; E(V) Avg: [Agent 0: 0.12674572, Agent 1: 0.14358538]) [Noise stdev: 0.18889296728201505]
Episode 292/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041405, Agent 1: 0.00070233] ; E(V) Avg: [Agent 0: 0.12833867, Agent 1: 0.15299762]) [Noise stdev: 0.18851518134745102]
Episode 293/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037711, Agent 1: 0.00065513] ; E(V) Avg: [Agent 0: 0.11475594, Agent 1: 0.15281229]) [Noise stdev: 0.18813815098475611]
Episode 294/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00044799, Agent 1: 0.00068907] ; E(V) Avg: [Agent 0: 0.12523038, Agent 1: 0.15167542]) [Noise stdev: 0.1877618746827866]
Episode 295/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00066986, Agent 1: 0.00053907] ; E(V) Avg: [Agent 0: 0.12251533, Agent 1: 0.14955235]) [Noise stdev: 0.18738635093342101]
Episode 296/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00055425, Agent 1: 0.00046824] ; E(V) Avg: [Agent 0: 0.12845267, Agent 1: 0.14791434]) [Noise stdev: 0.18701157823155418]
Episode 297/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045169, Agent 1: 0.00040945] ; E(V) Avg: [Agent 0: 0.12757305, Agent 1: 0.14520145]) [Noise stdev: 0.18663755507509106]
Episode 298/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00048075, Agent 1: 0.00043905] ; E(V) Avg: [Agent 0: 0.12528554, Agent 1: 0.14758937]) [Noise stdev: 0.18626427996494088]
Episode 299/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00049638, Agent 1: 0.00053465] ; E(V) Avg: [Agent 0: 0.12419915, Agent 1: 0.15112900]) [Noise stdev: 0.185891751405011]
Episode 300/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00044929, Agent 1: 0.00047611] ; E(V) Avg: [Agent 0: 0.12426571, Agent 1: 0.14871237]) [Noise stdev: 0.18551996790220096]
Episode 301/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00040636, Agent 1: 0.00045074] ; E(V) Avg: [Agent 0: 0.13102692, Agent 1: 0.14250531]) [Noise stdev: 0.18514892796639656]
Episode 302/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041623, Agent 1: 0.00044148] ; E(V) Avg: [Agent 0: 0.13301033, Agent 1: 0.14854000]) [Noise stdev: 0.18477863011046378]
Episode 303/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00048679, Agent 1: 0.00044948] ; E(V) Avg: [Agent 0: 0.11976099, Agent 1: 0.14612189]) [Noise stdev: 0.18440907285024286]
Episode 304/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00050252, Agent 1: 0.00051304] ; E(V) Avg: [Agent 0: 0.12718223, Agent 1: 0.14455285]) [Noise stdev: 0.18404025470454238]
Episode 305/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00044043, Agent 1: 0.00039062] ; E(V) Avg: [Agent 0: 0.12847070, Agent 1: 0.13894741]) [Noise stdev: 0.1836721741951333]
Episode 306/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00043620, Agent 1: 0.00044841] ; E(V) Avg: [Agent 0: 0.12489742, Agent 1: 0.14445057]) [Noise stdev: 0.183304829846743]
Episode 307/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038744, Agent 1: 0.00044054] ; E(V) Avg: [Agent 0: 0.12792444, Agent 1: 0.14472546]) [Noise stdev: 0.18293822018704953]
Episode 308/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041877, Agent 1: 0.00042138] ; E(V) Avg: [Agent 0: 0.12797313, Agent 1: 0.13430389]) [Noise stdev: 0.18257234374667544]
Episode 309/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038321, Agent 1: 0.00039220] ; E(V) Avg: [Agent 0: 0.12322186, Agent 1: 0.13861378]) [Noise stdev: 0.1822071990591821]
Episode 310/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038440, Agent 1: 0.00047421] ; E(V) Avg: [Agent 0: 0.12389356, Agent 1: 0.14136503]) [Noise stdev: 0.18184278466106374]
Episode 311/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00043412, Agent 1: 0.00046474] ; E(V) Avg: [Agent 0: 0.12957458, Agent 1: 0.14340505]) [Noise stdev: 0.18147909909174162]
Episode 312/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045294, Agent 1: 0.00048655] ; E(V) Avg: [Agent 0: 0.12805961, Agent 1: 0.13664852]) [Noise stdev: 0.18111614089355813]
Episode 313/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045274, Agent 1: 0.00046447] ; E(V) Avg: [Agent 0: 0.12890193, Agent 1: 0.14034837]) [Noise stdev: 0.180753908611771]
Episode 314/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00039984, Agent 1: 0.00043468] ; E(V) Avg: [Agent 0: 0.13172838, Agent 1: 0.13681872]) [Noise stdev: 0.18039240079454746]
Episode 315/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041610, Agent 1: 0.00043843] ; E(V) Avg: [Agent 0: 0.12291671, Agent 1: 0.12980051]) [Noise stdev: 0.18003161599295836]
Episode 316/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041443, Agent 1: 0.00041518] ; E(V) Avg: [Agent 0: 0.11979918, Agent 1: 0.12635622]) [Noise stdev: 0.17967155276097244]
Episode 317/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038339, Agent 1: 0.00047531] ; E(V) Avg: [Agent 0: 0.12535372, Agent 1: 0.13462538]) [Noise stdev: 0.1793122096554505]
Episode 318/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00042316, Agent 1: 0.00042940] ; E(V) Avg: [Agent 0: 0.12072957, Agent 1: 0.13358741]) [Noise stdev: 0.1789535852361396]
Episode 319/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00047958, Agent 1: 0.00038711] ; E(V) Avg: [Agent 0: 0.13001403, Agent 1: 0.12820232]) [Noise stdev: 0.17859567806566734]
Episode 320/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045486, Agent 1: 0.00039874] ; E(V) Avg: [Agent 0: 0.13074019, Agent 1: 0.13618281]) [Noise stdev: 0.17823848670953601]
Episode 321/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041179, Agent 1: 0.00037888] ; E(V) Avg: [Agent 0: 0.12720509, Agent 1: 0.13052838]) [Noise stdev: 0.17788200973611695]
Episode 322/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00047092, Agent 1: 0.00040175] ; E(V) Avg: [Agent 0: 0.12682236, Agent 1: 0.13575642]) [Noise stdev: 0.17752624571664471]
Episode 323/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045032, Agent 1: 0.00041747] ; E(V) Avg: [Agent 0: 0.12986016, Agent 1: 0.13025086]) [Noise stdev: 0.17717119322521144]
Episode 324/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037485, Agent 1: 0.00037029] ; E(V) Avg: [Agent 0: 0.12778616, Agent 1: 0.12646009]) [Noise stdev: 0.176816850838761]
Episode 325/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00040880, Agent 1: 0.00044144] ; E(V) Avg: [Agent 0: 0.12260619, Agent 1: 0.12593694]) [Noise stdev: 0.1764632171370835]
Episode 326/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041044, Agent 1: 0.00046291] ; E(V) Avg: [Agent 0: 0.12738828, Agent 1: 0.12529668]) [Noise stdev: 0.1761102907028093]
Episode 327/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00043824, Agent 1: 0.00047614] ; E(V) Avg: [Agent 0: 0.12079232, Agent 1: 0.12645508]) [Noise stdev: 0.17575807012140368]
Episode 328/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051481, Agent 1: 0.00059734] ; E(V) Avg: [Agent 0: 0.11666061, Agent 1: 0.12451357]) [Noise stdev: 0.17540655398116087]
Episode 329/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00048063, Agent 1: 0.00061283] ; E(V) Avg: [Agent 0: 0.11879612, Agent 1: 0.11603119]) [Noise stdev: 0.17505574087319856]
Episode 330/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00046951, Agent 1: 0.00045368] ; E(V) Avg: [Agent 0: 0.12183471, Agent 1: 0.12540600]) [Noise stdev: 0.17470562939145215]
Episode 331/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041328, Agent 1: 0.00042320] ; E(V) Avg: [Agent 0: 0.12518698, Agent 1: 0.12944682]) [Noise stdev: 0.17435621813266924]
Episode 332/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045671, Agent 1: 0.00038091] ; E(V) Avg: [Agent 0: 0.12873642, Agent 1: 0.12083706]) [Noise stdev: 0.1740075056964039]
Episode 333/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045455, Agent 1: 0.00038144] ; E(V) Avg: [Agent 0: 0.12278194, Agent 1: 0.11862783]) [Noise stdev: 0.1736594906850111]
Episode 334/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038629, Agent 1: 0.00035124] ; E(V) Avg: [Agent 0: 0.13146117, Agent 1: 0.12215288]) [Noise stdev: 0.17331217170364108]
Episode 335/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00040561, Agent 1: 0.00032879] ; E(V) Avg: [Agent 0: 0.12683915, Agent 1: 0.11911879]) [Noise stdev: 0.17296554736023378]
Episode 336/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038464, Agent 1: 0.00034815] ; E(V) Avg: [Agent 0: 0.12398424, Agent 1: 0.11620817]) [Noise stdev: 0.1726196162655133]
Episode 337/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00044353, Agent 1: 0.00033948] ; E(V) Avg: [Agent 0: 0.12539861, Agent 1: 0.11880108]) [Noise stdev: 0.1722743770329823]
Episode 338/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041959, Agent 1: 0.00038676] ; E(V) Avg: [Agent 0: 0.12225679, Agent 1: 0.12290350]) [Noise stdev: 0.1719298282789163]
Episode 339/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038689, Agent 1: 0.00031549] ; E(V) Avg: [Agent 0: 0.12386921, Agent 1: 0.11548483]) [Noise stdev: 0.17158596862235848]
Episode 340/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038373, Agent 1: 0.00034774] ; E(V) Avg: [Agent 0: 0.12135806, Agent 1: 0.11627121]) [Noise stdev: 0.17124279668511375]
Episode 341/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034948, Agent 1: 0.00033735] ; E(V) Avg: [Agent 0: 0.12328582, Agent 1: 0.11197553]) [Noise stdev: 0.17090031109174353]
Episode 342/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037631, Agent 1: 0.00034222] ; E(V) Avg: [Agent 0: 0.12018335, Agent 1: 0.11648761]) [Noise stdev: 0.17055851046956005]
Episode 343/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035195, Agent 1: 0.00038471] ; E(V) Avg: [Agent 0: 0.12469400, Agent 1: 0.11861241]) [Noise stdev: 0.17021739344862094]
Episode 344/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00044817, Agent 1: 0.00037995] ; E(V) Avg: [Agent 0: 0.12324327, Agent 1: 0.11013472]) [Noise stdev: 0.1698769586617237]
Episode 345/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045482, Agent 1: 0.00036284] ; E(V) Avg: [Agent 0: 0.12774773, Agent 1: 0.11593551]) [Noise stdev: 0.16953720474440026]
Episode 346/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045316, Agent 1: 0.00036359] ; E(V) Avg: [Agent 0: 0.11880794, Agent 1: 0.12017359]) [Noise stdev: 0.16919813033491146]
Episode 347/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00046735, Agent 1: 0.00041525] ; E(V) Avg: [Agent 0: 0.12024962, Agent 1: 0.11046525]) [Noise stdev: 0.16885973407424165]
Episode 348/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00047470, Agent 1: 0.00050075] ; E(V) Avg: [Agent 0: 0.12365433, Agent 1: 0.11435108]) [Noise stdev: 0.16852201460609317]
Episode 349/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00047348, Agent 1: 0.00051390] ; E(V) Avg: [Agent 0: 0.11881149, Agent 1: 0.11729669]) [Noise stdev: 0.16818497057688098]
Episode 350/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038199, Agent 1: 0.00050019] ; E(V) Avg: [Agent 0: 0.11821926, Agent 1: 0.11545705]) [Noise stdev: 0.1678486006357272]
Episode 351/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038110, Agent 1: 0.00041338] ; E(V) Avg: [Agent 0: 0.12183387, Agent 1: 0.11424159]) [Noise stdev: 0.16751290343445574]
Episode 352/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00043972, Agent 1: 0.00047288] ; E(V) Avg: [Agent 0: 0.11867844, Agent 1: 0.11793332]) [Noise stdev: 0.16717787762758682]
Episode 353/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032995, Agent 1: 0.00051319] ; E(V) Avg: [Agent 0: 0.11882492, Agent 1: 0.11050769]) [Noise stdev: 0.16684352187233165]
Episode 354/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031910, Agent 1: 0.00049980] ; E(V) Avg: [Agent 0: 0.12024292, Agent 1: 0.10571219]) [Noise stdev: 0.16650983482858697]
Episode 355/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032386, Agent 1: 0.00042251] ; E(V) Avg: [Agent 0: 0.12059523, Agent 1: 0.10517727]) [Noise stdev: 0.1661768151589298]
Episode 356/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031635, Agent 1: 0.00044816] ; E(V) Avg: [Agent 0: 0.12127188, Agent 1: 0.10883173]) [Noise stdev: 0.16584446152861193]
Episode 357/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00030580, Agent 1: 0.00043070] ; E(V) Avg: [Agent 0: 0.11803095, Agent 1: 0.10840844]) [Noise stdev: 0.16551277260555472]
Episode 358/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00036218, Agent 1: 0.00042235] ; E(V) Avg: [Agent 0: 0.11562806, Agent 1: 0.10646118]) [Noise stdev: 0.1651817470603436]
Episode 359/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00036277, Agent 1: 0.00038354] ; E(V) Avg: [Agent 0: 0.11968061, Agent 1: 0.10068974]) [Noise stdev: 0.16485138356622292]
Episode 360/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035373, Agent 1: 0.00040040] ; E(V) Avg: [Agent 0: 0.12642473, Agent 1: 0.11187755]) [Noise stdev: 0.16452168079909046]
Episode 361/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034400, Agent 1: 0.00043027] ; E(V) Avg: [Agent 0: 0.11887871, Agent 1: 0.10480652]) [Noise stdev: 0.1641926374374923]
Episode 362/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038163, Agent 1: 0.00042729] ; E(V) Avg: [Agent 0: 0.12127889, Agent 1: 0.10445464]) [Noise stdev: 0.1638642521626173]
Episode 363/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037909, Agent 1: 0.00047507] ; E(V) Avg: [Agent 0: 0.12131575, Agent 1: 0.10410977]) [Noise stdev: 0.16353652365829205]
Episode 364/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00039497, Agent 1: 0.00043069] ; E(V) Avg: [Agent 0: 0.12244979, Agent 1: 0.10030592]) [Noise stdev: 0.16320945061097547]
Episode 365/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00036283, Agent 1: 0.00044848] ; E(V) Avg: [Agent 0: 0.12427867, Agent 1: 0.10984316]) [Noise stdev: 0.16288303170975352]
Episode 366/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037375, Agent 1: 0.00043582] ; E(V) Avg: [Agent 0: 0.12020899, Agent 1: 0.09623768]) [Noise stdev: 0.16255726564633402]
Episode 367/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037348, Agent 1: 0.00050479] ; E(V) Avg: [Agent 0: 0.11948508, Agent 1: 0.10498451]) [Noise stdev: 0.16223215111504136]
Episode 368/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00039984, Agent 1: 0.00056468] ; E(V) Avg: [Agent 0: 0.12348232, Agent 1: 0.10348234]) [Noise stdev: 0.16190768681281129]
Episode 369/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00042589, Agent 1: 0.00054701] ; E(V) Avg: [Agent 0: 0.11466964, Agent 1: 0.09676911]) [Noise stdev: 0.16158387143918565]
Episode 370/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00036955, Agent 1: 0.00045354] ; E(V) Avg: [Agent 0: 0.11694818, Agent 1: 0.10391918]) [Noise stdev: 0.16126070369630727]
Episode 371/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038287, Agent 1: 0.00049436] ; E(V) Avg: [Agent 0: 0.11880224, Agent 1: 0.09765003]) [Noise stdev: 0.16093818228891466]
Episode 372/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035471, Agent 1: 0.00049586] ; E(V) Avg: [Agent 0: 0.12066462, Agent 1: 0.09543749]) [Noise stdev: 0.16061630592433682]
Episode 373/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00036622, Agent 1: 0.00047457] ; E(V) Avg: [Agent 0: 0.12281921, Agent 1: 0.09892403]) [Noise stdev: 0.16029507331248813]
Episode 374/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037698, Agent 1: 0.00043909] ; E(V) Avg: [Agent 0: 0.12201495, Agent 1: 0.09695419]) [Noise stdev: 0.15997448316586316]
Episode 375/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035241, Agent 1: 0.00036453] ; E(V) Avg: [Agent 0: 0.11943934, Agent 1: 0.09141315]) [Noise stdev: 0.15965453419953143]
Episode 376/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034956, Agent 1: 0.00037721] ; E(V) Avg: [Agent 0: 0.12092260, Agent 1: 0.09893866]) [Noise stdev: 0.15933522513113238]
Episode 377/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035716, Agent 1: 0.00038707] ; E(V) Avg: [Agent 0: 0.12163446, Agent 1: 0.09063011]) [Noise stdev: 0.15901655468087011]
Episode 378/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035508, Agent 1: 0.00040123] ; E(V) Avg: [Agent 0: 0.11672465, Agent 1: 0.10027094]) [Noise stdev: 0.15869852157150838]
Episode 379/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032933, Agent 1: 0.00042973] ; E(V) Avg: [Agent 0: 0.11640664, Agent 1: 0.09801394]) [Noise stdev: 0.15838112452836536]
Episode 380/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00028992, Agent 1: 0.00045997] ; E(V) Avg: [Agent 0: 0.12054200, Agent 1: 0.09461952]) [Noise stdev: 0.15806436227930865]
Episode 381/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00029793, Agent 1: 0.00040136] ; E(V) Avg: [Agent 0: 0.12036070, Agent 1: 0.10177012]) [Noise stdev: 0.15774823355475004]
Episode 382/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031023, Agent 1: 0.00042078] ; E(V) Avg: [Agent 0: 0.11860121, Agent 1: 0.09539893]) [Noise stdev: 0.15743273708764055]
Episode 383/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00033165, Agent 1: 0.00036750] ; E(V) Avg: [Agent 0: 0.11406616, Agent 1: 0.09684194]) [Noise stdev: 0.15711787161346527]
Episode 384/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032212, Agent 1: 0.00040585] ; E(V) Avg: [Agent 0: 0.12539892, Agent 1: 0.09324006]) [Noise stdev: 0.15680363587023835]
Episode 385/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032572, Agent 1: 0.00036351] ; E(V) Avg: [Agent 0: 0.11550627, Agent 1: 0.09499969]) [Noise stdev: 0.15649002859849787]
Episode 386/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00042527, Agent 1: 0.00033260] ; E(V) Avg: [Agent 0: 0.12019828, Agent 1: 0.09785749]) [Noise stdev: 0.15617704854130088]
Episode 387/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00044725, Agent 1: 0.00035256] ; E(V) Avg: [Agent 0: 0.12248532, Agent 1: 0.09310222]) [Noise stdev: 0.15586469444421827]
Episode 388/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00049442, Agent 1: 0.00034231] ; E(V) Avg: [Agent 0: 0.11625082, Agent 1: 0.09252554]) [Noise stdev: 0.15555296505532984]
Episode 389/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034966, Agent 1: 0.00038806] ; E(V) Avg: [Agent 0: 0.12665446, Agent 1: 0.09755421]) [Noise stdev: 0.15524185912521918]
Episode 390/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032431, Agent 1: 0.00042713] ; E(V) Avg: [Agent 0: 0.11878579, Agent 1: 0.09680238]) [Noise stdev: 0.15493137540696875]
Episode 391/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034371, Agent 1: 0.00036885] ; E(V) Avg: [Agent 0: 0.11616850, Agent 1: 0.08940789]) [Noise stdev: 0.1546215126561548]
Episode 392/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031067, Agent 1: 0.00034869] ; E(V) Avg: [Agent 0: 0.11359316, Agent 1: 0.08846993]) [Noise stdev: 0.1543122696308425]
Episode 393/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00033724, Agent 1: 0.00036633] ; E(V) Avg: [Agent 0: 0.12228343, Agent 1: 0.09380716]) [Noise stdev: 0.1540036450915808]
Episode 394/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00039678, Agent 1: 0.00038591] ; E(V) Avg: [Agent 0: 0.11302061, Agent 1: 0.09583226]) [Noise stdev: 0.15369563780139764]
Episode 395/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00049681, Agent 1: 0.00050797] ; E(V) Avg: [Agent 0: 0.11329226, Agent 1: 0.09097558]) [Noise stdev: 0.15338824652579483]
Episode 396/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00050842, Agent 1: 0.00039822] ; E(V) Avg: [Agent 0: 0.11966207, Agent 1: 0.08449857]) [Noise stdev: 0.15308147003274325]
Episode 397/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051376, Agent 1: 0.00033866] ; E(V) Avg: [Agent 0: 0.11543070, Agent 1: 0.07979846]) [Noise stdev: 0.15277530709267775]
Episode 398/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00046085, Agent 1: 0.00037524] ; E(V) Avg: [Agent 0: 0.11514799, Agent 1: 0.10219596]) [Noise stdev: 0.1524697564784924]
Episode 399/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00048868, Agent 1: 0.00034916] ; E(V) Avg: [Agent 0: 0.11708243, Agent 1: 0.09565545]) [Noise stdev: 0.1521648169655354]
Episode 400/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00039486, Agent 1: 0.00041380] ; E(V) Avg: [Agent 0: 0.11560294, Agent 1: 0.08506007]) [Noise stdev: 0.15186048733160434]
Episode 401/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034749, Agent 1: 0.00041050] ; E(V) Avg: [Agent 0: 0.11481714, Agent 1: 0.08627369]) [Noise stdev: 0.15155676635694112]
Episode 402/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032939, Agent 1: 0.00037376] ; E(V) Avg: [Agent 0: 0.11585559, Agent 1: 0.07765786]) [Noise stdev: 0.15125365282422723]
Episode 403/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00033430, Agent 1: 0.00042929] ; E(V) Avg: [Agent 0: 0.11146220, Agent 1: 0.09191355]) [Noise stdev: 0.15095114551857877]
Episode 404/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035396, Agent 1: 0.00038936] ; E(V) Avg: [Agent 0: 0.11118744, Agent 1: 0.08835657]) [Noise stdev: 0.1506492432275416]
Episode 405/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00030366, Agent 1: 0.00034215] ; E(V) Avg: [Agent 0: 0.11561517, Agent 1: 0.07275451]) [Noise stdev: 0.15034794474108654]
Episode 406/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035281, Agent 1: 0.00031812] ; E(V) Avg: [Agent 0: 0.11415861, Agent 1: 0.08547789]) [Noise stdev: 0.15004724885160436]
Episode 407/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034653, Agent 1: 0.00031665] ; E(V) Avg: [Agent 0: 0.11290484, Agent 1: 0.08968808]) [Noise stdev: 0.14974715435390115]
Episode 408/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00039340, Agent 1: 0.00033042] ; E(V) Avg: [Agent 0: 0.11751668, Agent 1: 0.08363778]) [Noise stdev: 0.14944766004519336]
Episode 409/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037178, Agent 1: 0.00033693] ; E(V) Avg: [Agent 0: 0.11354280, Agent 1: 0.08753239]) [Noise stdev: 0.14914876472510297]
Episode 410/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00042656, Agent 1: 0.00033057] ; E(V) Avg: [Agent 0: 0.11064931, Agent 1: 0.08493726]) [Noise stdev: 0.14885046719565276]
Episode 411/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038108, Agent 1: 0.00038249] ; E(V) Avg: [Agent 0: 0.12004169, Agent 1: 0.08030484]) [Noise stdev: 0.14855276626126146]
Episode 412/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00039397, Agent 1: 0.00033865] ; E(V) Avg: [Agent 0: 0.11282631, Agent 1: 0.08387203]) [Noise stdev: 0.14825566072873894]
Episode 413/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034325, Agent 1: 0.00035464] ; E(V) Avg: [Agent 0: 0.11745682, Agent 1: 0.08149520]) [Noise stdev: 0.14795914940728147]
Episode 414/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034073, Agent 1: 0.00039064] ; E(V) Avg: [Agent 0: 0.11630823, Agent 1: 0.07902213]) [Noise stdev: 0.1476632311084669]
Episode 415/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035052, Agent 1: 0.00034077] ; E(V) Avg: [Agent 0: 0.11109073, Agent 1: 0.08720174]) [Noise stdev: 0.14736790464624996]
Episode 416/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031895, Agent 1: 0.00033251] ; E(V) Avg: [Agent 0: 0.11226618, Agent 1: 0.07742854]) [Noise stdev: 0.14707316883695745]
Episode 417/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031535, Agent 1: 0.00034334] ; E(V) Avg: [Agent 0: 0.11156592, Agent 1: 0.07687573]) [Noise stdev: 0.14677902249928354]
Episode 418/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00033578, Agent 1: 0.00032781] ; E(V) Avg: [Agent 0: 0.10864150, Agent 1: 0.07950076]) [Noise stdev: 0.14648546445428498]
Episode 419/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032466, Agent 1: 0.00039206] ; E(V) Avg: [Agent 0: 0.11570712, Agent 1: 0.07397777]) [Noise stdev: 0.14619249352537642]
Episode 420/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00030944, Agent 1: 0.00033912] ; E(V) Avg: [Agent 0: 0.11232399, Agent 1: 0.07660524]) [Noise stdev: 0.14590010853832566]
Episode 421/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00030762, Agent 1: 0.00038417] ; E(V) Avg: [Agent 0: 0.11489553, Agent 1: 0.07304179]) [Noise stdev: 0.14560830832124902]
Episode 422/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00030322, Agent 1: 0.00033148] ; E(V) Avg: [Agent 0: 0.11229583, Agent 1: 0.08122234]) [Noise stdev: 0.14531709170460652]
Episode 423/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00028198, Agent 1: 0.00032374] ; E(V) Avg: [Agent 0: 0.11899232, Agent 1: 0.06831280]) [Noise stdev: 0.1450264575211973]
Episode 424/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00029239, Agent 1: 0.00030784] ; E(V) Avg: [Agent 0: 0.10972419, Agent 1: 0.06789846]) [Noise stdev: 0.14473640460615492]
Episode 425/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00027388, Agent 1: 0.00032416] ; E(V) Avg: [Agent 0: 0.11870941, Agent 1: 0.07659142]) [Noise stdev: 0.1444469317969426]
Episode 426/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00027595, Agent 1: 0.00034662] ; E(V) Avg: [Agent 0: 0.11073137, Agent 1: 0.07654871]) [Noise stdev: 0.14415803793334872]
Episode 427/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00029732, Agent 1: 0.00036256] ; E(V) Avg: [Agent 0: 0.11263179, Agent 1: 0.07387319]) [Noise stdev: 0.14386972185748204]
Episode 428/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00029512, Agent 1: 0.00034933] ; E(V) Avg: [Agent 0: 0.11490387, Agent 1: 0.07498415]) [Noise stdev: 0.14358198241376707]
Episode 429/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00028363, Agent 1: 0.00041911] ; E(V) Avg: [Agent 0: 0.11930227, Agent 1: 0.07185299]) [Noise stdev: 0.14329481844893954]
Episode 430/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00029551, Agent 1: 0.00043912] ; E(V) Avg: [Agent 0: 0.10991174, Agent 1: 0.07470968]) [Noise stdev: 0.14300822881204167]
Episode 431/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00029851, Agent 1: 0.00040279] ; E(V) Avg: [Agent 0: 0.11892183, Agent 1: 0.07044639]) [Noise stdev: 0.1427222123544176]
Episode 432/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00033567, Agent 1: 0.00039076] ; E(V) Avg: [Agent 0: 0.11555584, Agent 1: 0.07794226]) [Noise stdev: 0.14243676792970875]
Episode 433/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032497, Agent 1: 0.00037406] ; E(V) Avg: [Agent 0: 0.12096574, Agent 1: 0.07889407]) [Noise stdev: 0.14215189439384932]
Episode 434/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034679, Agent 1: 0.00032987] ; E(V) Avg: [Agent 0: 0.13690129, Agent 1: 0.08163008]) [Noise stdev: 0.14186759060506163]
Episode 435/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032249, Agent 1: 0.00029575] ; E(V) Avg: [Agent 0: 0.12109481, Agent 1: 0.08105276]) [Noise stdev: 0.14158385542385152]
Episode 436/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00029825, Agent 1: 0.00031164] ; E(V) Avg: [Agent 0: 0.11935270, Agent 1: 0.08738449]) [Noise stdev: 0.1413006877130038]
Episode 437/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031984, Agent 1: 0.00035417] ; E(V) Avg: [Agent 0: 0.12495683, Agent 1: 0.08453674]) [Noise stdev: 0.14101808633757779]
Episode 438/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035402, Agent 1: 0.00046272] ; E(V) Avg: [Agent 0: 0.12108550, Agent 1: 0.08068652]) [Noise stdev: 0.14073605016490262]
Episode 439/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034835, Agent 1: 0.00040452] ; E(V) Avg: [Agent 0: 0.12159808, Agent 1: 0.08056858]) [Noise stdev: 0.14045457806457282]
Episode 440/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038300, Agent 1: 0.00046298] ; E(V) Avg: [Agent 0: 0.12151697, Agent 1: 0.08126893]) [Noise stdev: 0.14017366890844368]
Episode 441/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041674, Agent 1: 0.00052762] ; E(V) Avg: [Agent 0: 0.11086607, Agent 1: 0.08543525]) [Noise stdev: 0.13989332157062678]
Episode 442/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00043989, Agent 1: 0.00050502] ; E(V) Avg: [Agent 0: 0.12330222, Agent 1: 0.08467119]) [Noise stdev: 0.13961353492748552]
Episode 443/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038671, Agent 1: 0.00044355] ; E(V) Avg: [Agent 0: 0.11253192, Agent 1: 0.08344260]) [Noise stdev: 0.13933430785763054]
Episode 444/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00052271, Agent 1: 0.00050661] ; E(V) Avg: [Agent 0: 0.12718240, Agent 1: 0.08983367]) [Noise stdev: 0.13905563924191527]
Episode 445/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051189, Agent 1: 0.00048164] ; E(V) Avg: [Agent 0: 0.12831699, Agent 1: 0.08737404]) [Noise stdev: 0.13877752796343143]
Episode 446/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00052729, Agent 1: 0.00051191] ; E(V) Avg: [Agent 0: 0.12191470, Agent 1: 0.08892379]) [Noise stdev: 0.13849997290750457]
Episode 447/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00052695, Agent 1: 0.00050122] ; E(V) Avg: [Agent 0: 0.12302154, Agent 1: 0.08251321]) [Noise stdev: 0.13822297296168956]
Episode 448/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00047376, Agent 1: 0.00050177] ; E(V) Avg: [Agent 0: 0.12250748, Agent 1: 0.08446346]) [Noise stdev: 0.13794652701576618]
Episode 449/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00043859, Agent 1: 0.00048897] ; E(V) Avg: [Agent 0: 0.11918294, Agent 1: 0.08001968]) [Noise stdev: 0.13767063396173465]
Episode 450/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00044249, Agent 1: 0.00045735] ; E(V) Avg: [Agent 0: 0.12193210, Agent 1: 0.08287674]) [Noise stdev: 0.13739529269381118]
Episode 451/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045165, Agent 1: 0.00045893] ; E(V) Avg: [Agent 0: 0.12868696, Agent 1: 0.07704566]) [Noise stdev: 0.13712050210842355]
Episode 452/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00046572, Agent 1: 0.00055735] ; E(V) Avg: [Agent 0: 0.12863727, Agent 1: 0.07984612]) [Noise stdev: 0.1368462611042067]
Episode 453/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00040568, Agent 1: 0.00047127] ; E(V) Avg: [Agent 0: 0.11108478, Agent 1: 0.08403540]) [Noise stdev: 0.1365725685819983]
Episode 454/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041236, Agent 1: 0.00044612] ; E(V) Avg: [Agent 0: 0.12512593, Agent 1: 0.08230959]) [Noise stdev: 0.13629942344483428]
Episode 455/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00040120, Agent 1: 0.00050069] ; E(V) Avg: [Agent 0: 0.12236476, Agent 1: 0.07974967]) [Noise stdev: 0.1360268245979446]
Episode 456/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00039766, Agent 1: 0.00045964] ; E(V) Avg: [Agent 0: 0.11472503, Agent 1: 0.07819507]) [Noise stdev: 0.13575477094874872]
Episode 457/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041243, Agent 1: 0.00040408] ; E(V) Avg: [Agent 0: 0.12059118, Agent 1: 0.07532383]) [Noise stdev: 0.1354832614068512]
Episode 458/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041541, Agent 1: 0.00049844] ; E(V) Avg: [Agent 0: 0.12531950, Agent 1: 0.07795559]) [Noise stdev: 0.13521229488403752]
Episode 459/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00042668, Agent 1: 0.00051205] ; E(V) Avg: [Agent 0: 0.12263507, Agent 1: 0.08669594]) [Noise stdev: 0.13494187029426943]
Episode 460/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00047035, Agent 1: 0.00038454] ; E(V) Avg: [Agent 0: 0.11617919, Agent 1: 0.08014383]) [Noise stdev: 0.1346719865536809]
Episode 461/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00043376, Agent 1: 0.00043479] ; E(V) Avg: [Agent 0: 0.12758526, Agent 1: 0.08131643]) [Noise stdev: 0.13440264258057355]
Episode 462/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00039949, Agent 1: 0.00041510] ; E(V) Avg: [Agent 0: 0.12742837, Agent 1: 0.08203287]) [Noise stdev: 0.1341338372954124]
Episode 463/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00040490, Agent 1: 0.00039819] ; E(V) Avg: [Agent 0: 0.13121308, Agent 1: 0.08261436]) [Noise stdev: 0.13386556962082158]
Episode 464/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038998, Agent 1: 0.00038768] ; E(V) Avg: [Agent 0: 0.13264485, Agent 1: 0.07449550]) [Noise stdev: 0.13359783848157994]
Episode 465/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00040336, Agent 1: 0.00031834] ; E(V) Avg: [Agent 0: 0.12457135, Agent 1: 0.08049124]) [Noise stdev: 0.1333306428046168]
Episode 466/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037636, Agent 1: 0.00035945] ; E(V) Avg: [Agent 0: 0.12907302, Agent 1: 0.07386008]) [Noise stdev: 0.13306398151900756]
Episode 467/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00040466, Agent 1: 0.00034809] ; E(V) Avg: [Agent 0: 0.13481747, Agent 1: 0.08149316]) [Noise stdev: 0.13279785355596954]
Episode 468/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00044931, Agent 1: 0.00037348] ; E(V) Avg: [Agent 0: 0.12530809, Agent 1: 0.08089061]) [Noise stdev: 0.1325322578488576]
Episode 469/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00050163, Agent 1: 0.00036658] ; E(V) Avg: [Agent 0: 0.13000210, Agent 1: 0.08042335]) [Noise stdev: 0.1322671933331599]
Episode 470/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00053133, Agent 1: 0.00040648] ; E(V) Avg: [Agent 0: 0.13614345, Agent 1: 0.07854181]) [Noise stdev: 0.13200265894649357]
Episode 471/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00048463, Agent 1: 0.00033786] ; E(V) Avg: [Agent 0: 0.12621897, Agent 1: 0.07866637]) [Noise stdev: 0.1317386536286006]
Episode 472/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00054266, Agent 1: 0.00030359] ; E(V) Avg: [Agent 0: 0.13138267, Agent 1: 0.07250633]) [Noise stdev: 0.1314751763213434]
Episode 473/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00052597, Agent 1: 0.00032485] ; E(V) Avg: [Agent 0: 0.13484675, Agent 1: 0.06646111]) [Noise stdev: 0.13121222596870072]
Episode 474/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00054846, Agent 1: 0.00035619] ; E(V) Avg: [Agent 0: 0.12618398, Agent 1: 0.07045415]) [Noise stdev: 0.1309498015167633]
Episode 475/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00055047, Agent 1: 0.00033793] ; E(V) Avg: [Agent 0: 0.13326318, Agent 1: 0.07216379]) [Noise stdev: 0.13068790191372978]
Episode 476/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00055734, Agent 1: 0.00036249] ; E(V) Avg: [Agent 0: 0.13014629, Agent 1: 0.07088496]) [Noise stdev: 0.13042652610990232]
Episode 477/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00057089, Agent 1: 0.00040021] ; E(V) Avg: [Agent 0: 0.12972632, Agent 1: 0.07060763]) [Noise stdev: 0.1301656730576825]
Episode 478/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00057104, Agent 1: 0.00038030] ; E(V) Avg: [Agent 0: 0.13292792, Agent 1: 0.06837796]) [Noise stdev: 0.12990534171156715]
Episode 479/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00057289, Agent 1: 0.00040322] ; E(V) Avg: [Agent 0: 0.13743371, Agent 1: 0.08050014]) [Noise stdev: 0.12964553102814402]
Episode 480/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051467, Agent 1: 0.00042487] ; E(V) Avg: [Agent 0: 0.13174247, Agent 1: 0.07002357]) [Noise stdev: 0.12938623996608772]
Episode 481/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00049400, Agent 1: 0.00035448] ; E(V) Avg: [Agent 0: 0.13019040, Agent 1: 0.07613732]) [Noise stdev: 0.12912746748615556]
Episode 482/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00050331, Agent 1: 0.00043273] ; E(V) Avg: [Agent 0: 0.12609479, Agent 1: 0.07069904]) [Noise stdev: 0.12886921255118325]
Episode 483/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051857, Agent 1: 0.00041455] ; E(V) Avg: [Agent 0: 0.13710333, Agent 1: 0.07749286]) [Noise stdev: 0.12861147412608087]
Episode 484/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00054980, Agent 1: 0.00038558] ; E(V) Avg: [Agent 0: 0.13065908, Agent 1: 0.07080222]) [Noise stdev: 0.1283542511778287]
Episode 485/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00059952, Agent 1: 0.00042655] ; E(V) Avg: [Agent 0: 0.12717383, Agent 1: 0.07755074]) [Noise stdev: 0.12809754267547305]
Episode 486/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00059708, Agent 1: 0.00037782] ; E(V) Avg: [Agent 0: 0.13599907, Agent 1: 0.07088407]) [Noise stdev: 0.1278413475901221]
Episode 487/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00059026, Agent 1: 0.00037928] ; E(V) Avg: [Agent 0: 0.13274960, Agent 1: 0.07657556]) [Noise stdev: 0.12758566489494186]
Episode 488/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00050674, Agent 1: 0.00043341] ; E(V) Avg: [Agent 0: 0.13244319, Agent 1: 0.07840454]) [Noise stdev: 0.12733049356515197]
Episode 489/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00049338, Agent 1: 0.00037559] ; E(V) Avg: [Agent 0: 0.13706316, Agent 1: 0.07628620]) [Noise stdev: 0.12707583257802166]
Episode 490/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051941, Agent 1: 0.00051539] ; E(V) Avg: [Agent 0: 0.13188284, Agent 1: 0.07724266]) [Noise stdev: 0.1268216809128656]
Episode 491/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00048623, Agent 1: 0.00045925] ; E(V) Avg: [Agent 0: 0.12990471, Agent 1: 0.07995882]) [Noise stdev: 0.12656803755103987]
Episode 492/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00055063, Agent 1: 0.00045880] ; E(V) Avg: [Agent 0: 0.13733451, Agent 1: 0.08593111]) [Noise stdev: 0.1263149014759378]
Episode 493/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00052314, Agent 1: 0.00040948] ; E(V) Avg: [Agent 0: 0.13647395, Agent 1: 0.07678189]) [Noise stdev: 0.12606227167298592]
Episode 494/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00048572, Agent 1: 0.00051973] ; E(V) Avg: [Agent 0: 0.13714895, Agent 1: 0.07145385]) [Noise stdev: 0.12581014712963995]
Episode 495/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00060669, Agent 1: 0.00056477] ; E(V) Avg: [Agent 0: 0.13662565, Agent 1: 0.07525951]) [Noise stdev: 0.12555852683538066]
Episode 496/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051437, Agent 1: 0.00056220] ; E(V) Avg: [Agent 0: 0.12805355, Agent 1: 0.08032401]) [Noise stdev: 0.1253074097817099]
Episode 497/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051826, Agent 1: 0.00043963] ; E(V) Avg: [Agent 0: 0.14031938, Agent 1: 0.08147771]) [Noise stdev: 0.12505679496214647]
Episode 498/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00053865, Agent 1: 0.00045066] ; E(V) Avg: [Agent 0: 0.13161368, Agent 1: 0.09113308]) [Noise stdev: 0.12480668137222217]
Episode 499/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00053487, Agent 1: 0.00039471] ; E(V) Avg: [Agent 0: 0.13490525, Agent 1: 0.08050789]) [Noise stdev: 0.12455706800947773]
Episode 500/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00055462, Agent 1: 0.00040804] ; E(V) Avg: [Agent 0: 0.12948166, Agent 1: 0.07769689]) [Noise stdev: 0.12430795387345878]
Episode 501/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00057434, Agent 1: 0.00043488] ; E(V) Avg: [Agent 0: 0.13177880, Agent 1: 0.07935628]) [Noise stdev: 0.12405933796571186]
Episode 502/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00049906, Agent 1: 0.00042870] ; E(V) Avg: [Agent 0: 0.13620583, Agent 1: 0.07956845]) [Noise stdev: 0.12381121928978044]
Episode 503/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00044897, Agent 1: 0.00041268] ; E(V) Avg: [Agent 0: 0.13410847, Agent 1: 0.07943015]) [Noise stdev: 0.12356359685120087]
Episode 504/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00050021, Agent 1: 0.00041215] ; E(V) Avg: [Agent 0: 0.13102842, Agent 1: 0.08017511]) [Noise stdev: 0.12331646965749847]
Episode 505/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00049773, Agent 1: 0.00057480] ; E(V) Avg: [Agent 0: 0.13222340, Agent 1: 0.08306032]) [Noise stdev: 0.12306983671818347]
Episode 506/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045034, Agent 1: 0.00063508] ; E(V) Avg: [Agent 0: 0.13026788, Agent 1: 0.08458650]) [Noise stdev: 0.1228236970447471]
Episode 507/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00054966, Agent 1: 0.00054045] ; E(V) Avg: [Agent 0: 0.13320633, Agent 1: 0.08815506]) [Noise stdev: 0.1225780496506576]
Episode 508/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00051178, Agent 1: 0.00051791] ; E(V) Avg: [Agent 0: 0.13372574, Agent 1: 0.08643427]) [Noise stdev: 0.12233289355135629]
Episode 509/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00055669, Agent 1: 0.00053292] ; E(V) Avg: [Agent 0: 0.14087529, Agent 1: 0.08647468]) [Noise stdev: 0.12208822776425358]
Episode 510/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00058158, Agent 1: 0.00057471] ; E(V) Avg: [Agent 0: 0.12982302, Agent 1: 0.09046341]) [Noise stdev: 0.12184405130872508]
Episode 511/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00061917, Agent 1: 0.00051523] ; E(V) Avg: [Agent 0: 0.13088570, Agent 1: 0.08793810]) [Noise stdev: 0.12160036320610763]
Episode 512/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00062551, Agent 1: 0.00049312] ; E(V) Avg: [Agent 0: 0.13692945, Agent 1: 0.09221861]) [Noise stdev: 0.12135716247969541]
Episode 513/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00057156, Agent 1: 0.00050219] ; E(V) Avg: [Agent 0: 0.13885941, Agent 1: 0.08548208]) [Noise stdev: 0.12111444815473602]
Episode 514/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00053062, Agent 1: 0.00048551] ; E(V) Avg: [Agent 0: 0.13001925, Agent 1: 0.08646168]) [Noise stdev: 0.12087221925842655]
Episode 515/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00052685, Agent 1: 0.00049100] ; E(V) Avg: [Agent 0: 0.13621566, Agent 1: 0.08623130]) [Noise stdev: 0.1206304748199097]
Episode 516/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00046525, Agent 1: 0.00047254] ; E(V) Avg: [Agent 0: 0.13408172, Agent 1: 0.08470962]) [Noise stdev: 0.12038921387026988]
Episode 517/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00050836, Agent 1: 0.00052620] ; E(V) Avg: [Agent 0: 0.13713383, Agent 1: 0.09126576]) [Noise stdev: 0.12014843544252934]
Episode 518/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0030 ; (Q Loss Avg: [Agent 0: 0.00051209, Agent 1: 0.00056874] ; E(V) Avg: [Agent 0: 0.13378401, Agent 1: 0.09017958]) [Noise stdev: 0.11990813857164428]
Episode 519/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00051812, Agent 1: 0.00056421] ; E(V) Avg: [Agent 0: 0.13800108, Agent 1: 0.09163770]) [Noise stdev: 0.119668322294501]
Episode 520/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00050586, Agent 1: 0.00057080] ; E(V) Avg: [Agent 0: 0.13448903, Agent 1: 0.09079927]) [Noise stdev: 0.119428985649912]
Episode 521/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00050207, Agent 1: 0.00059901] ; E(V) Avg: [Agent 0: 0.13815750, Agent 1: 0.09133290]) [Noise stdev: 0.11919012767861217]
Episode 522/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00058043, Agent 1: 0.00072618] ; E(V) Avg: [Agent 0: 0.14281091, Agent 1: 0.09048197]) [Noise stdev: 0.11895174742325496]
Episode 523/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00051571, Agent 1: 0.00063606] ; E(V) Avg: [Agent 0: 0.14169238, Agent 1: 0.08780337]) [Noise stdev: 0.11871384392840845]
Episode 524/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00047053, Agent 1: 0.00055426] ; E(V) Avg: [Agent 0: 0.13150228, Agent 1: 0.09756995]) [Noise stdev: 0.11847641624055164]
Episode 525/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00053713, Agent 1: 0.00060585] ; E(V) Avg: [Agent 0: 0.14478675, Agent 1: 0.08866475]) [Noise stdev: 0.11823946340807054]
Episode 526/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00053758, Agent 1: 0.00059620] ; E(V) Avg: [Agent 0: 0.14107914, Agent 1: 0.09615374]) [Noise stdev: 0.1180029844812544]
Episode 527/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00052544, Agent 1: 0.00057392] ; E(V) Avg: [Agent 0: 0.14165604, Agent 1: 0.08907061]) [Noise stdev: 0.1177669785122919]
Episode 528/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00048007, Agent 1: 0.00059785] ; E(V) Avg: [Agent 0: 0.14462883, Agent 1: 0.09468314]) [Noise stdev: 0.11753144455526732]
Episode 529/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00049504, Agent 1: 0.00051770] ; E(V) Avg: [Agent 0: 0.14031153, Agent 1: 0.09380911]) [Noise stdev: 0.11729638166615679]
Episode 530/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00045677, Agent 1: 0.00045138] ; E(V) Avg: [Agent 0: 0.13433670, Agent 1: 0.09407387]) [Noise stdev: 0.11706178890282447]
Episode 531/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00047629, Agent 1: 0.00047072] ; E(V) Avg: [Agent 0: 0.13635191, Agent 1: 0.09022434]) [Noise stdev: 0.11682766532501881]
Episode 532/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00051832, Agent 1: 0.00053075] ; E(V) Avg: [Agent 0: 0.14512458, Agent 1: 0.09772646]) [Noise stdev: 0.11659400999436878]
Episode 533/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00051588, Agent 1: 0.00052180] ; E(V) Avg: [Agent 0: 0.14188205, Agent 1: 0.10045977]) [Noise stdev: 0.11636082197438004]
Episode 534/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00047434, Agent 1: 0.00044826] ; E(V) Avg: [Agent 0: 0.14441458, Agent 1: 0.09479951]) [Noise stdev: 0.11612810033043128]
Episode 535/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00047953, Agent 1: 0.00045861] ; E(V) Avg: [Agent 0: 0.14551507, Agent 1: 0.09504936]) [Noise stdev: 0.11589584412977041]
Episode 536/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00047219, Agent 1: 0.00040016] ; E(V) Avg: [Agent 0: 0.14457568, Agent 1: 0.09740159]) [Noise stdev: 0.11566405244151087]
Episode 537/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00044149, Agent 1: 0.00047077] ; E(V) Avg: [Agent 0: 0.14835293, Agent 1: 0.10171490]) [Noise stdev: 0.11543272433662785]
Episode 538/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00041069, Agent 1: 0.00044140] ; E(V) Avg: [Agent 0: 0.14239287, Agent 1: 0.09645450]) [Noise stdev: 0.11520185888795459]
Episode 539/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00044505, Agent 1: 0.00042594] ; E(V) Avg: [Agent 0: 0.15138140, Agent 1: 0.09725633]) [Noise stdev: 0.11497145517017868]
Episode 540/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00043576, Agent 1: 0.00040119] ; E(V) Avg: [Agent 0: 0.12908708, Agent 1: 0.09613675]) [Noise stdev: 0.11474151225983832]
Episode 541/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00055236, Agent 1: 0.00040567] ; E(V) Avg: [Agent 0: 0.12847233, Agent 1: 0.09861561]) [Noise stdev: 0.11451202923531864]
Episode 542/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00051789, Agent 1: 0.00050110] ; E(V) Avg: [Agent 0: 0.13342049, Agent 1: 0.10273269]) [Noise stdev: 0.114283005176848]
Episode 543/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00048400, Agent 1: 0.00048373] ; E(V) Avg: [Agent 0: 0.14313710, Agent 1: 0.09034481]) [Noise stdev: 0.11405443916649431]
Episode 544/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00048410, Agent 1: 0.00045312] ; E(V) Avg: [Agent 0: 0.13645410, Agent 1: 0.08981118]) [Noise stdev: 0.11382633028816132]
Episode 545/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00044228, Agent 1: 0.00041779] ; E(V) Avg: [Agent 0: 0.13437563, Agent 1: 0.09076228]) [Noise stdev: 0.11359867762758499]
Episode 546/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00041643, Agent 1: 0.00048905] ; E(V) Avg: [Agent 0: 0.14106799, Agent 1: 0.09853704]) [Noise stdev: 0.11337148027232982]
Episode 547/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00038900, Agent 1: 0.00047604] ; E(V) Avg: [Agent 0: 0.13290729, Agent 1: 0.09986138]) [Noise stdev: 0.11314473731178516]
Episode 548/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00044336, Agent 1: 0.00042222] ; E(V) Avg: [Agent 0: 0.13962246, Agent 1: 0.10027708]) [Noise stdev: 0.1129184478371616]
Episode 549/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00040052, Agent 1: 0.00048072] ; E(V) Avg: [Agent 0: 0.13601158, Agent 1: 0.09517479]) [Noise stdev: 0.11269261094148728]
Episode 550/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00037250, Agent 1: 0.00053210] ; E(V) Avg: [Agent 0: 0.14070494, Agent 1: 0.09467308]) [Noise stdev: 0.1124672257196043]
Episode 551/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00037576, Agent 1: 0.00048520] ; E(V) Avg: [Agent 0: 0.13279217, Agent 1: 0.09531820]) [Noise stdev: 0.11224229126816508]
Episode 552/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00036168, Agent 1: 0.00044167] ; E(V) Avg: [Agent 0: 0.13345449, Agent 1: 0.09616662]) [Noise stdev: 0.11201780668562875]
Episode 553/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00033531, Agent 1: 0.00044946] ; E(V) Avg: [Agent 0: 0.13580188, Agent 1: 0.09499771]) [Noise stdev: 0.11179377107225749]
Episode 554/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00034902, Agent 1: 0.00049681] ; E(V) Avg: [Agent 0: 0.13867713, Agent 1: 0.09983102]) [Noise stdev: 0.11157018353011297]
Episode 555/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00033181, Agent 1: 0.00047247] ; E(V) Avg: [Agent 0: 0.13938483, Agent 1: 0.10024707]) [Noise stdev: 0.11134704316305274]
Episode 556/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00040881, Agent 1: 0.00043012] ; E(V) Avg: [Agent 0: 0.13894075, Agent 1: 0.09280380]) [Noise stdev: 0.11112434907672664]
Episode 557/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00046478, Agent 1: 0.00050950] ; E(V) Avg: [Agent 0: 0.14207418, Agent 1: 0.09968633]) [Noise stdev: 0.11090210037857318]
Episode 558/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00040074, Agent 1: 0.00049762] ; E(V) Avg: [Agent 0: 0.13809121, Agent 1: 0.10152888]) [Noise stdev: 0.11068029617781602]
Episode 559/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00047352, Agent 1: 0.00053602] ; E(V) Avg: [Agent 0: 0.14321164, Agent 1: 0.10842235]) [Noise stdev: 0.1104589355854604]
Episode 560/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00040994, Agent 1: 0.00052068] ; E(V) Avg: [Agent 0: 0.14171781, Agent 1: 0.10971766]) [Noise stdev: 0.11023801771428948]
Episode 561/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00044318, Agent 1: 0.00050675] ; E(V) Avg: [Agent 0: 0.14264731, Agent 1: 0.10312703]) [Noise stdev: 0.11001754167886091]
Episode 562/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00038015, Agent 1: 0.00053367] ; E(V) Avg: [Agent 0: 0.13985326, Agent 1: 0.10705004]) [Noise stdev: 0.10979750659550319]
Episode 563/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00034829, Agent 1: 0.00045626] ; E(V) Avg: [Agent 0: 0.14238520, Agent 1: 0.10877647]) [Noise stdev: 0.10957791158231218]
Episode 564/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00038029, Agent 1: 0.00050605] ; E(V) Avg: [Agent 0: 0.15259667, Agent 1: 0.11207497]) [Noise stdev: 0.10935875575914757]
Episode 565/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00039003, Agent 1: 0.00043230] ; E(V) Avg: [Agent 0: 0.15377969, Agent 1: 0.10777214]) [Noise stdev: 0.10914003824762927]
Episode 566/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00037559, Agent 1: 0.00053127] ; E(V) Avg: [Agent 0: 0.14926116, Agent 1: 0.10561740]) [Noise stdev: 0.10892175817113402]
Episode 567/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00035027, Agent 1: 0.00047958] ; E(V) Avg: [Agent 0: 0.14793829, Agent 1: 0.10977447]) [Noise stdev: 0.10870391465479175]
Episode 568/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00042826, Agent 1: 0.00045227] ; E(V) Avg: [Agent 0: 0.15412207, Agent 1: 0.10943828]) [Noise stdev: 0.10848650682548217]
Episode 569/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00037324, Agent 1: 0.00053709] ; E(V) Avg: [Agent 0: 0.15285102, Agent 1: 0.10803777]) [Noise stdev: 0.1082695338118312]
Episode 570/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00041637, Agent 1: 0.00045980] ; E(V) Avg: [Agent 0: 0.15802434, Agent 1: 0.11357157]) [Noise stdev: 0.10805299474420754]
Episode 571/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00041578, Agent 1: 0.00048839] ; E(V) Avg: [Agent 0: 0.15591100, Agent 1: 0.11054976]) [Noise stdev: 0.10783688875471913]
Episode 572/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00038504, Agent 1: 0.00054390] ; E(V) Avg: [Agent 0: 0.15567943, Agent 1: 0.10421579]) [Noise stdev: 0.10762121497720968]
Episode 573/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00042542, Agent 1: 0.00052079] ; E(V) Avg: [Agent 0: 0.15070077, Agent 1: 0.10671450]) [Noise stdev: 0.10740597254725526]
Episode 574/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00047440, Agent 1: 0.00047903] ; E(V) Avg: [Agent 0: 0.15406615, Agent 1: 0.11165727]) [Noise stdev: 0.10719116060216075]
Episode 575/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00042347, Agent 1: 0.00045289] ; E(V) Avg: [Agent 0: 0.15042062, Agent 1: 0.11118828]) [Noise stdev: 0.10697677828095642]
Episode 576/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00035475, Agent 1: 0.00048657] ; E(V) Avg: [Agent 0: 0.14933335, Agent 1: 0.11434541]) [Noise stdev: 0.10676282472439451]
Episode 577/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00041817, Agent 1: 0.00045329] ; E(V) Avg: [Agent 0: 0.15263906, Agent 1: 0.11102276]) [Noise stdev: 0.10654929907494572]
Episode 578/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00041720, Agent 1: 0.00042740] ; E(V) Avg: [Agent 0: 0.15258235, Agent 1: 0.10295403]) [Noise stdev: 0.10633620047679583]
Episode 579/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00030567, Agent 1: 0.00042255] ; E(V) Avg: [Agent 0: 0.14802691, Agent 1: 0.11159863]) [Noise stdev: 0.10612352807584224]
Episode 580/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00034850, Agent 1: 0.00041871] ; E(V) Avg: [Agent 0: 0.15241937, Agent 1: 0.11431140]) [Noise stdev: 0.10591128101969056]
Episode 581/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00037154, Agent 1: 0.00036662] ; E(V) Avg: [Agent 0: 0.14825666, Agent 1: 0.11281452]) [Noise stdev: 0.10569945845765118]
Episode 582/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00040526, Agent 1: 0.00037747] ; E(V) Avg: [Agent 0: 0.14643215, Agent 1: 0.11289569]) [Noise stdev: 0.10548805954073587]
Episode 583/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00032690, Agent 1: 0.00039552] ; E(V) Avg: [Agent 0: 0.14598627, Agent 1: 0.10526294]) [Noise stdev: 0.1052770834216544]
Episode 584/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00034743, Agent 1: 0.00038957] ; E(V) Avg: [Agent 0: 0.14564933, Agent 1: 0.10263660]) [Noise stdev: 0.1050665292548111]
Episode 585/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00034756, Agent 1: 0.00036746] ; E(V) Avg: [Agent 0: 0.15619534, Agent 1: 0.09931779]) [Noise stdev: 0.10485639619630147]
Episode 586/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00031233, Agent 1: 0.00036304] ; E(V) Avg: [Agent 0: 0.16184841, Agent 1: 0.10054755]) [Noise stdev: 0.10464668340390887]
Episode 587/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00032656, Agent 1: 0.00034668] ; E(V) Avg: [Agent 0: 0.14794609, Agent 1: 0.09798597]) [Noise stdev: 0.10443739003710105]
Episode 588/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00036670, Agent 1: 0.00035907] ; E(V) Avg: [Agent 0: 0.15090067, Agent 1: 0.10598795]) [Noise stdev: 0.10422851525702685]
Episode 589/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00034001, Agent 1: 0.00041935] ; E(V) Avg: [Agent 0: 0.15171202, Agent 1: 0.10162372]) [Noise stdev: 0.1040200582265128]
Episode 590/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00028286, Agent 1: 0.00039360] ; E(V) Avg: [Agent 0: 0.15851864, Agent 1: 0.10657181]) [Noise stdev: 0.10381201811005977]
Episode 591/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00033853, Agent 1: 0.00040740] ; E(V) Avg: [Agent 0: 0.15333177, Agent 1: 0.10525723]) [Noise stdev: 0.10360439407383966]
Episode 592/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00039113, Agent 1: 0.00041555] ; E(V) Avg: [Agent 0: 0.15275194, Agent 1: 0.10386493]) [Noise stdev: 0.10339718528569197]
Episode 593/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00040543, Agent 1: 0.00045869] ; E(V) Avg: [Agent 0: 0.14794271, Agent 1: 0.10432080]) [Noise stdev: 0.10319039091512058]
Episode 594/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00033576, Agent 1: 0.00043591] ; E(V) Avg: [Agent 0: 0.14698799, Agent 1: 0.09786336]) [Noise stdev: 0.10298401013329034]
Episode 595/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00032116, Agent 1: 0.00039845] ; E(V) Avg: [Agent 0: 0.15090578, Agent 1: 0.10441646]) [Noise stdev: 0.10277804211302376]
Episode 596/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00039114, Agent 1: 0.00038598] ; E(V) Avg: [Agent 0: 0.15440213, Agent 1: 0.10438120]) [Noise stdev: 0.1025724860287977]
Episode 597/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00039675, Agent 1: 0.00038040] ; E(V) Avg: [Agent 0: 0.14977752, Agent 1: 0.09968779]) [Noise stdev: 0.10236734105674011]
Episode 598/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00043956, Agent 1: 0.00041010] ; E(V) Avg: [Agent 0: 0.15642529, Agent 1: 0.10049272]) [Noise stdev: 0.10216260637462664]
Episode 599/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00043014, Agent 1: 0.00047774] ; E(V) Avg: [Agent 0: 0.14799910, Agent 1: 0.10378766]) [Noise stdev: 0.10195828116187738]
Episode 600/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00033974, Agent 1: 0.00048741] ; E(V) Avg: [Agent 0: 0.15316436, Agent 1: 0.10452352]) [Noise stdev: 0.10175436459955363]
Episode 601/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00032511, Agent 1: 0.00047468] ; E(V) Avg: [Agent 0: 0.15146739, Agent 1: 0.10339699]) [Noise stdev: 0.10155085587035452]
Episode 602/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00031895, Agent 1: 0.00045999] ; E(V) Avg: [Agent 0: 0.15793205, Agent 1: 0.10491850]) [Noise stdev: 0.10134775415861381]
Episode 603/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00031634, Agent 1: 0.00044779] ; E(V) Avg: [Agent 0: 0.15489289, Agent 1: 0.10832359]) [Noise stdev: 0.10114505865029659]
Episode 604/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00030450, Agent 1: 0.00053199] ; E(V) Avg: [Agent 0: 0.14860038, Agent 1: 0.10602872]) [Noise stdev: 0.100942768532996]
Episode 605/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00032601, Agent 1: 0.00050329] ; E(V) Avg: [Agent 0: 0.15349624, Agent 1: 0.10653168]) [Noise stdev: 0.10074088299593001]
Episode 606/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00038536, Agent 1: 0.00047949] ; E(V) Avg: [Agent 0: 0.15598004, Agent 1: 0.10445674]) [Noise stdev: 0.10053940122993815]
Episode 607/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00038598, Agent 1: 0.00042273] ; E(V) Avg: [Agent 0: 0.14968763, Agent 1: 0.10462101]) [Noise stdev: 0.10033832242747828]
Episode 608/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00033529, Agent 1: 0.00041942] ; E(V) Avg: [Agent 0: 0.15028600, Agent 1: 0.10894009]) [Noise stdev: 0.10013764578262332]
Episode 609/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00033844, Agent 1: 0.00046325] ; E(V) Avg: [Agent 0: 0.14970803, Agent 1: 0.10748878]) [Noise stdev: 0.09993737049105808]
Episode 610/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00034058, Agent 1: 0.00041654] ; E(V) Avg: [Agent 0: 0.15565257, Agent 1: 0.10734856]) [Noise stdev: 0.09973749575007597]
Episode 611/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00035139, Agent 1: 0.00035355] ; E(V) Avg: [Agent 0: 0.15021620, Agent 1: 0.10426588]) [Noise stdev: 0.09953802075857582]
Episode 612/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00035537, Agent 1: 0.00033030] ; E(V) Avg: [Agent 0: 0.15423105, Agent 1: 0.10656704]) [Noise stdev: 0.09933894471705866]
Episode 613/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0030 ; (Q Loss Avg: [Agent 0: 0.00033798, Agent 1: 0.00036252] ; E(V) Avg: [Agent 0: 0.15129616, Agent 1: 0.10722302]) [Noise stdev: 0.09914026682762454]
Episode 614/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0030 ; (Q Loss Avg: [Agent 0: 0.00033273, Agent 1: 0.00038988] ; E(V) Avg: [Agent 0: 0.15387576, Agent 1: 0.10132703]) [Noise stdev: 0.09894198629396929]
Episode 615/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0030 ; (Q Loss Avg: [Agent 0: 0.00035302, Agent 1: 0.00035592] ; E(V) Avg: [Agent 0: 0.14950692, Agent 1: 0.10432340]) [Noise stdev: 0.09874410232138135]
Episode 616/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0030 ; (Q Loss Avg: [Agent 0: 0.00034725, Agent 1: 0.00036139] ; E(V) Avg: [Agent 0: 0.15700026, Agent 1: 0.10222910]) [Noise stdev: 0.0985466141167386]
Episode 617/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0030 ; (Q Loss Avg: [Agent 0: 0.00033023, Agent 1: 0.00036348] ; E(V) Avg: [Agent 0: 0.15710825, Agent 1: 0.10059604]) [Noise stdev: 0.09834952088850511]
Episode 618/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00037597, Agent 1: 0.00041737] ; E(V) Avg: [Agent 0: 0.14727974, Agent 1: 0.10224475]) [Noise stdev: 0.0981528218467281]
Episode 619/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00035037, Agent 1: 0.00037172] ; E(V) Avg: [Agent 0: 0.14138818, Agent 1: 0.10127315]) [Noise stdev: 0.09795651620303465]
Episode 620/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00041526, Agent 1: 0.00040600] ; E(V) Avg: [Agent 0: 0.14841514, Agent 1: 0.10925264]) [Noise stdev: 0.09776060317062858]
Episode 621/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00039286, Agent 1: 0.00043541] ; E(V) Avg: [Agent 0: 0.15087234, Agent 1: 0.11024056]) [Noise stdev: 0.09756508196428733]
Episode 622/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00033255, Agent 1: 0.00045949] ; E(V) Avg: [Agent 0: 0.15149828, Agent 1: 0.11084691]) [Noise stdev: 0.09736995180035875]
Episode 623/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00033218, Agent 1: 0.00047345] ; E(V) Avg: [Agent 0: 0.15051484, Agent 1: 0.10907237]) [Noise stdev: 0.09717521189675803]
Episode 624/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00036238, Agent 1: 0.00041458] ; E(V) Avg: [Agent 0: 0.14304155, Agent 1: 0.10887179]) [Noise stdev: 0.09698086147296452]
Episode 625/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00033971, Agent 1: 0.00041286] ; E(V) Avg: [Agent 0: 0.14847413, Agent 1: 0.11004737]) [Noise stdev: 0.09678689975001858]
Episode 626/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00035441, Agent 1: 0.00044035] ; E(V) Avg: [Agent 0: 0.14629862, Agent 1: 0.10639709]) [Noise stdev: 0.09659332595051855]
Episode 627/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00037488, Agent 1: 0.00042309] ; E(V) Avg: [Agent 0: 0.15140436, Agent 1: 0.11343356]) [Noise stdev: 0.09640013929861752]
Episode 628/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00040090, Agent 1: 0.00041831] ; E(V) Avg: [Agent 0: 0.15045658, Agent 1: 0.11092082]) [Noise stdev: 0.09620733902002028]
Episode 629/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037608, Agent 1: 0.00036553] ; E(V) Avg: [Agent 0: 0.15962359, Agent 1: 0.10808172]) [Noise stdev: 0.09601492434198024]
Episode 630/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031641, Agent 1: 0.00038944] ; E(V) Avg: [Agent 0: 0.15377201, Agent 1: 0.11350035]) [Noise stdev: 0.09582289449329627]
Episode 631/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031431, Agent 1: 0.00040346] ; E(V) Avg: [Agent 0: 0.15649801, Agent 1: 0.11189765]) [Noise stdev: 0.09563124870430968]
Episode 632/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031746, Agent 1: 0.00038256] ; E(V) Avg: [Agent 0: 0.15235649, Agent 1: 0.10910684]) [Noise stdev: 0.09543998620690106]
Episode 633/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00030845, Agent 1: 0.00040532] ; E(V) Avg: [Agent 0: 0.15216907, Agent 1: 0.11049382]) [Noise stdev: 0.09524910623448725]
Episode 634/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032022, Agent 1: 0.00037682] ; E(V) Avg: [Agent 0: 0.14759981, Agent 1: 0.11618186]) [Noise stdev: 0.09505860802201828]
Episode 635/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00028617, Agent 1: 0.00044180] ; E(V) Avg: [Agent 0: 0.15591770, Agent 1: 0.11070302]) [Noise stdev: 0.09486849080597425]
Episode 636/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00027207, Agent 1: 0.00040081] ; E(V) Avg: [Agent 0: 0.15424642, Agent 1: 0.11101917]) [Noise stdev: 0.09467875382436229]
Episode 637/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00029850, Agent 1: 0.00045736] ; E(V) Avg: [Agent 0: 0.16091806, Agent 1: 0.11278459]) [Noise stdev: 0.09448939631671356]
Episode 638/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00028890, Agent 1: 0.00039023] ; E(V) Avg: [Agent 0: 0.15085369, Agent 1: 0.10763348]) [Noise stdev: 0.09430041752408014]
Episode 639/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00027049, Agent 1: 0.00037297] ; E(V) Avg: [Agent 0: 0.15346436, Agent 1: 0.10909168]) [Noise stdev: 0.09411181668903197]
Episode 640/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00029120, Agent 1: 0.00036060] ; E(V) Avg: [Agent 0: 0.15757222, Agent 1: 0.10524918]) [Noise stdev: 0.09392359305565391]
Episode 641/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00027460, Agent 1: 0.00040530] ; E(V) Avg: [Agent 0: 0.16508487, Agent 1: 0.10246302]) [Noise stdev: 0.0937357458695426]
Episode 642/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00028540, Agent 1: 0.00036684] ; E(V) Avg: [Agent 0: 0.15965185, Agent 1: 0.10186770]) [Noise stdev: 0.09354827437780351]
Episode 643/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00024366, Agent 1: 0.00038099] ; E(V) Avg: [Agent 0: 0.15806395, Agent 1: 0.10458714]) [Noise stdev: 0.0933611778290479]
Episode 644/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032582, Agent 1: 0.00035614] ; E(V) Avg: [Agent 0: 0.15838158, Agent 1: 0.09697972]) [Noise stdev: 0.0931744554733898]
Episode 645/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031257, Agent 1: 0.00040037] ; E(V) Avg: [Agent 0: 0.15767326, Agent 1: 0.10509315]) [Noise stdev: 0.09298810656244302]
Episode 646/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032245, Agent 1: 0.00033376] ; E(V) Avg: [Agent 0: 0.16499327, Agent 1: 0.10581091]) [Noise stdev: 0.09280213034931814]
Episode 647/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00033280, Agent 1: 0.00032294] ; E(V) Avg: [Agent 0: 0.15753186, Agent 1: 0.10367744]) [Noise stdev: 0.0926165260886195]
Episode 648/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035006, Agent 1: 0.00033031] ; E(V) Avg: [Agent 0: 0.15729791, Agent 1: 0.10272431]) [Noise stdev: 0.09243129303644226]
Episode 649/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00033642, Agent 1: 0.00034726] ; E(V) Avg: [Agent 0: 0.15946628, Agent 1: 0.10570340]) [Noise stdev: 0.09224643045036938]
Episode 650/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032244, Agent 1: 0.00033761] ; E(V) Avg: [Agent 0: 0.15808432, Agent 1: 0.10654597]) [Noise stdev: 0.09206193758946865]
Episode 651/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00029849, Agent 1: 0.00030158] ; E(V) Avg: [Agent 0: 0.15676965, Agent 1: 0.10783475]) [Noise stdev: 0.09187781371428971]
Episode 652/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00027915, Agent 1: 0.00029847] ; E(V) Avg: [Agent 0: 0.15691207, Agent 1: 0.10805545]) [Noise stdev: 0.09169405808686114]
Episode 653/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00029935, Agent 1: 0.00031500] ; E(V) Avg: [Agent 0: 0.15625505, Agent 1: 0.10810726]) [Noise stdev: 0.09151066997068742]
Episode 654/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032499, Agent 1: 0.00032834] ; E(V) Avg: [Agent 0: 0.16315919, Agent 1: 0.10638053]) [Noise stdev: 0.09132764863074605]
Episode 655/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00031220, Agent 1: 0.00035058] ; E(V) Avg: [Agent 0: 0.15978824, Agent 1: 0.10767293]) [Noise stdev: 0.09114499333348455]
Episode 656/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034132, Agent 1: 0.00037250] ; E(V) Avg: [Agent 0: 0.15374066, Agent 1: 0.10841345]) [Noise stdev: 0.09096270334681758]
Episode 657/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00042023, Agent 1: 0.00040349] ; E(V) Avg: [Agent 0: 0.15441605, Agent 1: 0.10776457]) [Noise stdev: 0.09078077794012394]
Episode 658/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00042688, Agent 1: 0.00040163] ; E(V) Avg: [Agent 0: 0.15936869, Agent 1: 0.10582422]) [Noise stdev: 0.0905992163842437]
Episode 659/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038958, Agent 1: 0.00041003] ; E(V) Avg: [Agent 0: 0.15804154, Agent 1: 0.10827871]) [Noise stdev: 0.09041801795147522]
Episode 660/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038281, Agent 1: 0.00035336] ; E(V) Avg: [Agent 0: 0.15878602, Agent 1: 0.10610100]) [Noise stdev: 0.09023718191557227]
Episode 661/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00042818, Agent 1: 0.00033911] ; E(V) Avg: [Agent 0: 0.15807073, Agent 1: 0.10841756]) [Noise stdev: 0.09005670755174112]
Episode 662/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035369, Agent 1: 0.00030623] ; E(V) Avg: [Agent 0: 0.16004827, Agent 1: 0.10232651]) [Noise stdev: 0.08987659413663764]
Episode 663/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032792, Agent 1: 0.00029800] ; E(V) Avg: [Agent 0: 0.16091105, Agent 1: 0.10100972]) [Noise stdev: 0.08969684094836436]
Episode 664/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034498, Agent 1: 0.00032115] ; E(V) Avg: [Agent 0: 0.15584039, Agent 1: 0.10463597]) [Noise stdev: 0.08951744726646763]
Episode 665/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037174, Agent 1: 0.00032434] ; E(V) Avg: [Agent 0: 0.16047851, Agent 1: 0.10314493]) [Noise stdev: 0.08933841237193468]
Episode 666/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038583, Agent 1: 0.00034891] ; E(V) Avg: [Agent 0: 0.16322430, Agent 1: 0.10111349]) [Noise stdev: 0.08915973554719081]
Episode 667/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00042350, Agent 1: 0.00034155] ; E(V) Avg: [Agent 0: 0.16374651, Agent 1: 0.10823547]) [Noise stdev: 0.08898141607609643]
Episode 668/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037528, Agent 1: 0.00030854] ; E(V) Avg: [Agent 0: 0.15884026, Agent 1: 0.10435311]) [Noise stdev: 0.08880345324394423]
Episode 669/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035916, Agent 1: 0.00027782] ; E(V) Avg: [Agent 0: 0.16044298, Agent 1: 0.10639712]) [Noise stdev: 0.08862584633745633]
Episode 670/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00039702, Agent 1: 0.00030252] ; E(V) Avg: [Agent 0: 0.15526132, Agent 1: 0.10696641]) [Noise stdev: 0.08844859464478143]
Episode 671/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00044448, Agent 1: 0.00033105] ; E(V) Avg: [Agent 0: 0.16127498, Agent 1: 0.10442062]) [Noise stdev: 0.08827169745549186]
Episode 672/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034574, Agent 1: 0.00031247] ; E(V) Avg: [Agent 0: 0.15737877, Agent 1: 0.10446760]) [Noise stdev: 0.08809515406058088]
Episode 673/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038251, Agent 1: 0.00030170] ; E(V) Avg: [Agent 0: 0.15963429, Agent 1: 0.10290316]) [Noise stdev: 0.08791896375245972]
Episode 674/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00043469, Agent 1: 0.00027217] ; E(V) Avg: [Agent 0: 0.15342028, Agent 1: 0.10101865]) [Noise stdev: 0.0877431258249548]
Episode 675/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00045564, Agent 1: 0.00028293] ; E(V) Avg: [Agent 0: 0.15530965, Agent 1: 0.09927361]) [Noise stdev: 0.08756763957330489]
Episode 676/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00043855, Agent 1: 0.00028476] ; E(V) Avg: [Agent 0: 0.16087782, Agent 1: 0.10745602]) [Noise stdev: 0.08739250429415828]
Episode 677/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00038634, Agent 1: 0.00034574] ; E(V) Avg: [Agent 0: 0.15773095, Agent 1: 0.10701657]) [Noise stdev: 0.08721771928556996]
Episode 678/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032182, Agent 1: 0.00031833] ; E(V) Avg: [Agent 0: 0.16866203, Agent 1: 0.10334585]) [Noise stdev: 0.08704328384699882]
Episode 679/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032321, Agent 1: 0.00026361] ; E(V) Avg: [Agent 0: 0.16248284, Agent 1: 0.10257728]) [Noise stdev: 0.08686919727930482]
Episode 680/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00036675, Agent 1: 0.00027714] ; E(V) Avg: [Agent 0: 0.16225504, Agent 1: 0.10301666]) [Noise stdev: 0.08669545888474621]
Episode 681/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00041054, Agent 1: 0.00035666] ; E(V) Avg: [Agent 0: 0.16296061, Agent 1: 0.10095534]) [Noise stdev: 0.08652206796697672]
Episode 682/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035631, Agent 1: 0.00043142] ; E(V) Avg: [Agent 0: 0.17000500, Agent 1: 0.10243376]) [Noise stdev: 0.08634902383104276]
Episode 683/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00036775, Agent 1: 0.00034609] ; E(V) Avg: [Agent 0: 0.15895665, Agent 1: 0.09994221]) [Noise stdev: 0.08617632578338068]
Episode 684/5000 (17 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00042954, Agent 1: 0.00035892] ; E(V) Avg: [Agent 0: 0.16260060, Agent 1: 0.09998802]) [Noise stdev: 0.08600397313181392]
Episode 685/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032047, Agent 1: 0.00035789] ; E(V) Avg: [Agent 0: 0.16983987, Agent 1: 0.10314720]) [Noise stdev: 0.0858319651855503]
Episode 686/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035609, Agent 1: 0.00034388] ; E(V) Avg: [Agent 0: 0.16559216, Agent 1: 0.10554928]) [Noise stdev: 0.08566030125517919]
Episode 687/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037734, Agent 1: 0.00032020] ; E(V) Avg: [Agent 0: 0.16292446, Agent 1: 0.09973715]) [Noise stdev: 0.08548898065266883]
Episode 688/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00043243, Agent 1: 0.00032578] ; E(V) Avg: [Agent 0: 0.17041389, Agent 1: 0.10302967]) [Noise stdev: 0.08531800269136348]
Episode 689/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00039388, Agent 1: 0.00032325] ; E(V) Avg: [Agent 0: 0.17256003, Agent 1: 0.10550592]) [Noise stdev: 0.08514736668598076]
Episode 690/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00035554, Agent 1: 0.00033213] ; E(V) Avg: [Agent 0: 0.16101893, Agent 1: 0.10253045]) [Noise stdev: 0.0849770719526088]
Episode 691/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037284, Agent 1: 0.00031596] ; E(V) Avg: [Agent 0: 0.16643823, Agent 1: 0.09944913]) [Noise stdev: 0.08480711780870358]
Episode 692/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034951, Agent 1: 0.00032483] ; E(V) Avg: [Agent 0: 0.16077641, Agent 1: 0.09988386]) [Noise stdev: 0.08463750357308616]
Episode 693/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032083, Agent 1: 0.00031795] ; E(V) Avg: [Agent 0: 0.16316954, Agent 1: 0.09869645]) [Noise stdev: 0.08446822856594]
Episode 694/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00040914, Agent 1: 0.00033294] ; E(V) Avg: [Agent 0: 0.16761967, Agent 1: 0.10180954]) [Noise stdev: 0.08429929210880811]
Episode 695/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00047015, Agent 1: 0.00029490] ; E(V) Avg: [Agent 0: 0.17479367, Agent 1: 0.10260962]) [Noise stdev: 0.0841306935245905]
Episode 696/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00037065, Agent 1: 0.00031516] ; E(V) Avg: [Agent 0: 0.16630126, Agent 1: 0.09895135]) [Noise stdev: 0.08396243213754132]
Episode 697/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034925, Agent 1: 0.00028976] ; E(V) Avg: [Agent 0: 0.16736305, Agent 1: 0.10261622]) [Noise stdev: 0.08379450727326623]
Episode 698/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00028782, Agent 1: 0.00030159] ; E(V) Avg: [Agent 0: 0.16982387, Agent 1: 0.10523259]) [Noise stdev: 0.0836269182587197]
Episode 699/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00034063, Agent 1: 0.00030458] ; E(V) Avg: [Agent 0: 0.17069870, Agent 1: 0.10084614]) [Noise stdev: 0.08345966442220226]
Episode 700/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0000 ; (Q Loss Avg: [Agent 0: 0.00032199, Agent 1: 0.00031912] ; E(V) Avg: [Agent 0: 0.16721442, Agent 1: 0.09359234]) [Noise stdev: 0.08329274509335785]
Episode 701/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00033537, Agent 1: 0.00031050] ; E(V) Avg: [Agent 0: 0.16824938, Agent 1: 0.09938393]) [Noise stdev: 0.08312615960317113]
Episode 702/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0010 ; (Q Loss Avg: [Agent 0: 0.00033676, Agent 1: 0.00033933] ; E(V) Avg: [Agent 0: 0.17446658, Agent 1: 0.09260871]) [Noise stdev: 0.08295990728396478]
Episode 703/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00041401, Agent 1: 0.00032849] ; E(V) Avg: [Agent 0: 0.16914965, Agent 1: 0.09593392]) [Noise stdev: 0.08279398746939685]
Episode 704/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00042719, Agent 1: 0.00033977] ; E(V) Avg: [Agent 0: 0.16888236, Agent 1: 0.09354358]) [Noise stdev: 0.08262839949445806]
Episode 705/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00038032, Agent 1: 0.00033428] ; E(V) Avg: [Agent 0: 0.16378309, Agent 1: 0.09657989]) [Noise stdev: 0.08246314269546914]
Episode 706/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0020 ; (Q Loss Avg: [Agent 0: 0.00034116, Agent 1: 0.00038651] ; E(V) Avg: [Agent 0: 0.16517547, Agent 1: 0.10173483]) [Noise stdev: 0.0822982164100782]
Episode 707/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0030 ; (Q Loss Avg: [Agent 0: 0.00041492, Agent 1: 0.00037870] ; E(V) Avg: [Agent 0: 0.16589639, Agent 1: 0.09702864]) [Noise stdev: 0.08213361997725804]
Episode 708/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0030 ; (Q Loss Avg: [Agent 0: 0.00040210, Agent 1: 0.00037703] ; E(V) Avg: [Agent 0: 0.17312529, Agent 1: 0.09787854]) [Noise stdev: 0.08196935273730353]
Episode 709/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00038814, Agent 1: 0.00042534] ; E(V) Avg: [Agent 0: 0.16842603, Agent 1: 0.09941470]) [Noise stdev: 0.08180541403182892]
Episode 710/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0040 ; (Q Loss Avg: [Agent 0: 0.00036369, Agent 1: 0.00042106] ; E(V) Avg: [Agent 0: 0.17060340, Agent 1: 0.09998168]) [Noise stdev: 0.08164180320376527]
Episode 711/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00039075, Agent 1: 0.00039455] ; E(V) Avg: [Agent 0: 0.16923747, Agent 1: 0.09967229]) [Noise stdev: 0.08147851959735773]
Episode 712/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0050 ; (Q Loss Avg: [Agent 0: 0.00035431, Agent 1: 0.00039632] ; E(V) Avg: [Agent 0: 0.17140658, Agent 1: 0.09648864]) [Noise stdev: 0.08131556255816301]
Episode 713/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0060 ; (Q Loss Avg: [Agent 0: 0.00042138, Agent 1: 0.00038132] ; E(V) Avg: [Agent 0: 0.17123137, Agent 1: 0.09965137]) [Noise stdev: 0.08115293143304668]
Episode 714/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0060 ; (Q Loss Avg: [Agent 0: 0.00054815, Agent 1: 0.00035771] ; E(V) Avg: [Agent 0: 0.16776485, Agent 1: 0.09762873]) [Noise stdev: 0.08099062557018058]
Episode 715/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0070 ; (Q Loss Avg: [Agent 0: 0.00049775, Agent 1: 0.00034840] ; E(V) Avg: [Agent 0: 0.16997977, Agent 1: 0.09757024]) [Noise stdev: 0.08082864431904022]
Episode 716/5000 (49 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0080 ; (Q Loss Avg: [Agent 0: 0.00045645, Agent 1: 0.00035718] ; E(V) Avg: [Agent 0: 0.16975715, Agent 1: 0.09733211]) [Noise stdev: 0.08066698703040213]
Episode 717/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0080 ; (Q Loss Avg: [Agent 0: 0.00050443, Agent 1: 0.00037100] ; E(V) Avg: [Agent 0: 0.16116097, Agent 1: 0.09578253]) [Noise stdev: 0.08050565305634133]
Episode 718/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0080 ; (Q Loss Avg: [Agent 0: 0.00051610, Agent 1: 0.00035548] ; E(V) Avg: [Agent 0: 0.16835581, Agent 1: 0.09890984]) [Noise stdev: 0.08034464175022865]
Episode 719/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0080 ; (Q Loss Avg: [Agent 0: 0.00044500, Agent 1: 0.00038692] ; E(V) Avg: [Agent 0: 0.17294298, Agent 1: 0.09737834]) [Noise stdev: 0.0801839524667282]
Episode 720/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0089 ; (Q Loss Avg: [Agent 0: 0.00041315, Agent 1: 0.00034499] ; E(V) Avg: [Agent 0: 0.16953386, Agent 1: 0.09737273]) [Noise stdev: 0.08002358456179474]
Episode 721/5000 (49 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0099 ; (Q Loss Avg: [Agent 0: 0.00038720, Agent 1: 0.00034492] ; E(V) Avg: [Agent 0: 0.16611019, Agent 1: 0.09749492]) [Noise stdev: 0.07986353739267116]
Episode 722/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0099 ; (Q Loss Avg: [Agent 0: 0.00044708, Agent 1: 0.00033352] ; E(V) Avg: [Agent 0: 0.16855178, Agent 1: 0.09773125]) [Noise stdev: 0.07970381031788581]
Episode 723/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0109 ; (Q Loss Avg: [Agent 0: 0.00036443, Agent 1: 0.00035827] ; E(V) Avg: [Agent 0: 0.17013420, Agent 1: 0.09736290]) [Noise stdev: 0.07954440269725005]
Episode 724/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0109 ; (Q Loss Avg: [Agent 0: 0.00046808, Agent 1: 0.00038666] ; E(V) Avg: [Agent 0: 0.16305670, Agent 1: 0.09703427]) [Noise stdev: 0.07938531389185555]
Episode 725/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0119 ; (Q Loss Avg: [Agent 0: 0.00047404, Agent 1: 0.00039264] ; E(V) Avg: [Agent 0: 0.17052074, Agent 1: 0.09495020]) [Noise stdev: 0.07922654326407183]
Episode 726/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0119 ; (Q Loss Avg: [Agent 0: 0.00041933, Agent 1: 0.00034710] ; E(V) Avg: [Agent 0: 0.16766596, Agent 1: 0.09576449]) [Noise stdev: 0.07906809017754368]
Episode 727/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0129 ; (Q Loss Avg: [Agent 0: 0.00035567, Agent 1: 0.00036009] ; E(V) Avg: [Agent 0: 0.16886388, Agent 1: 0.09555151]) [Noise stdev: 0.0789099539971886]
Episode 728/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0129 ; (Q Loss Avg: [Agent 0: 0.00034629, Agent 1: 0.00033280] ; E(V) Avg: [Agent 0: 0.16799101, Agent 1: 0.09249695]) [Noise stdev: 0.07875213408919422]
Episode 729/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0129 ; (Q Loss Avg: [Agent 0: 0.00034394, Agent 1: 0.00036275] ; E(V) Avg: [Agent 0: 0.16921559, Agent 1: 0.09780586]) [Noise stdev: 0.07859462982101584]
Episode 730/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0139 ; (Q Loss Avg: [Agent 0: 0.00040067, Agent 1: 0.00037069] ; E(V) Avg: [Agent 0: 0.16860292, Agent 1: 0.09569683]) [Noise stdev: 0.0784374405613738]
Episode 731/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0139 ; (Q Loss Avg: [Agent 0: 0.00033635, Agent 1: 0.00038772] ; E(V) Avg: [Agent 0: 0.17501318, Agent 1: 0.09568646]) [Noise stdev: 0.07828056568025106]
Episode 732/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0139 ; (Q Loss Avg: [Agent 0: 0.00034433, Agent 1: 0.00040934] ; E(V) Avg: [Agent 0: 0.16972089, Agent 1: 0.09536398]) [Noise stdev: 0.07812400454889055]
Episode 733/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0139 ; (Q Loss Avg: [Agent 0: 0.00034480, Agent 1: 0.00039133] ; E(V) Avg: [Agent 0: 0.17778417, Agent 1: 0.09329923]) [Noise stdev: 0.07796775653979278]
Episode 734/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00033187, Agent 1: 0.00035810] ; E(V) Avg: [Agent 0: 0.16851692, Agent 1: 0.09320817]) [Noise stdev: 0.07781182102671319]
Episode 735/5000 (17 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00037367, Agent 1: 0.00036475] ; E(V) Avg: [Agent 0: 0.16912213, Agent 1: 0.09425043]) [Noise stdev: 0.07765619738465976]
Episode 736/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00036336, Agent 1: 0.00038138] ; E(V) Avg: [Agent 0: 0.16773701, Agent 1: 0.09396033]) [Noise stdev: 0.07750088498989044]
Episode 737/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00036156, Agent 1: 0.00045633] ; E(V) Avg: [Agent 0: 0.17250477, Agent 1: 0.09523945]) [Noise stdev: 0.07734588321991066]
Episode 738/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00038064, Agent 1: 0.00035122] ; E(V) Avg: [Agent 0: 0.17052129, Agent 1: 0.09351630]) [Noise stdev: 0.07719119145347084]
Episode 739/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00034956, Agent 1: 0.00033128] ; E(V) Avg: [Agent 0: 0.16970893, Agent 1: 0.09584757]) [Noise stdev: 0.0770368090705639]
Episode 740/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00036808, Agent 1: 0.00034066] ; E(V) Avg: [Agent 0: 0.17085458, Agent 1: 0.09576161]) [Noise stdev: 0.07688273545242277]
Episode 741/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00035927, Agent 1: 0.00036575] ; E(V) Avg: [Agent 0: 0.16510916, Agent 1: 0.09698730]) [Noise stdev: 0.07672896998151793]
Episode 742/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00040048, Agent 1: 0.00034674] ; E(V) Avg: [Agent 0: 0.17132810, Agent 1: 0.09172806]) [Noise stdev: 0.0765755120415549]
Episode 743/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00044082, Agent 1: 0.00036006] ; E(V) Avg: [Agent 0: 0.16720985, Agent 1: 0.09804883]) [Noise stdev: 0.07642236101747178]
Episode 744/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00039278, Agent 1: 0.00039434] ; E(V) Avg: [Agent 0: 0.17091685, Agent 1: 0.09346210]) [Noise stdev: 0.07626951629543684]
Episode 745/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00036262, Agent 1: 0.00044478] ; E(V) Avg: [Agent 0: 0.16902833, Agent 1: 0.09455621]) [Noise stdev: 0.07611697726284597]
Episode 746/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00038480, Agent 1: 0.00041069] ; E(V) Avg: [Agent 0: 0.16895108, Agent 1: 0.09074245]) [Noise stdev: 0.07596474330832027]
Episode 747/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00042116, Agent 1: 0.00038427] ; E(V) Avg: [Agent 0: 0.17137336, Agent 1: 0.09589185]) [Noise stdev: 0.07581281382170363]
Episode 748/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00049121, Agent 1: 0.00041041] ; E(V) Avg: [Agent 0: 0.17363872, Agent 1: 0.09389180]) [Noise stdev: 0.07566118819406022]
Episode 749/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00052069, Agent 1: 0.00036449] ; E(V) Avg: [Agent 0: 0.17146497, Agent 1: 0.09031486]) [Noise stdev: 0.07550986581767209]
Episode 750/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00042703, Agent 1: 0.00037598] ; E(V) Avg: [Agent 0: 0.16648222, Agent 1: 0.09156028]) [Noise stdev: 0.07535884608603675]
Episode 751/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00042850, Agent 1: 0.00033371] ; E(V) Avg: [Agent 0: 0.17033680, Agent 1: 0.09138370]) [Noise stdev: 0.07520812839386468]
Episode 752/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00037461, Agent 1: 0.00031849] ; E(V) Avg: [Agent 0: 0.17295312, Agent 1: 0.09636978]) [Noise stdev: 0.07505771213707695]
Episode 753/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0149 ; (Q Loss Avg: [Agent 0: 0.00033416, Agent 1: 0.00029482] ; E(V) Avg: [Agent 0: 0.17093115, Agent 1: 0.09088471]) [Noise stdev: 0.0749075967128028]
Episode 754/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00034683, Agent 1: 0.00032073] ; E(V) Avg: [Agent 0: 0.17169096, Agent 1: 0.09522600]) [Noise stdev: 0.07475778151937719]
Episode 755/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00035776, Agent 1: 0.00030401] ; E(V) Avg: [Agent 0: 0.16785335, Agent 1: 0.09345387]) [Noise stdev: 0.07460826595633843]
Episode 756/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00030534, Agent 1: 0.00034274] ; E(V) Avg: [Agent 0: 0.17031537, Agent 1: 0.09595388]) [Noise stdev: 0.07445904942442576]
Episode 757/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00030434, Agent 1: 0.00037225] ; E(V) Avg: [Agent 0: 0.16317159, Agent 1: 0.09613356]) [Noise stdev: 0.0743101313255769]
Episode 758/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00030333, Agent 1: 0.00037343] ; E(V) Avg: [Agent 0: 0.16867131, Agent 1: 0.09219286]) [Noise stdev: 0.07416151106292575]
Episode 759/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00032113, Agent 1: 0.00037183] ; E(V) Avg: [Agent 0: 0.17105898, Agent 1: 0.09394693]) [Noise stdev: 0.0740131880407999]
Episode 760/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00030428, Agent 1: 0.00034364] ; E(V) Avg: [Agent 0: 0.16599094, Agent 1: 0.09265998]) [Noise stdev: 0.07386516166471829]
Episode 761/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00032045, Agent 1: 0.00031239] ; E(V) Avg: [Agent 0: 0.16994165, Agent 1: 0.09403205]) [Noise stdev: 0.07371743134138886]
Episode 762/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00033897, Agent 1: 0.00032304] ; E(V) Avg: [Agent 0: 0.16741226, Agent 1: 0.09316949]) [Noise stdev: 0.07356999647870609]
Episode 763/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00033395, Agent 1: 0.00031339] ; E(V) Avg: [Agent 0: 0.17065675, Agent 1: 0.09198587]) [Noise stdev: 0.07342285648574867]
Episode 764/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00038504, Agent 1: 0.00030231] ; E(V) Avg: [Agent 0: 0.16783869, Agent 1: 0.09391325]) [Noise stdev: 0.07327601077277718]
Episode 765/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00042571, Agent 1: 0.00029745] ; E(V) Avg: [Agent 0: 0.16954669, Agent 1: 0.09113864]) [Noise stdev: 0.07312945875123163]
Episode 766/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00043384, Agent 1: 0.00036123] ; E(V) Avg: [Agent 0: 0.16825735, Agent 1: 0.09076068]) [Noise stdev: 0.07298319983372917]
Episode 767/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00039594, Agent 1: 0.00034553] ; E(V) Avg: [Agent 0: 0.17587072, Agent 1: 0.09554069]) [Noise stdev: 0.0728372334340617]
Episode 768/5000 (70 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0169 ; (Q Loss Avg: [Agent 0: 0.00038524, Agent 1: 0.00033751] ; E(V) Avg: [Agent 0: 0.16960843, Agent 1: 0.09295820]) [Noise stdev: 0.07269155896719358]
Episode 769/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0169 ; (Q Loss Avg: [Agent 0: 0.00030138, Agent 1: 0.00030282] ; E(V) Avg: [Agent 0: 0.16817547, Agent 1: 0.09330463]) [Noise stdev: 0.0725461758492592]
Episode 770/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0169 ; (Q Loss Avg: [Agent 0: 0.00033623, Agent 1: 0.00028961] ; E(V) Avg: [Agent 0: 0.16971953, Agent 1: 0.09181909]) [Noise stdev: 0.07240108349756068]
Episode 771/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0179 ; (Q Loss Avg: [Agent 0: 0.00034127, Agent 1: 0.00032826] ; E(V) Avg: [Agent 0: 0.16726585, Agent 1: 0.09330265]) [Noise stdev: 0.07225628133056555]
Episode 772/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0179 ; (Q Loss Avg: [Agent 0: 0.00032757, Agent 1: 0.00027811] ; E(V) Avg: [Agent 0: 0.16654016, Agent 1: 0.09323862]) [Noise stdev: 0.07211176876790443]
Episode 773/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0179 ; (Q Loss Avg: [Agent 0: 0.00031421, Agent 1: 0.00029780] ; E(V) Avg: [Agent 0: 0.16678066, Agent 1: 0.09320192]) [Noise stdev: 0.07196754523036862]
Episode 774/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0179 ; (Q Loss Avg: [Agent 0: 0.00035127, Agent 1: 0.00025908] ; E(V) Avg: [Agent 0: 0.16843393, Agent 1: 0.09408194]) [Noise stdev: 0.07182361013990789]
Episode 775/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00040921, Agent 1: 0.00028447] ; E(V) Avg: [Agent 0: 0.16822795, Agent 1: 0.09210873]) [Noise stdev: 0.07167996291962807]
Episode 776/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00036070, Agent 1: 0.00022896] ; E(V) Avg: [Agent 0: 0.17115816, Agent 1: 0.09008995]) [Noise stdev: 0.0715366029937888]
Episode 777/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00037560, Agent 1: 0.00028282] ; E(V) Avg: [Agent 0: 0.17681660, Agent 1: 0.08956628]) [Noise stdev: 0.07139352978780122]
Episode 778/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00036866, Agent 1: 0.00028925] ; E(V) Avg: [Agent 0: 0.17631176, Agent 1: 0.08924794]) [Noise stdev: 0.07125074272822562]
Episode 779/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00034361, Agent 1: 0.00026046] ; E(V) Avg: [Agent 0: 0.16834147, Agent 1: 0.08965850]) [Noise stdev: 0.07110824124276917]
Episode 780/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00037481, Agent 1: 0.00024308] ; E(V) Avg: [Agent 0: 0.16668218, Agent 1: 0.08778131]) [Noise stdev: 0.07096602476028363]
Episode 781/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00039400, Agent 1: 0.00028036] ; E(V) Avg: [Agent 0: 0.16964495, Agent 1: 0.09040262]) [Noise stdev: 0.07082409271076306]
Episode 782/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00037326, Agent 1: 0.00027039] ; E(V) Avg: [Agent 0: 0.16553324, Agent 1: 0.08973106]) [Noise stdev: 0.07068244452534155]
Episode 783/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00035062, Agent 1: 0.00033309] ; E(V) Avg: [Agent 0: 0.16755250, Agent 1: 0.09091578]) [Noise stdev: 0.07054107963629086]
Episode 784/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00033463, Agent 1: 0.00040373] ; E(V) Avg: [Agent 0: 0.16678370, Agent 1: 0.09343525]) [Noise stdev: 0.07039999747701828]
Episode 785/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00034333, Agent 1: 0.00031695] ; E(V) Avg: [Agent 0: 0.16893105, Agent 1: 0.09216298]) [Noise stdev: 0.07025919748206425]
Episode 786/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00033585, Agent 1: 0.00031324] ; E(V) Avg: [Agent 0: 0.17265696, Agent 1: 0.09186132]) [Noise stdev: 0.07011867908710012]
Episode 787/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00031121, Agent 1: 0.00034828] ; E(V) Avg: [Agent 0: 0.16371404, Agent 1: 0.09032211]) [Noise stdev: 0.06997844172892592]
Episode 788/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00032660, Agent 1: 0.00031630] ; E(V) Avg: [Agent 0: 0.17635514, Agent 1: 0.09209902]) [Noise stdev: 0.06983848484546808]
Episode 789/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00037108, Agent 1: 0.00032394] ; E(V) Avg: [Agent 0: 0.16551748, Agent 1: 0.09287392]) [Noise stdev: 0.06969880787577715]
Episode 790/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00034853, Agent 1: 0.00033422] ; E(V) Avg: [Agent 0: 0.17061575, Agent 1: 0.09501300]) [Noise stdev: 0.06955941026002559]
Episode 791/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00032315, Agent 1: 0.00037347] ; E(V) Avg: [Agent 0: 0.16808473, Agent 1: 0.09309355]) [Noise stdev: 0.06942029143950554]
Episode 792/5000 (20 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00033948, Agent 1: 0.00037407] ; E(V) Avg: [Agent 0: 0.17273952, Agent 1: 0.09124398]) [Noise stdev: 0.06928145085662653]
Episode 793/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00033910, Agent 1: 0.00030352] ; E(V) Avg: [Agent 0: 0.17207038, Agent 1: 0.09104535]) [Noise stdev: 0.06914288795491327]
Episode 794/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00043675, Agent 1: 0.00033550] ; E(V) Avg: [Agent 0: 0.17536002, Agent 1: 0.09004816]) [Noise stdev: 0.06900460217900345]
Episode 795/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00045905, Agent 1: 0.00034448] ; E(V) Avg: [Agent 0: 0.16264023, Agent 1: 0.09442380]) [Noise stdev: 0.06886659297464544]
Episode 796/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00048093, Agent 1: 0.00031995] ; E(V) Avg: [Agent 0: 0.16655078, Agent 1: 0.09138169]) [Noise stdev: 0.06872885978869615]
Episode 797/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00045291, Agent 1: 0.00030103] ; E(V) Avg: [Agent 0: 0.16371808, Agent 1: 0.09296138]) [Noise stdev: 0.06859140206911876]
Episode 798/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00042137, Agent 1: 0.00028425] ; E(V) Avg: [Agent 0: 0.17189678, Agent 1: 0.09257504]) [Noise stdev: 0.06845421926498052]
Episode 799/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00036401, Agent 1: 0.00026708] ; E(V) Avg: [Agent 0: 0.16736602, Agent 1: 0.09296332]) [Noise stdev: 0.06831731082645055]
Episode 800/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00035953, Agent 1: 0.00029661] ; E(V) Avg: [Agent 0: 0.16868581, Agent 1: 0.09352678]) [Noise stdev: 0.06818067620479765]
Episode 801/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00037384, Agent 1: 0.00034791] ; E(V) Avg: [Agent 0: 0.16853478, Agent 1: 0.09086800]) [Noise stdev: 0.06804431485238806]
Episode 802/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00036888, Agent 1: 0.00029353] ; E(V) Avg: [Agent 0: 0.16377312, Agent 1: 0.09274152]) [Noise stdev: 0.06790822622268328]
Episode 803/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0179 ; (Q Loss Avg: [Agent 0: 0.00036702, Agent 1: 0.00030049] ; E(V) Avg: [Agent 0: 0.16433623, Agent 1: 0.09207356]) [Noise stdev: 0.0677724097702379]
Episode 804/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0179 ; (Q Loss Avg: [Agent 0: 0.00034704, Agent 1: 0.00035680] ; E(V) Avg: [Agent 0: 0.16818362, Agent 1: 0.09310717]) [Noise stdev: 0.06763686495069743]
Episode 805/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0179 ; (Q Loss Avg: [Agent 0: 0.00034312, Agent 1: 0.00032293] ; E(V) Avg: [Agent 0: 0.17286988, Agent 1: 0.09431382]) [Noise stdev: 0.06750159122079603]
Episode 806/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00040348, Agent 1: 0.00027901] ; E(V) Avg: [Agent 0: 0.16900591, Agent 1: 0.09015313]) [Noise stdev: 0.06736658803835444]
Episode 807/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00036241, Agent 1: 0.00029725] ; E(V) Avg: [Agent 0: 0.16745420, Agent 1: 0.09368539]) [Noise stdev: 0.06723185486227773]
Episode 808/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0189 ; (Q Loss Avg: [Agent 0: 0.00035688, Agent 1: 0.00034063] ; E(V) Avg: [Agent 0: 0.16448077, Agent 1: 0.09169266]) [Noise stdev: 0.06709739115255317]
Episode 809/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0179 ; (Q Loss Avg: [Agent 0: 0.00036887, Agent 1: 0.00037419] ; E(V) Avg: [Agent 0: 0.17015909, Agent 1: 0.09371329]) [Noise stdev: 0.06696319637024807]
Episode 810/5000 (17 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0179 ; (Q Loss Avg: [Agent 0: 0.00037141, Agent 1: 0.00044194] ; E(V) Avg: [Agent 0: 0.16991104, Agent 1: 0.09151410]) [Noise stdev: 0.06682926997750757]
Episode 811/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0169 ; (Q Loss Avg: [Agent 0: 0.00036018, Agent 1: 0.00034419] ; E(V) Avg: [Agent 0: 0.16955010, Agent 1: 0.09021932]) [Noise stdev: 0.06669561143755255]
Episode 812/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0169 ; (Q Loss Avg: [Agent 0: 0.00032095, Agent 1: 0.00028361] ; E(V) Avg: [Agent 0: 0.16933338, Agent 1: 0.09273098]) [Noise stdev: 0.06656222021467745]
Episode 813/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0159 ; (Q Loss Avg: [Agent 0: 0.00032187, Agent 1: 0.00030085] ; E(V) Avg: [Agent 0: 0.16909595, Agent 1: 0.09524090]) [Noise stdev: 0.06642909577424809]
Episode 814/5000 (24 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0169 ; (Q Loss Avg: [Agent 0: 0.00032748, Agent 1: 0.00030646] ; E(V) Avg: [Agent 0: 0.16746423, Agent 1: 0.09073832]) [Noise stdev: 0.06629623758269959]
Episode 815/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0169 ; (Q Loss Avg: [Agent 0: 0.00039734, Agent 1: 0.00031565] ; E(V) Avg: [Agent 0: 0.16746229, Agent 1: 0.09245303]) [Noise stdev: 0.0661636451075342]
Episode 816/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0169 ; (Q Loss Avg: [Agent 0: 0.00046254, Agent 1: 0.00031161] ; E(V) Avg: [Agent 0: 0.17368078, Agent 1: 0.09373311]) [Noise stdev: 0.06603131781731914]
Episode 817/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0169 ; (Q Loss Avg: [Agent 0: 0.00034523, Agent 1: 0.00029596] ; E(V) Avg: [Agent 0: 0.17004952, Agent 1: 0.08901177]) [Noise stdev: 0.06589925518168449]
Episode 818/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0169 ; (Q Loss Avg: [Agent 0: 0.00054621, Agent 1: 0.00036283] ; E(V) Avg: [Agent 0: 0.17043964, Agent 1: 0.09064205]) [Noise stdev: 0.06576745667132113]
Episode 819/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0179 ; (Q Loss Avg: [Agent 0: 0.00046632, Agent 1: 0.00032822] ; E(V) Avg: [Agent 0: 0.16843673, Agent 1: 0.09205248]) [Noise stdev: 0.06563592175797849]
Episode 820/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00046934, Agent 1: 0.00031134] ; E(V) Avg: [Agent 0: 0.16447815, Agent 1: 0.09189021]) [Noise stdev: 0.06550464991446253]
Episode 821/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0160 ; (Q Loss Avg: [Agent 0: 0.00044529, Agent 1: 0.00031548] ; E(V) Avg: [Agent 0: 0.17264078, Agent 1: 0.08908489]) [Noise stdev: 0.0653736406146336]
Episode 822/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0160 ; (Q Loss Avg: [Agent 0: 0.00042905, Agent 1: 0.00030932] ; E(V) Avg: [Agent 0: 0.17057441, Agent 1: 0.09167265]) [Noise stdev: 0.06524289333340434]
Episode 823/5000 (36 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0150 ; (Q Loss Avg: [Agent 0: 0.00039782, Agent 1: 0.00032511] ; E(V) Avg: [Agent 0: 0.17225628, Agent 1: 0.09169907]) [Noise stdev: 0.06511240754673753]
Episode 824/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0150 ; (Q Loss Avg: [Agent 0: 0.00041030, Agent 1: 0.00033423] ; E(V) Avg: [Agent 0: 0.16953377, Agent 1: 0.09403865]) [Noise stdev: 0.06498218273164405]
Episode 825/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0140 ; (Q Loss Avg: [Agent 0: 0.00035124, Agent 1: 0.00031745] ; E(V) Avg: [Agent 0: 0.17191681, Agent 1: 0.09231997]) [Noise stdev: 0.06485221836618076]
Episode 826/5000 (24 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0150 ; (Q Loss Avg: [Agent 0: 0.00036916, Agent 1: 0.00041802] ; E(V) Avg: [Agent 0: 0.17114706, Agent 1: 0.09173645]) [Noise stdev: 0.06472251392944839]
Episode 827/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0140 ; (Q Loss Avg: [Agent 0: 0.00039684, Agent 1: 0.00039485] ; E(V) Avg: [Agent 0: 0.17424509, Agent 1: 0.09313682]) [Noise stdev: 0.06459306890158949]
Episode 828/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0150 ; (Q Loss Avg: [Agent 0: 0.00041346, Agent 1: 0.00032966] ; E(V) Avg: [Agent 0: 0.17089229, Agent 1: 0.09253047]) [Noise stdev: 0.06446388276378631]
Episode 829/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0150 ; (Q Loss Avg: [Agent 0: 0.00043247, Agent 1: 0.00031616] ; E(V) Avg: [Agent 0: 0.16974677, Agent 1: 0.09564833]) [Noise stdev: 0.06433495499825874]
Episode 830/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0140 ; (Q Loss Avg: [Agent 0: 0.00039030, Agent 1: 0.00032164] ; E(V) Avg: [Agent 0: 0.17218239, Agent 1: 0.09079708]) [Noise stdev: 0.06420628508826222]
Episode 831/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0140 ; (Q Loss Avg: [Agent 0: 0.00041127, Agent 1: 0.00033787] ; E(V) Avg: [Agent 0: 0.16660441, Agent 1: 0.08754314]) [Noise stdev: 0.0640778725180857]
Episode 832/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0140 ; (Q Loss Avg: [Agent 0: 0.00041991, Agent 1: 0.00043595] ; E(V) Avg: [Agent 0: 0.16957073, Agent 1: 0.09181589]) [Noise stdev: 0.06394971677304952]
Episode 833/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0140 ; (Q Loss Avg: [Agent 0: 0.00040125, Agent 1: 0.00036940] ; E(V) Avg: [Agent 0: 0.17108422, Agent 1: 0.09610275]) [Noise stdev: 0.06382181733950341]
Episode 834/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0130 ; (Q Loss Avg: [Agent 0: 0.00038933, Agent 1: 0.00034984] ; E(V) Avg: [Agent 0: 0.17948952, Agent 1: 0.09420077]) [Noise stdev: 0.06369417370482441]
Episode 835/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0130 ; (Q Loss Avg: [Agent 0: 0.00035924, Agent 1: 0.00037543] ; E(V) Avg: [Agent 0: 0.17577261, Agent 1: 0.09555181]) [Noise stdev: 0.06356678535741477]
Episode 836/5000 (30 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0130 ; (Q Loss Avg: [Agent 0: 0.00034102, Agent 1: 0.00033385] ; E(V) Avg: [Agent 0: 0.17431925, Agent 1: 0.09389243]) [Noise stdev: 0.06343965178669994]
Episode 837/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0130 ; (Q Loss Avg: [Agent 0: 0.00037706, Agent 1: 0.00032757] ; E(V) Avg: [Agent 0: 0.16844338, Agent 1: 0.08957167]) [Noise stdev: 0.06331277248312654]
Episode 838/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0140 ; (Q Loss Avg: [Agent 0: 0.00036272, Agent 1: 0.00030463] ; E(V) Avg: [Agent 0: 0.17426726, Agent 1: 0.09006075]) [Noise stdev: 0.06318614693816028]
Episode 839/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0140 ; (Q Loss Avg: [Agent 0: 0.00036969, Agent 1: 0.00041322] ; E(V) Avg: [Agent 0: 0.16974783, Agent 1: 0.09123511]) [Noise stdev: 0.06305977464428396]
Episode 840/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0140 ; (Q Loss Avg: [Agent 0: 0.00033190, Agent 1: 0.00038454] ; E(V) Avg: [Agent 0: 0.17728283, Agent 1: 0.09382303]) [Noise stdev: 0.06293365509499539]
Episode 841/5000 (33 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0140 ; (Q Loss Avg: [Agent 0: 0.00033742, Agent 1: 0.00036202] ; E(V) Avg: [Agent 0: 0.17402479, Agent 1: 0.09458798]) [Noise stdev: 0.0628077877848054]
Episode 842/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0140 ; (Q Loss Avg: [Agent 0: 0.00038575, Agent 1: 0.00029624] ; E(V) Avg: [Agent 0: 0.17143469, Agent 1: 0.09285315]) [Noise stdev: 0.06268217220923579]
Episode 843/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0150 ; (Q Loss Avg: [Agent 0: 0.00036972, Agent 1: 0.00032578] ; E(V) Avg: [Agent 0: 0.17237051, Agent 1: 0.09281882]) [Noise stdev: 0.06255680786481732]
Episode 844/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0160 ; (Q Loss Avg: [Agent 0: 0.00039382, Agent 1: 0.00036015] ; E(V) Avg: [Agent 0: 0.16933982, Agent 1: 0.09307341]) [Noise stdev: 0.062431694249087684]
Episode 845/5000 (18 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0160 ; (Q Loss Avg: [Agent 0: 0.00040779, Agent 1: 0.00030997] ; E(V) Avg: [Agent 0: 0.17126314, Agent 1: 0.09525070]) [Noise stdev: 0.06230683086058951]
Episode 846/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0160 ; (Q Loss Avg: [Agent 0: 0.00032767, Agent 1: 0.00034088] ; E(V) Avg: [Agent 0: 0.16852676, Agent 1: 0.09419597]) [Noise stdev: 0.06218221719886833]
Episode 847/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0160 ; (Q Loss Avg: [Agent 0: 0.00037520, Agent 1: 0.00031037] ; E(V) Avg: [Agent 0: 0.17100324, Agent 1: 0.09029289]) [Noise stdev: 0.062057852764470595]
Episode 848/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00043077, Agent 1: 0.00035157] ; E(V) Avg: [Agent 0: 0.17521881, Agent 1: 0.09324352]) [Noise stdev: 0.06193373705894165]
Episode 849/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00035970, Agent 1: 0.00037389] ; E(V) Avg: [Agent 0: 0.17122115, Agent 1: 0.09216953]) [Noise stdev: 0.06180986958482377]
Episode 850/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00037045, Agent 1: 0.00041421] ; E(V) Avg: [Agent 0: 0.17137620, Agent 1: 0.09167629]) [Noise stdev: 0.06168624984565412]
Episode 851/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00034457, Agent 1: 0.00038772] ; E(V) Avg: [Agent 0: 0.17383623, Agent 1: 0.09198856]) [Noise stdev: 0.061562877345962816]
Episode 852/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00034898, Agent 1: 0.00035599] ; E(V) Avg: [Agent 0: 0.17973493, Agent 1: 0.09224435]) [Noise stdev: 0.06143975159127089]
Episode 853/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00042178, Agent 1: 0.00033879] ; E(V) Avg: [Agent 0: 0.17416607, Agent 1: 0.09324325]) [Noise stdev: 0.061316872088088344]
Episode 854/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00041241, Agent 1: 0.00035196] ; E(V) Avg: [Agent 0: 0.17347205, Agent 1: 0.09327746]) [Noise stdev: 0.06119423834391217]
Episode 855/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00039025, Agent 1: 0.00040081] ; E(V) Avg: [Agent 0: 0.17069622, Agent 1: 0.09489963]) [Noise stdev: 0.061071849867224345]
Episode 856/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00039782, Agent 1: 0.00040790] ; E(V) Avg: [Agent 0: 0.17445351, Agent 1: 0.09223163]) [Noise stdev: 0.060949706167489896]
Episode 857/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00040438, Agent 1: 0.00036908] ; E(V) Avg: [Agent 0: 0.17039739, Agent 1: 0.09218352]) [Noise stdev: 0.060827806755154916]
Episode 858/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00042675, Agent 1: 0.00030414] ; E(V) Avg: [Agent 0: 0.17302060, Agent 1: 0.09210805]) [Noise stdev: 0.060706151141644606]
Episode 859/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0170 ; (Q Loss Avg: [Agent 0: 0.00039847, Agent 1: 0.00029886] ; E(V) Avg: [Agent 0: 0.17505204, Agent 1: 0.09142505]) [Noise stdev: 0.060584738839361316]
Episode 860/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0180 ; (Q Loss Avg: [Agent 0: 0.00034633, Agent 1: 0.00030403] ; E(V) Avg: [Agent 0: 0.17086045, Agent 1: 0.09040540]) [Noise stdev: 0.060463569361682595]
Episode 861/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0180 ; (Q Loss Avg: [Agent 0: 0.00038111, Agent 1: 0.00030229] ; E(V) Avg: [Agent 0: 0.17481445, Agent 1: 0.09390437]) [Noise stdev: 0.06034264222295923]
Episode 862/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0190 ; (Q Loss Avg: [Agent 0: 0.00034459, Agent 1: 0.00030247] ; E(V) Avg: [Agent 0: 0.16886647, Agent 1: 0.09366547]) [Noise stdev: 0.06022195693851331]
Episode 863/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0190 ; (Q Loss Avg: [Agent 0: 0.00039379, Agent 1: 0.00028489] ; E(V) Avg: [Agent 0: 0.17446985, Agent 1: 0.09250273]) [Noise stdev: 0.06010151302463628]
Episode 864/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0200 ; (Q Loss Avg: [Agent 0: 0.00038299, Agent 1: 0.00026213] ; E(V) Avg: [Agent 0: 0.17163864, Agent 1: 0.09252162]) [Noise stdev: 0.05998130999858701]
Episode 865/5000 (27 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0200 ; (Q Loss Avg: [Agent 0: 0.00036431, Agent 1: 0.00030764] ; E(V) Avg: [Agent 0: 0.17665292, Agent 1: 0.09287475]) [Noise stdev: 0.059861347378589835]
Episode 866/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0200 ; (Q Loss Avg: [Agent 0: 0.00035539, Agent 1: 0.00030308] ; E(V) Avg: [Agent 0: 0.17826772, Agent 1: 0.09250386]) [Noise stdev: 0.05974162468383266]
Episode 867/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0200 ; (Q Loss Avg: [Agent 0: 0.00045650, Agent 1: 0.00034038] ; E(V) Avg: [Agent 0: 0.17440366, Agent 1: 0.09626968]) [Noise stdev: 0.059622141434464995]
Episode 868/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0190 ; (Q Loss Avg: [Agent 0: 0.00044140, Agent 1: 0.00031837] ; E(V) Avg: [Agent 0: 0.17316564, Agent 1: 0.09191058]) [Noise stdev: 0.05950289715159607]
Episode 869/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0190 ; (Q Loss Avg: [Agent 0: 0.00047310, Agent 1: 0.00030011] ; E(V) Avg: [Agent 0: 0.17948032, Agent 1: 0.09593101]) [Noise stdev: 0.05938389135729288]
Episode 870/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0200 ; (Q Loss Avg: [Agent 0: 0.00042808, Agent 1: 0.00038412] ; E(V) Avg: [Agent 0: 0.17702696, Agent 1: 0.09252798]) [Noise stdev: 0.05926512357457829]
Episode 871/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0200 ; (Q Loss Avg: [Agent 0: 0.00039271, Agent 1: 0.00033658] ; E(V) Avg: [Agent 0: 0.17639680, Agent 1: 0.09449619]) [Noise stdev: 0.05914659332742914]
Episode 872/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0200 ; (Q Loss Avg: [Agent 0: 0.00041196, Agent 1: 0.00030004] ; E(V) Avg: [Agent 0: 0.17491503, Agent 1: 0.09371186]) [Noise stdev: 0.05902830014077428]
Episode 873/5000 (17 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0200 ; (Q Loss Avg: [Agent 0: 0.00044344, Agent 1: 0.00033080] ; E(V) Avg: [Agent 0: 0.17074944, Agent 1: 0.09399141]) [Noise stdev: 0.05891024354049273]
Episode 874/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0200 ; (Q Loss Avg: [Agent 0: 0.00036187, Agent 1: 0.00032872] ; E(V) Avg: [Agent 0: 0.17634227, Agent 1: 0.09553887]) [Noise stdev: 0.058792423053411744]
Episode 875/5000 (33 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00041020, Agent 1: 0.00036053] ; E(V) Avg: [Agent 0: 0.17453075, Agent 1: 0.09602780]) [Noise stdev: 0.05867483820730492]
Episode 876/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00053584, Agent 1: 0.00037847] ; E(V) Avg: [Agent 0: 0.17361961, Agent 1: 0.09341490]) [Noise stdev: 0.058557488530890316]
Episode 877/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0199 ; (Q Loss Avg: [Agent 0: 0.00050458, Agent 1: 0.00029398] ; E(V) Avg: [Agent 0: 0.17598502, Agent 1: 0.09481936]) [Noise stdev: 0.058440373553828535]
Episode 878/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0209 ; (Q Loss Avg: [Agent 0: 0.00042003, Agent 1: 0.00033714] ; E(V) Avg: [Agent 0: 0.17426919, Agent 1: 0.09396578]) [Noise stdev: 0.058323492806720875]
Episode 879/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0219 ; (Q Loss Avg: [Agent 0: 0.00039411, Agent 1: 0.00030962] ; E(V) Avg: [Agent 0: 0.17581490, Agent 1: 0.09351394]) [Noise stdev: 0.05820684582110743]
Episode 880/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0219 ; (Q Loss Avg: [Agent 0: 0.00044264, Agent 1: 0.00027878] ; E(V) Avg: [Agent 0: 0.17284492, Agent 1: 0.09466798]) [Noise stdev: 0.058090432129465214]
Episode 881/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0229 ; (Q Loss Avg: [Agent 0: 0.00048246, Agent 1: 0.00028482] ; E(V) Avg: [Agent 0: 0.17722198, Agent 1: 0.09334781]) [Noise stdev: 0.057974251265206285]
Episode 882/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0229 ; (Q Loss Avg: [Agent 0: 0.00039278, Agent 1: 0.00028701] ; E(V) Avg: [Agent 0: 0.18115777, Agent 1: 0.09263536]) [Noise stdev: 0.057858302762675874]
Episode 883/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0229 ; (Q Loss Avg: [Agent 0: 0.00046337, Agent 1: 0.00026161] ; E(V) Avg: [Agent 0: 0.17810290, Agent 1: 0.09617831]) [Noise stdev: 0.05774258615715052]
Episode 884/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0229 ; (Q Loss Avg: [Agent 0: 0.00040157, Agent 1: 0.00032418] ; E(V) Avg: [Agent 0: 0.17111587, Agent 1: 0.09702690]) [Noise stdev: 0.05762710098483622]
Episode 885/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0219 ; (Q Loss Avg: [Agent 0: 0.00040684, Agent 1: 0.00037976] ; E(V) Avg: [Agent 0: 0.17469563, Agent 1: 0.09675355]) [Noise stdev: 0.05751184678286655]
Episode 886/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0219 ; (Q Loss Avg: [Agent 0: 0.00036757, Agent 1: 0.00034521] ; E(V) Avg: [Agent 0: 0.17353447, Agent 1: 0.09650805]) [Noise stdev: 0.057396823089300816]
Episode 887/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0219 ; (Q Loss Avg: [Agent 0: 0.00045778, Agent 1: 0.00032364] ; E(V) Avg: [Agent 0: 0.18114714, Agent 1: 0.09115116]) [Noise stdev: 0.05728202944312222]
Episode 888/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0219 ; (Q Loss Avg: [Agent 0: 0.00048194, Agent 1: 0.00036002] ; E(V) Avg: [Agent 0: 0.16509818, Agent 1: 0.09090541]) [Noise stdev: 0.05716746538423597]
Episode 889/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0229 ; (Q Loss Avg: [Agent 0: 0.00046426, Agent 1: 0.00038227] ; E(V) Avg: [Agent 0: 0.17675825, Agent 1: 0.08947433]) [Noise stdev: 0.0570531304534675]
Episode 890/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0229 ; (Q Loss Avg: [Agent 0: 0.00045720, Agent 1: 0.00031577] ; E(V) Avg: [Agent 0: 0.17163330, Agent 1: 0.09056105]) [Noise stdev: 0.05693902419256056]
Episode 891/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0229 ; (Q Loss Avg: [Agent 0: 0.00040060, Agent 1: 0.00033096] ; E(V) Avg: [Agent 0: 0.17084661, Agent 1: 0.09120761]) [Noise stdev: 0.05682514614417544]
Episode 892/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0229 ; (Q Loss Avg: [Agent 0: 0.00039442, Agent 1: 0.00037268] ; E(V) Avg: [Agent 0: 0.17815467, Agent 1: 0.09105337]) [Noise stdev: 0.05671149585188709]
Episode 893/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0229 ; (Q Loss Avg: [Agent 0: 0.00033547, Agent 1: 0.00036687] ; E(V) Avg: [Agent 0: 0.17627057, Agent 1: 0.09226202]) [Noise stdev: 0.05659807286018331]
Episode 894/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0229 ; (Q Loss Avg: [Agent 0: 0.00034638, Agent 1: 0.00040896] ; E(V) Avg: [Agent 0: 0.17629544, Agent 1: 0.09771974]) [Noise stdev: 0.056484876714462944]
Episode 895/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0239 ; (Q Loss Avg: [Agent 0: 0.00043043, Agent 1: 0.00039457] ; E(V) Avg: [Agent 0: 0.18031551, Agent 1: 0.09215569]) [Noise stdev: 0.056371906961034016]
Episode 896/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0249 ; (Q Loss Avg: [Agent 0: 0.00043806, Agent 1: 0.00040933] ; E(V) Avg: [Agent 0: 0.17585502, Agent 1: 0.09373646]) [Noise stdev: 0.05625916314711195]
Episode 897/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0259 ; (Q Loss Avg: [Agent 0: 0.00040045, Agent 1: 0.00035113] ; E(V) Avg: [Agent 0: 0.17863259, Agent 1: 0.09445078]) [Noise stdev: 0.05614664482081772]
Episode 898/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0259 ; (Q Loss Avg: [Agent 0: 0.00046606, Agent 1: 0.00037415] ; E(V) Avg: [Agent 0: 0.18529710, Agent 1: 0.09198173]) [Noise stdev: 0.056034351531176085]
Episode 899/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0259 ; (Q Loss Avg: [Agent 0: 0.00038949, Agent 1: 0.00034091] ; E(V) Avg: [Agent 0: 0.17204699, Agent 1: 0.09169001]) [Noise stdev: 0.055922282828113734]
Episode 900/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0259 ; (Q Loss Avg: [Agent 0: 0.00047026, Agent 1: 0.00039361] ; E(V) Avg: [Agent 0: 0.17805565, Agent 1: 0.09023445]) [Noise stdev: 0.055810438262457504]
Episode 901/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0269 ; (Q Loss Avg: [Agent 0: 0.00046535, Agent 1: 0.00034423] ; E(V) Avg: [Agent 0: 0.17602155, Agent 1: 0.09482798]) [Noise stdev: 0.05569881738593259]
Episode 902/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0269 ; (Q Loss Avg: [Agent 0: 0.00045452, Agent 1: 0.00030970] ; E(V) Avg: [Agent 0: 0.17719522, Agent 1: 0.09532540]) [Noise stdev: 0.05558741975116072]
Episode 903/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0279 ; (Q Loss Avg: [Agent 0: 0.00044940, Agent 1: 0.00035920] ; E(V) Avg: [Agent 0: 0.17793248, Agent 1: 0.09662831]) [Noise stdev: 0.0554762449116584]
Episode 904/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0279 ; (Q Loss Avg: [Agent 0: 0.00044178, Agent 1: 0.00033376] ; E(V) Avg: [Agent 0: 0.18253325, Agent 1: 0.09772718]) [Noise stdev: 0.055365292421835084]
Episode 905/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00044005, Agent 1: 0.00040109] ; E(V) Avg: [Agent 0: 0.18194478, Agent 1: 0.09524998]) [Noise stdev: 0.055254561836991414]
Episode 906/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0279 ; (Q Loss Avg: [Agent 0: 0.00043641, Agent 1: 0.00041620] ; E(V) Avg: [Agent 0: 0.17647447, Agent 1: 0.09336578]) [Noise stdev: 0.05514405271331743]
Episode 907/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0279 ; (Q Loss Avg: [Agent 0: 0.00047978, Agent 1: 0.00037562] ; E(V) Avg: [Agent 0: 0.17382292, Agent 1: 0.09404163]) [Noise stdev: 0.0550337646078908]
Episode 908/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00041863, Agent 1: 0.00036468] ; E(V) Avg: [Agent 0: 0.17727861, Agent 1: 0.09469522]) [Noise stdev: 0.05492369707867502]
Episode 909/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00045352, Agent 1: 0.00039155] ; E(V) Avg: [Agent 0: 0.17869306, Agent 1: 0.09609232]) [Noise stdev: 0.05481384968451767]
Episode 910/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00042733, Agent 1: 0.00041584] ; E(V) Avg: [Agent 0: 0.17772160, Agent 1: 0.09428935]) [Noise stdev: 0.054704221985148634]
Episode 911/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00041331, Agent 1: 0.00042148] ; E(V) Avg: [Agent 0: 0.17707939, Agent 1: 0.09791530]) [Noise stdev: 0.05459481354117834]
Episode 912/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00038980, Agent 1: 0.00038063] ; E(V) Avg: [Agent 0: 0.17173478, Agent 1: 0.09428122]) [Noise stdev: 0.05448562391409598]
Episode 913/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00040187, Agent 1: 0.00035928] ; E(V) Avg: [Agent 0: 0.17910646, Agent 1: 0.09401015]) [Noise stdev: 0.05437665266626779]
Episode 914/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0279 ; (Q Loss Avg: [Agent 0: 0.00037798, Agent 1: 0.00042494] ; E(V) Avg: [Agent 0: 0.17910108, Agent 1: 0.09317863]) [Noise stdev: 0.054267899360935255]
Episode 915/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0279 ; (Q Loss Avg: [Agent 0: 0.00046315, Agent 1: 0.00036252] ; E(V) Avg: [Agent 0: 0.18025006, Agent 1: 0.09694808]) [Noise stdev: 0.05415936356221338]
Episode 916/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0269 ; (Q Loss Avg: [Agent 0: 0.00048163, Agent 1: 0.00036669] ; E(V) Avg: [Agent 0: 0.17945735, Agent 1: 0.09323922]) [Noise stdev: 0.054051044835088956]
Episode 917/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0269 ; (Q Loss Avg: [Agent 0: 0.00046619, Agent 1: 0.00030660] ; E(V) Avg: [Agent 0: 0.18046990, Agent 1: 0.09455538]) [Noise stdev: 0.053942942745418776]
Episode 918/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0279 ; (Q Loss Avg: [Agent 0: 0.00051791, Agent 1: 0.00040422] ; E(V) Avg: [Agent 0: 0.17612832, Agent 1: 0.09530434]) [Noise stdev: 0.05383505685992794]
Episode 919/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0269 ; (Q Loss Avg: [Agent 0: 0.00059464, Agent 1: 0.00034743] ; E(V) Avg: [Agent 0: 0.18873088, Agent 1: 0.09249110]) [Noise stdev: 0.05372738674620808]
Episode 920/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0269 ; (Q Loss Avg: [Agent 0: 0.00053744, Agent 1: 0.00035894] ; E(V) Avg: [Agent 0: 0.17694269, Agent 1: 0.09626773]) [Noise stdev: 0.05361993197271567]
Episode 921/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0279 ; (Q Loss Avg: [Agent 0: 0.00051038, Agent 1: 0.00032430] ; E(V) Avg: [Agent 0: 0.18059791, Agent 1: 0.09842940]) [Noise stdev: 0.053512692108770236]
Episode 922/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00055196, Agent 1: 0.00031177] ; E(V) Avg: [Agent 0: 0.18004233, Agent 1: 0.09411203]) [Noise stdev: 0.05340566672455269]
Episode 923/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00058927, Agent 1: 0.00033017] ; E(V) Avg: [Agent 0: 0.18111807, Agent 1: 0.09578937]) [Noise stdev: 0.053298855391103586]
Episode 924/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0299 ; (Q Loss Avg: [Agent 0: 0.00053932, Agent 1: 0.00037548] ; E(V) Avg: [Agent 0: 0.17790098, Agent 1: 0.09498952]) [Noise stdev: 0.05319225768032138]
Episode 925/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0299 ; (Q Loss Avg: [Agent 0: 0.00065151, Agent 1: 0.00032989] ; E(V) Avg: [Agent 0: 0.18522881, Agent 1: 0.09809363]) [Noise stdev: 0.05308587316496074]
Episode 926/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00059919, Agent 1: 0.00030650] ; E(V) Avg: [Agent 0: 0.17938272, Agent 1: 0.09786981]) [Noise stdev: 0.052979701418630815]
Episode 927/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00056147, Agent 1: 0.00032820] ; E(V) Avg: [Agent 0: 0.17867981, Agent 1: 0.09584895]) [Noise stdev: 0.052873742015793555]
Episode 928/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0279 ; (Q Loss Avg: [Agent 0: 0.00056581, Agent 1: 0.00045782] ; E(V) Avg: [Agent 0: 0.17726190, Agent 1: 0.09260017]) [Noise stdev: 0.052767994531761966]
Episode 929/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00051025, Agent 1: 0.00042552] ; E(V) Avg: [Agent 0: 0.18222443, Agent 1: 0.09453096]) [Noise stdev: 0.052662458542698444]
Episode 930/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0289 ; (Q Loss Avg: [Agent 0: 0.00045928, Agent 1: 0.00033057] ; E(V) Avg: [Agent 0: 0.18313494, Agent 1: 0.09507861]) [Noise stdev: 0.052557133625613046]
Episode 931/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0298 ; (Q Loss Avg: [Agent 0: 0.00043346, Agent 1: 0.00036832] ; E(V) Avg: [Agent 0: 0.18000555, Agent 1: 0.09471859]) [Noise stdev: 0.05245201935836182]
Episode 932/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0298 ; (Q Loss Avg: [Agent 0: 0.00060493, Agent 1: 0.00034119] ; E(V) Avg: [Agent 0: 0.18132262, Agent 1: 0.09569422]) [Noise stdev: 0.052347115319645095]
Episode 933/5000 (87 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0318 ; (Q Loss Avg: [Agent 0: 0.00053680, Agent 1: 0.00033236] ; E(V) Avg: [Agent 0: 0.17920847, Agent 1: 0.09517113]) [Noise stdev: 0.05224242108900581]
Episode 934/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0328 ; (Q Loss Avg: [Agent 0: 0.00045055, Agent 1: 0.00033413] ; E(V) Avg: [Agent 0: 0.17873478, Agent 1: 0.09402616]) [Noise stdev: 0.0521379362468278]
Episode 935/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0328 ; (Q Loss Avg: [Agent 0: 0.00051051, Agent 1: 0.00038436] ; E(V) Avg: [Agent 0: 0.18067475, Agent 1: 0.09257176]) [Noise stdev: 0.05203366037433414]
Episode 936/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0328 ; (Q Loss Avg: [Agent 0: 0.00051884, Agent 1: 0.00038878] ; E(V) Avg: [Agent 0: 0.18327678, Agent 1: 0.09775090]) [Noise stdev: 0.05192959305358547]
Episode 937/5000 (32 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0337 ; (Q Loss Avg: [Agent 0: 0.00043638, Agent 1: 0.00036058] ; E(V) Avg: [Agent 0: 0.18028577, Agent 1: 0.09319313]) [Noise stdev: 0.0518257338674783]
Episode 938/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00046678, Agent 1: 0.00030520] ; E(V) Avg: [Agent 0: 0.17656344, Agent 1: 0.09480624]) [Noise stdev: 0.051722082399743345]
Episode 939/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00036411, Agent 1: 0.00034159] ; E(V) Avg: [Agent 0: 0.17992182, Agent 1: 0.09625471]) [Noise stdev: 0.05161863823494386]
Episode 940/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00048412, Agent 1: 0.00035956] ; E(V) Avg: [Agent 0: 0.18075723, Agent 1: 0.09045349]) [Noise stdev: 0.05151540095847397]
Episode 941/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00043323, Agent 1: 0.00030706] ; E(V) Avg: [Agent 0: 0.17749583, Agent 1: 0.09260673]) [Noise stdev: 0.05141237015655702]
Episode 942/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00041721, Agent 1: 0.00032745] ; E(V) Avg: [Agent 0: 0.17716680, Agent 1: 0.09609398]) [Noise stdev: 0.05130954541624391]
Episode 943/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00042823, Agent 1: 0.00034110] ; E(V) Avg: [Agent 0: 0.18054458, Agent 1: 0.09384898]) [Noise stdev: 0.05120692632541143]
Episode 944/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0317 ; (Q Loss Avg: [Agent 0: 0.00055397, Agent 1: 0.00035168] ; E(V) Avg: [Agent 0: 0.17904091, Agent 1: 0.09643357]) [Noise stdev: 0.051104512472760606]
Episode 945/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00053071, Agent 1: 0.00027453] ; E(V) Avg: [Agent 0: 0.17786248, Agent 1: 0.09502720]) [Noise stdev: 0.05100230344781508]
Episode 946/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00049511, Agent 1: 0.00028094] ; E(V) Avg: [Agent 0: 0.17777577, Agent 1: 0.09508530]) [Noise stdev: 0.050900298840919456]
Episode 947/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00046128, Agent 1: 0.00031497] ; E(V) Avg: [Agent 0: 0.17656056, Agent 1: 0.09398053]) [Noise stdev: 0.050798498243237615]
Episode 948/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0317 ; (Q Loss Avg: [Agent 0: 0.00053170, Agent 1: 0.00033143] ; E(V) Avg: [Agent 0: 0.18216525, Agent 1: 0.09467951]) [Noise stdev: 0.05069690124675114]
Episode 949/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0317 ; (Q Loss Avg: [Agent 0: 0.00051320, Agent 1: 0.00031239] ; E(V) Avg: [Agent 0: 0.17805802, Agent 1: 0.09092039]) [Noise stdev: 0.05059550744425764]
Episode 950/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0317 ; (Q Loss Avg: [Agent 0: 0.00042959, Agent 1: 0.00028706] ; E(V) Avg: [Agent 0: 0.17227402, Agent 1: 0.09521314]) [Noise stdev: 0.05049431642936913]
Episode 951/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0317 ; (Q Loss Avg: [Agent 0: 0.00049967, Agent 1: 0.00039621] ; E(V) Avg: [Agent 0: 0.17716504, Agent 1: 0.09332275]) [Noise stdev: 0.05039332779651039]
Episode 952/5000 (33 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0317 ; (Q Loss Avg: [Agent 0: 0.00044092, Agent 1: 0.00029966] ; E(V) Avg: [Agent 0: 0.17967597, Agent 1: 0.09259002]) [Noise stdev: 0.050292541140917364]
Episode 953/5000 (42 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00047033, Agent 1: 0.00031850] ; E(V) Avg: [Agent 0: 0.17711676, Agent 1: 0.09243660]) [Noise stdev: 0.05019195605863553]
Episode 954/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0317 ; (Q Loss Avg: [Agent 0: 0.00041091, Agent 1: 0.00032593] ; E(V) Avg: [Agent 0: 0.17557522, Agent 1: 0.09336572]) [Noise stdev: 0.05009157214651826]
Episode 955/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0317 ; (Q Loss Avg: [Agent 0: 0.00038961, Agent 1: 0.00033375] ; E(V) Avg: [Agent 0: 0.17300956, Agent 1: 0.09098766]) [Noise stdev: 0.04999138900222522]
Episode 956/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00035482, Agent 1: 0.00031280] ; E(V) Avg: [Agent 0: 0.17485586, Agent 1: 0.09245090]) [Noise stdev: 0.049891406224220766]
Episode 957/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00041560, Agent 1: 0.00028174] ; E(V) Avg: [Agent 0: 0.17836331, Agent 1: 0.09324375]) [Noise stdev: 0.049791623411772325]
Episode 958/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00037599, Agent 1: 0.00032844] ; E(V) Avg: [Agent 0: 0.17529499, Agent 1: 0.09377114]) [Noise stdev: 0.04969204016494878]
Episode 959/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00045002, Agent 1: 0.00031738] ; E(V) Avg: [Agent 0: 0.17181387, Agent 1: 0.09074801]) [Noise stdev: 0.04959265608461888]
Episode 960/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0317 ; (Q Loss Avg: [Agent 0: 0.00041150, Agent 1: 0.00027564] ; E(V) Avg: [Agent 0: 0.17332438, Agent 1: 0.09361812]) [Noise stdev: 0.049493470772449644]
Episode 961/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0317 ; (Q Loss Avg: [Agent 0: 0.00047877, Agent 1: 0.00029506] ; E(V) Avg: [Agent 0: 0.18045934, Agent 1: 0.09295093]) [Noise stdev: 0.049394483830904744]
Episode 962/5000 (87 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00044816, Agent 1: 0.00028771] ; E(V) Avg: [Agent 0: 0.17761191, Agent 1: 0.09148571]) [Noise stdev: 0.049295694863242936]
Episode 963/5000 (33 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0336 ; (Q Loss Avg: [Agent 0: 0.00046405, Agent 1: 0.00030339] ; E(V) Avg: [Agent 0: 0.17780410, Agent 1: 0.09194491]) [Noise stdev: 0.04919710347351645]
Episode 964/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0336 ; (Q Loss Avg: [Agent 0: 0.00050849, Agent 1: 0.00030012] ; E(V) Avg: [Agent 0: 0.17834386, Agent 1: 0.09010202]) [Noise stdev: 0.04909870926656942]
Episode 965/5000 (25 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0336 ; (Q Loss Avg: [Agent 0: 0.00049346, Agent 1: 0.00029553] ; E(V) Avg: [Agent 0: 0.16994488, Agent 1: 0.09214761]) [Noise stdev: 0.04900051184803628]
Episode 966/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0336 ; (Q Loss Avg: [Agent 0: 0.00052704, Agent 1: 0.00022911] ; E(V) Avg: [Agent 0: 0.17304735, Agent 1: 0.09379989]) [Noise stdev: 0.0489025108243402]
Episode 967/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0356 ; (Q Loss Avg: [Agent 0: 0.00061218, Agent 1: 0.00032528] ; E(V) Avg: [Agent 0: 0.17390310, Agent 1: 0.09125713]) [Noise stdev: 0.04880470580269152]
Episode 968/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0356 ; (Q Loss Avg: [Agent 0: 0.00057654, Agent 1: 0.00030522] ; E(V) Avg: [Agent 0: 0.17113928, Agent 1: 0.09360119]) [Noise stdev: 0.04870709639108614]
Episode 969/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0356 ; (Q Loss Avg: [Agent 0: 0.00048567, Agent 1: 0.00034449] ; E(V) Avg: [Agent 0: 0.17532973, Agent 1: 0.09131511]) [Noise stdev: 0.04860968219830397]
Episode 970/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0346 ; (Q Loss Avg: [Agent 0: 0.00051110, Agent 1: 0.00034318] ; E(V) Avg: [Agent 0: 0.17174136, Agent 1: 0.09212369]) [Noise stdev: 0.048512462833907366]
Episode 971/5000 (39 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0346 ; (Q Loss Avg: [Agent 0: 0.00055761, Agent 1: 0.00032106] ; E(V) Avg: [Agent 0: 0.17222117, Agent 1: 0.09164258]) [Noise stdev: 0.04841543790823955]
Episode 972/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0346 ; (Q Loss Avg: [Agent 0: 0.00064060, Agent 1: 0.00034087] ; E(V) Avg: [Agent 0: 0.17039407, Agent 1: 0.09407835]) [Noise stdev: 0.04831860703242307]
Episode 973/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0346 ; (Q Loss Avg: [Agent 0: 0.00056078, Agent 1: 0.00031228] ; E(V) Avg: [Agent 0: 0.17493394, Agent 1: 0.09489479]) [Noise stdev: 0.048221969818358225]
Episode 974/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0346 ; (Q Loss Avg: [Agent 0: 0.00058301, Agent 1: 0.00029209] ; E(V) Avg: [Agent 0: 0.17078704, Agent 1: 0.08879850]) [Noise stdev: 0.04812552587872151]
Episode 975/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0337 ; (Q Loss Avg: [Agent 0: 0.00087552, Agent 1: 0.00031771] ; E(V) Avg: [Agent 0: 0.17241908, Agent 1: 0.09493020]) [Noise stdev: 0.048029274826964063]
Episode 976/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0337 ; (Q Loss Avg: [Agent 0: 0.00080349, Agent 1: 0.00035219] ; E(V) Avg: [Agent 0: 0.17169253, Agent 1: 0.09475472]) [Noise stdev: 0.047933216277310134]
Episode 977/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0337 ; (Q Loss Avg: [Agent 0: 0.00070157, Agent 1: 0.00033112] ; E(V) Avg: [Agent 0: 0.16903018, Agent 1: 0.09156377]) [Noise stdev: 0.04783734984475552]
Episode 978/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0337 ; (Q Loss Avg: [Agent 0: 0.00066705, Agent 1: 0.00030104] ; E(V) Avg: [Agent 0: 0.16852933, Agent 1: 0.09244811]) [Noise stdev: 0.047741675145066005]
Episode 979/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00053303, Agent 1: 0.00029980] ; E(V) Avg: [Agent 0: 0.17264885, Agent 1: 0.09160096]) [Noise stdev: 0.047646191794775875]
Episode 980/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0327 ; (Q Loss Avg: [Agent 0: 0.00046386, Agent 1: 0.00036282] ; E(V) Avg: [Agent 0: 0.17325221, Agent 1: 0.09175024]) [Noise stdev: 0.04755089941118632]
Episode 981/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0317 ; (Q Loss Avg: [Agent 0: 0.00049810, Agent 1: 0.00032333] ; E(V) Avg: [Agent 0: 0.17262106, Agent 1: 0.09160057]) [Noise stdev: 0.04745579761236395]
Episode 982/5000 (87 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0337 ; (Q Loss Avg: [Agent 0: 0.00051131, Agent 1: 0.00033488] ; E(V) Avg: [Agent 0: 0.17191700, Agent 1: 0.09057270]) [Noise stdev: 0.047360886017139225]
Episode 983/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0337 ; (Q Loss Avg: [Agent 0: 0.00042868, Agent 1: 0.00034964] ; E(V) Avg: [Agent 0: 0.16824486, Agent 1: 0.08886262]) [Noise stdev: 0.04726616424510495]
Episode 984/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0347 ; (Q Loss Avg: [Agent 0: 0.00054073, Agent 1: 0.00034123] ; E(V) Avg: [Agent 0: 0.17393269, Agent 1: 0.08983652]) [Noise stdev: 0.04717163191661474]
Episode 985/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0357 ; (Q Loss Avg: [Agent 0: 0.00047502, Agent 1: 0.00031074] ; E(V) Avg: [Agent 0: 0.16939294, Agent 1: 0.08992646]) [Noise stdev: 0.04707728865278151]
Episode 986/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0367 ; (Q Loss Avg: [Agent 0: 0.00053510, Agent 1: 0.00029758] ; E(V) Avg: [Agent 0: 0.17248875, Agent 1: 0.09080055]) [Noise stdev: 0.04698313407547595]
Episode 987/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0377 ; (Q Loss Avg: [Agent 0: 0.00047121, Agent 1: 0.00029306] ; E(V) Avg: [Agent 0: 0.17413322, Agent 1: 0.09152268]) [Noise stdev: 0.046889167807324994]
Episode 988/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0377 ; (Q Loss Avg: [Agent 0: 0.00051378, Agent 1: 0.00029535] ; E(V) Avg: [Agent 0: 0.16924767, Agent 1: 0.09073887]) [Noise stdev: 0.046795389471710344]
Episode 989/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0367 ; (Q Loss Avg: [Agent 0: 0.00050596, Agent 1: 0.00030338] ; E(V) Avg: [Agent 0: 0.16902336, Agent 1: 0.08975724]) [Noise stdev: 0.04670179869276692]
Episode 990/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0367 ; (Q Loss Avg: [Agent 0: 0.00056166, Agent 1: 0.00024870] ; E(V) Avg: [Agent 0: 0.17288016, Agent 1: 0.08633727]) [Noise stdev: 0.04660839509538139]
Episode 991/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0367 ; (Q Loss Avg: [Agent 0: 0.00081350, Agent 1: 0.00025033] ; E(V) Avg: [Agent 0: 0.17171332, Agent 1: 0.08753684]) [Noise stdev: 0.04651517830519063]
Episode 992/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0367 ; (Q Loss Avg: [Agent 0: 0.00079195, Agent 1: 0.00029191] ; E(V) Avg: [Agent 0: 0.17301949, Agent 1: 0.09198072]) [Noise stdev: 0.046422147948580246]
Episode 993/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0367 ; (Q Loss Avg: [Agent 0: 0.00072357, Agent 1: 0.00027031] ; E(V) Avg: [Agent 0: 0.17235121, Agent 1: 0.08957838]) [Noise stdev: 0.04632930365268308]
Episode 994/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0376 ; (Q Loss Avg: [Agent 0: 0.00054168, Agent 1: 0.00027183] ; E(V) Avg: [Agent 0: 0.17164980, Agent 1: 0.08911324]) [Noise stdev: 0.046236645045377715]
Episode 995/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0366 ; (Q Loss Avg: [Agent 0: 0.00048103, Agent 1: 0.00034999] ; E(V) Avg: [Agent 0: 0.17610065, Agent 1: 0.08938140]) [Noise stdev: 0.04614417175528696]
Episode 996/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0356 ; (Q Loss Avg: [Agent 0: 0.00050721, Agent 1: 0.00030736] ; E(V) Avg: [Agent 0: 0.17101862, Agent 1: 0.09123070]) [Noise stdev: 0.046051883411776386]
Episode 997/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0346 ; (Q Loss Avg: [Agent 0: 0.00043044, Agent 1: 0.00028183] ; E(V) Avg: [Agent 0: 0.17387761, Agent 1: 0.08648255]) [Noise stdev: 0.04595977964495283]
Episode 998/5000 (32 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0346 ; (Q Loss Avg: [Agent 0: 0.00053496, Agent 1: 0.00029962] ; E(V) Avg: [Agent 0: 0.16981514, Agent 1: 0.09005187]) [Noise stdev: 0.045867860085662925]
Episode 999/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0356 ; (Q Loss Avg: [Agent 0: 0.00055000, Agent 1: 0.00031679] ; E(V) Avg: [Agent 0: 0.17119231, Agent 1: 0.08798293]) [Noise stdev: 0.0457761243654916]
Episode 1000/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0366 ; (Q Loss Avg: [Agent 0: 0.00058310, Agent 1: 0.00027720] ; E(V) Avg: [Agent 0: 0.17300262, Agent 1: 0.08915460]) [Noise stdev: 0.045684572116760615]
Episode 1001/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0366 ; (Q Loss Avg: [Agent 0: 0.00049971, Agent 1: 0.00028363] ; E(V) Avg: [Agent 0: 0.17545818, Agent 1: 0.08825747]) [Noise stdev: 0.04559320297252709]
Episode 1002/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0376 ; (Q Loss Avg: [Agent 0: 0.00051894, Agent 1: 0.00032845] ; E(V) Avg: [Agent 0: 0.17107253, Agent 1: 0.08964410]) [Noise stdev: 0.04550201656658204]
Episode 1003/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0376 ; (Q Loss Avg: [Agent 0: 0.00043857, Agent 1: 0.00027521] ; E(V) Avg: [Agent 0: 0.17110709, Agent 1: 0.09033238]) [Noise stdev: 0.045411012533448876]
Episode 1004/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0376 ; (Q Loss Avg: [Agent 0: 0.00044064, Agent 1: 0.00024365] ; E(V) Avg: [Agent 0: 0.17271912, Agent 1: 0.08853870]) [Noise stdev: 0.04532019050838198]
Episode 1005/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0376 ; (Q Loss Avg: [Agent 0: 0.00047449, Agent 1: 0.00025596] ; E(V) Avg: [Agent 0: 0.17301709, Agent 1: 0.08835473]) [Noise stdev: 0.045229550127365216]
Episode 1006/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0376 ; (Q Loss Avg: [Agent 0: 0.00054116, Agent 1: 0.00029888] ; E(V) Avg: [Agent 0: 0.17418658, Agent 1: 0.09208483]) [Noise stdev: 0.04513909102711049]
Episode 1007/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0376 ; (Q Loss Avg: [Agent 0: 0.00054821, Agent 1: 0.00025899] ; E(V) Avg: [Agent 0: 0.16768698, Agent 1: 0.08739620]) [Noise stdev: 0.04504881284505627]
Episode 1008/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0376 ; (Q Loss Avg: [Agent 0: 0.00057017, Agent 1: 0.00024401] ; E(V) Avg: [Agent 0: 0.17132068, Agent 1: 0.08671554]) [Noise stdev: 0.044958715219366154]
Episode 1009/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0385 ; (Q Loss Avg: [Agent 0: 0.00065838, Agent 1: 0.00022956] ; E(V) Avg: [Agent 0: 0.17500060, Agent 1: 0.08965172]) [Noise stdev: 0.04486879778892742]
Episode 1010/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0395 ; (Q Loss Avg: [Agent 0: 0.00054259, Agent 1: 0.00024341] ; E(V) Avg: [Agent 0: 0.17029015, Agent 1: 0.08630942]) [Noise stdev: 0.04477906019334956]
Episode 1011/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0405 ; (Q Loss Avg: [Agent 0: 0.00050003, Agent 1: 0.00026355] ; E(V) Avg: [Agent 0: 0.16919880, Agent 1: 0.08835862]) [Noise stdev: 0.044689502072962864]
Episode 1012/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0415 ; (Q Loss Avg: [Agent 0: 0.00059379, Agent 1: 0.00028796] ; E(V) Avg: [Agent 0: 0.17231434, Agent 1: 0.08870800]) [Noise stdev: 0.04460012306881694]
Episode 1013/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0424 ; (Q Loss Avg: [Agent 0: 0.00058178, Agent 1: 0.00024928] ; E(V) Avg: [Agent 0: 0.16835426, Agent 1: 0.08804726]) [Noise stdev: 0.0445109228226793]
Episode 1014/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0434 ; (Q Loss Avg: [Agent 0: 0.00049470, Agent 1: 0.00029505] ; E(V) Avg: [Agent 0: 0.17072850, Agent 1: 0.08729338]) [Noise stdev: 0.04442190097703394]
Episode 1015/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0434 ; (Q Loss Avg: [Agent 0: 0.00046134, Agent 1: 0.00024668] ; E(V) Avg: [Agent 0: 0.17089951, Agent 1: 0.08892541]) [Noise stdev: 0.04433305717507987]
Episode 1016/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0444 ; (Q Loss Avg: [Agent 0: 0.00046983, Agent 1: 0.00030466] ; E(V) Avg: [Agent 0: 0.17027135, Agent 1: 0.08883499]) [Noise stdev: 0.04424439106072971]
Episode 1017/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0454 ; (Q Loss Avg: [Agent 0: 0.00052284, Agent 1: 0.00028695] ; E(V) Avg: [Agent 0: 0.16628203, Agent 1: 0.08884416]) [Noise stdev: 0.044155902278608244]
Episode 1018/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0453 ; (Q Loss Avg: [Agent 0: 0.00045032, Agent 1: 0.00023789] ; E(V) Avg: [Agent 0: 0.16925216, Agent 1: 0.08700988]) [Noise stdev: 0.04406759047405103]
Episode 1019/5000 (29 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0462 ; (Q Loss Avg: [Agent 0: 0.00053524, Agent 1: 0.00021471] ; E(V) Avg: [Agent 0: 0.17134718, Agent 1: 0.08819998]) [Noise stdev: 0.04397945529310293]
Episode 1020/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0472 ; (Q Loss Avg: [Agent 0: 0.00049572, Agent 1: 0.00024125] ; E(V) Avg: [Agent 0: 0.16795611, Agent 1: 0.08740892]) [Noise stdev: 0.04389149638251673]
Episode 1021/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0472 ; (Q Loss Avg: [Agent 0: 0.00061269, Agent 1: 0.00026586] ; E(V) Avg: [Agent 0: 0.16874102, Agent 1: 0.08585933]) [Noise stdev: 0.0438037133897517]
Episode 1022/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0471 ; (Q Loss Avg: [Agent 0: 0.00049221, Agent 1: 0.00026758] ; E(V) Avg: [Agent 0: 0.17131225, Agent 1: 0.08666676]) [Noise stdev: 0.04371610596297219]
Episode 1023/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0481 ; (Q Loss Avg: [Agent 0: 0.00056472, Agent 1: 0.00029300] ; E(V) Avg: [Agent 0: 0.16952882, Agent 1: 0.08609266]) [Noise stdev: 0.043628673751046246]
Episode 1024/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0481 ; (Q Loss Avg: [Agent 0: 0.00058190, Agent 1: 0.00029121] ; E(V) Avg: [Agent 0: 0.16765977, Agent 1: 0.08501128]) [Noise stdev: 0.043541416403544156]
Episode 1025/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0491 ; (Q Loss Avg: [Agent 0: 0.00047977, Agent 1: 0.00029281] ; E(V) Avg: [Agent 0: 0.16751827, Agent 1: 0.08812634]) [Noise stdev: 0.043454333570737066]
Episode 1026/5000 (65 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0511 ; (Q Loss Avg: [Agent 0: 0.00057842, Agent 1: 0.00027400] ; E(V) Avg: [Agent 0: 0.16929581, Agent 1: 0.08592237]) [Noise stdev: 0.04336742490359559]
Episode 1027/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0520 ; (Q Loss Avg: [Agent 0: 0.00055734, Agent 1: 0.00027329] ; E(V) Avg: [Agent 0: 0.16580833, Agent 1: 0.08689562]) [Noise stdev: 0.0432806900537884]
Episode 1028/5000 (36 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0530 ; (Q Loss Avg: [Agent 0: 0.00061501, Agent 1: 0.00029060] ; E(V) Avg: [Agent 0: 0.16911571, Agent 1: 0.08653095]) [Noise stdev: 0.043194128673680825]
Episode 1029/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0520 ; (Q Loss Avg: [Agent 0: 0.00058720, Agent 1: 0.00036643] ; E(V) Avg: [Agent 0: 0.16662914, Agent 1: 0.08466164]) [Noise stdev: 0.043107740416333466]
Episode 1030/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0530 ; (Q Loss Avg: [Agent 0: 0.00063142, Agent 1: 0.00029413] ; E(V) Avg: [Agent 0: 0.16848180, Agent 1: 0.08520621]) [Noise stdev: 0.0430215249355008]
Episode 1031/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0531 ; (Q Loss Avg: [Agent 0: 0.00064057, Agent 1: 0.00032027] ; E(V) Avg: [Agent 0: 0.16580013, Agent 1: 0.08382124]) [Noise stdev: 0.042935481885629796]
Episode 1032/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0541 ; (Q Loss Avg: [Agent 0: 0.00052313, Agent 1: 0.00033581] ; E(V) Avg: [Agent 0: 0.16220928, Agent 1: 0.08543690]) [Noise stdev: 0.04284961092185854]
Episode 1033/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0531 ; (Q Loss Avg: [Agent 0: 0.00051090, Agent 1: 0.00032518] ; E(V) Avg: [Agent 0: 0.16371997, Agent 1: 0.08278625]) [Noise stdev: 0.04276391170001482]
Episode 1034/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0531 ; (Q Loss Avg: [Agent 0: 0.00061860, Agent 1: 0.00031640] ; E(V) Avg: [Agent 0: 0.16680683, Agent 1: 0.08412126]) [Noise stdev: 0.04267838387661479]
Episode 1035/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0541 ; (Q Loss Avg: [Agent 0: 0.00060229, Agent 1: 0.00030045] ; E(V) Avg: [Agent 0: 0.16605747, Agent 1: 0.08723161]) [Noise stdev: 0.04259302710886156]
Episode 1036/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0551 ; (Q Loss Avg: [Agent 0: 0.00061561, Agent 1: 0.00027440] ; E(V) Avg: [Agent 0: 0.16534784, Agent 1: 0.08579125]) [Noise stdev: 0.042507841054643836]
Episode 1037/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0552 ; (Q Loss Avg: [Agent 0: 0.00059286, Agent 1: 0.00026205] ; E(V) Avg: [Agent 0: 0.16678927, Agent 1: 0.08549834]) [Noise stdev: 0.04242282537253455]
Episode 1038/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0562 ; (Q Loss Avg: [Agent 0: 0.00051958, Agent 1: 0.00026314] ; E(V) Avg: [Agent 0: 0.16563115, Agent 1: 0.08404652]) [Noise stdev: 0.04233797972178948]
Episode 1039/5000 (66 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0582 ; (Q Loss Avg: [Agent 0: 0.00057278, Agent 1: 0.00030422] ; E(V) Avg: [Agent 0: 0.16599766, Agent 1: 0.08496104]) [Noise stdev: 0.0422533037623459]
Episode 1040/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0592 ; (Q Loss Avg: [Agent 0: 0.00064329, Agent 1: 0.00033013] ; E(V) Avg: [Agent 0: 0.16298802, Agent 1: 0.08613870]) [Noise stdev: 0.042168797154821205]
Episode 1041/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0602 ; (Q Loss Avg: [Agent 0: 0.00054944, Agent 1: 0.00028786] ; E(V) Avg: [Agent 0: 0.16745307, Agent 1: 0.08681023]) [Noise stdev: 0.042084459560511565]
Episode 1042/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0602 ; (Q Loss Avg: [Agent 0: 0.00060988, Agent 1: 0.00029777] ; E(V) Avg: [Agent 0: 0.16548524, Agent 1: 0.08376594]) [Noise stdev: 0.04200029064139054]
Episode 1043/5000 (29 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0601 ; (Q Loss Avg: [Agent 0: 0.00063175, Agent 1: 0.00027139] ; E(V) Avg: [Agent 0: 0.16638950, Agent 1: 0.08306091]) [Noise stdev: 0.04191629006010776]
Episode 1044/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0601 ; (Q Loss Avg: [Agent 0: 0.00056131, Agent 1: 0.00028280] ; E(V) Avg: [Agent 0: 0.16914067, Agent 1: 0.08374647]) [Noise stdev: 0.041832457479987546]
Episode 1045/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0601 ; (Q Loss Avg: [Agent 0: 0.00056857, Agent 1: 0.00025963] ; E(V) Avg: [Agent 0: 0.16288644, Agent 1: 0.08413305]) [Noise stdev: 0.04174879256502757]
Episode 1046/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0611 ; (Q Loss Avg: [Agent 0: 0.00069808, Agent 1: 0.00029149] ; E(V) Avg: [Agent 0: 0.16517898, Agent 1: 0.08602289]) [Noise stdev: 0.041665294979897516]
Episode 1047/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0621 ; (Q Loss Avg: [Agent 0: 0.00057449, Agent 1: 0.00029966] ; E(V) Avg: [Agent 0: 0.15947662, Agent 1: 0.08300287]) [Noise stdev: 0.04158196438993772]
Episode 1048/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0621 ; (Q Loss Avg: [Agent 0: 0.00056864, Agent 1: 0.00032085] ; E(V) Avg: [Agent 0: 0.16337690, Agent 1: 0.08329824]) [Noise stdev: 0.04149880046115784]
Episode 1049/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00058811, Agent 1: 0.00033661] ; E(V) Avg: [Agent 0: 0.16298416, Agent 1: 0.08530012]) [Noise stdev: 0.04141580286023553]
Episode 1050/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0641 ; (Q Loss Avg: [Agent 0: 0.00058926, Agent 1: 0.00029482] ; E(V) Avg: [Agent 0: 0.16280862, Agent 1: 0.08565348]) [Noise stdev: 0.04133297125451506]
Episode 1051/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0651 ; (Q Loss Avg: [Agent 0: 0.00055992, Agent 1: 0.00027597] ; E(V) Avg: [Agent 0: 0.16108505, Agent 1: 0.08551392]) [Noise stdev: 0.041250305312006026]
Episode 1052/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0651 ; (Q Loss Avg: [Agent 0: 0.00055635, Agent 1: 0.00028053] ; E(V) Avg: [Agent 0: 0.16200289, Agent 1: 0.08434508]) [Noise stdev: 0.04116780470138202]
Episode 1053/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0651 ; (Q Loss Avg: [Agent 0: 0.00063426, Agent 1: 0.00027534] ; E(V) Avg: [Agent 0: 0.16170878, Agent 1: 0.08502256]) [Noise stdev: 0.04108546909197925]
Episode 1054/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0651 ; (Q Loss Avg: [Agent 0: 0.00077402, Agent 1: 0.00023587] ; E(V) Avg: [Agent 0: 0.16140714, Agent 1: 0.08347257]) [Noise stdev: 0.04100329815379529]
Episode 1055/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0651 ; (Q Loss Avg: [Agent 0: 0.00065119, Agent 1: 0.00024759] ; E(V) Avg: [Agent 0: 0.16740716, Agent 1: 0.08385345]) [Noise stdev: 0.0409212915574877]
Episode 1056/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0651 ; (Q Loss Avg: [Agent 0: 0.00065520, Agent 1: 0.00028135] ; E(V) Avg: [Agent 0: 0.16320929, Agent 1: 0.08284709]) [Noise stdev: 0.04083944897437272]
Episode 1057/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00060845, Agent 1: 0.00026378] ; E(V) Avg: [Agent 0: 0.16656891, Agent 1: 0.08533205]) [Noise stdev: 0.04075777007642398]
Episode 1058/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00062068, Agent 1: 0.00030027] ; E(V) Avg: [Agent 0: 0.16712479, Agent 1: 0.08410582]) [Noise stdev: 0.04067625453627113]
Episode 1059/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00057301, Agent 1: 0.00027650] ; E(V) Avg: [Agent 0: 0.16745478, Agent 1: 0.08337164]) [Noise stdev: 0.040594902027198586]
Episode 1060/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00058328, Agent 1: 0.00024336] ; E(V) Avg: [Agent 0: 0.16194838, Agent 1: 0.08560159]) [Noise stdev: 0.04051371222314419]
Episode 1061/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0671 ; (Q Loss Avg: [Agent 0: 0.00062756, Agent 1: 0.00026536] ; E(V) Avg: [Agent 0: 0.16316595, Agent 1: 0.08505293]) [Noise stdev: 0.0404326847986979]
Episode 1062/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00064016, Agent 1: 0.00023834] ; E(V) Avg: [Agent 0: 0.16496433, Agent 1: 0.08374626]) [Noise stdev: 0.04035181942910051]
Episode 1063/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0662 ; (Q Loss Avg: [Agent 0: 0.00053536, Agent 1: 0.00025627] ; E(V) Avg: [Agent 0: 0.16431149, Agent 1: 0.08458434]) [Noise stdev: 0.040271115790242305]
Episode 1064/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0662 ; (Q Loss Avg: [Agent 0: 0.00053415, Agent 1: 0.00026162] ; E(V) Avg: [Agent 0: 0.16369792, Agent 1: 0.08381403]) [Noise stdev: 0.04019057355866182]
Episode 1065/5000 (76 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0682 ; (Q Loss Avg: [Agent 0: 0.00060013, Agent 1: 0.00026368] ; E(V) Avg: [Agent 0: 0.16305504, Agent 1: 0.08313696]) [Noise stdev: 0.040110192411544496]
Episode 1066/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0682 ; (Q Loss Avg: [Agent 0: 0.00050968, Agent 1: 0.00028139] ; E(V) Avg: [Agent 0: 0.15970787, Agent 1: 0.08393970]) [Noise stdev: 0.04002997202672141]
Episode 1067/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0662 ; (Q Loss Avg: [Agent 0: 0.00045442, Agent 1: 0.00025000] ; E(V) Avg: [Agent 0: 0.16057919, Agent 1: 0.08535962]) [Noise stdev: 0.03994991208266797]
Episode 1068/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0672 ; (Q Loss Avg: [Agent 0: 0.00059605, Agent 1: 0.00024236] ; E(V) Avg: [Agent 0: 0.16096497, Agent 1: 0.08313572]) [Noise stdev: 0.03987001225850263]
Episode 1069/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0682 ; (Q Loss Avg: [Agent 0: 0.00068028, Agent 1: 0.00029390] ; E(V) Avg: [Agent 0: 0.16414528, Agent 1: 0.08493178]) [Noise stdev: 0.039790272233985624]
Episode 1070/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0682 ; (Q Loss Avg: [Agent 0: 0.00056343, Agent 1: 0.00027428] ; E(V) Avg: [Agent 0: 0.16002866, Agent 1: 0.08215953]) [Noise stdev: 0.03971069168951765]
Episode 1071/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0682 ; (Q Loss Avg: [Agent 0: 0.00057890, Agent 1: 0.00024165] ; E(V) Avg: [Agent 0: 0.16201755, Agent 1: 0.08483050]) [Noise stdev: 0.03963127030613862]
Episode 1072/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0682 ; (Q Loss Avg: [Agent 0: 0.00062389, Agent 1: 0.00023307] ; E(V) Avg: [Agent 0: 0.16217902, Agent 1: 0.08693227]) [Noise stdev: 0.03955200776552634]
Episode 1073/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0682 ; (Q Loss Avg: [Agent 0: 0.00059565, Agent 1: 0.00022373] ; E(V) Avg: [Agent 0: 0.15919043, Agent 1: 0.08430152]) [Noise stdev: 0.039472903749995285]
Episode 1074/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0682 ; (Q Loss Avg: [Agent 0: 0.00072746, Agent 1: 0.00019732] ; E(V) Avg: [Agent 0: 0.16554268, Agent 1: 0.08180728]) [Noise stdev: 0.0393939579424953]
Episode 1075/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0692 ; (Q Loss Avg: [Agent 0: 0.00059628, Agent 1: 0.00025212] ; E(V) Avg: [Agent 0: 0.15908262, Agent 1: 0.08253469]) [Noise stdev: 0.03931517002661031]
Episode 1076/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0701 ; (Q Loss Avg: [Agent 0: 0.00066876, Agent 1: 0.00023033] ; E(V) Avg: [Agent 0: 0.16411981, Agent 1: 0.08455052]) [Noise stdev: 0.03923653968655709]
Episode 1077/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0701 ; (Q Loss Avg: [Agent 0: 0.00056035, Agent 1: 0.00022966] ; E(V) Avg: [Agent 0: 0.16080885, Agent 1: 0.08390291]) [Noise stdev: 0.039158066607183975]
Episode 1078/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0701 ; (Q Loss Avg: [Agent 0: 0.00070332, Agent 1: 0.00025760] ; E(V) Avg: [Agent 0: 0.16218175, Agent 1: 0.08327068]) [Noise stdev: 0.03907975047396961]
Episode 1079/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0701 ; (Q Loss Avg: [Agent 0: 0.00062745, Agent 1: 0.00019561] ; E(V) Avg: [Agent 0: 0.16262067, Agent 1: 0.08179382]) [Noise stdev: 0.03900159097302167]
Episode 1080/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0711 ; (Q Loss Avg: [Agent 0: 0.00064735, Agent 1: 0.00021764] ; E(V) Avg: [Agent 0: 0.16180993, Agent 1: 0.08223393]) [Noise stdev: 0.03892358779107563]
Episode 1081/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0720 ; (Q Loss Avg: [Agent 0: 0.00060446, Agent 1: 0.00026427] ; E(V) Avg: [Agent 0: 0.15761576, Agent 1: 0.08469425]) [Noise stdev: 0.03884574061549348]
Episode 1082/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0700 ; (Q Loss Avg: [Agent 0: 0.00059988, Agent 1: 0.00023362] ; E(V) Avg: [Agent 0: 0.16003801, Agent 1: 0.08480370]) [Noise stdev: 0.03876804913426249]
Episode 1083/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0710 ; (Q Loss Avg: [Agent 0: 0.00074955, Agent 1: 0.00025262] ; E(V) Avg: [Agent 0: 0.16074467, Agent 1: 0.08326724]) [Noise stdev: 0.03869051303599396]
Episode 1084/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0700 ; (Q Loss Avg: [Agent 0: 0.00056083, Agent 1: 0.00024884] ; E(V) Avg: [Agent 0: 0.15831048, Agent 1: 0.08488730]) [Noise stdev: 0.038613132009921974]
Episode 1085/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0700 ; (Q Loss Avg: [Agent 0: 0.00060474, Agent 1: 0.00022911] ; E(V) Avg: [Agent 0: 0.15877754, Agent 1: 0.08278316]) [Noise stdev: 0.03853590574590213]
Episode 1086/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0690 ; (Q Loss Avg: [Agent 0: 0.00050721, Agent 1: 0.00025525] ; E(V) Avg: [Agent 0: 0.15904538, Agent 1: 0.08509789]) [Noise stdev: 0.038458833934410325]
Episode 1087/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0690 ; (Q Loss Avg: [Agent 0: 0.00066121, Agent 1: 0.00025228] ; E(V) Avg: [Agent 0: 0.16190929, Agent 1: 0.08457553]) [Noise stdev: 0.038381916266541506]
Episode 1088/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0690 ; (Q Loss Avg: [Agent 0: 0.00053730, Agent 1: 0.00027163] ; E(V) Avg: [Agent 0: 0.15873953, Agent 1: 0.08180671]) [Noise stdev: 0.03830515243400842]
Episode 1089/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0700 ; (Q Loss Avg: [Agent 0: 0.00055608, Agent 1: 0.00023035] ; E(V) Avg: [Agent 0: 0.15799314, Agent 1: 0.08280620]) [Noise stdev: 0.038228542129140404]
Episode 1090/5000 (32 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0709 ; (Q Loss Avg: [Agent 0: 0.00056330, Agent 1: 0.00025997] ; E(V) Avg: [Agent 0: 0.15762420, Agent 1: 0.08390871]) [Noise stdev: 0.03815208504488212]
Episode 1091/5000 (32 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0718 ; (Q Loss Avg: [Agent 0: 0.00075538, Agent 1: 0.00025899] ; E(V) Avg: [Agent 0: 0.15847147, Agent 1: 0.08199441]) [Noise stdev: 0.03807578087479236]
Episode 1092/5000 (32 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0727 ; (Q Loss Avg: [Agent 0: 0.00068996, Agent 1: 0.00025343] ; E(V) Avg: [Agent 0: 0.16033973, Agent 1: 0.08257137]) [Noise stdev: 0.03799962931304277]
Episode 1093/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0727 ; (Q Loss Avg: [Agent 0: 0.00051256, Agent 1: 0.00025576] ; E(V) Avg: [Agent 0: 0.15498421, Agent 1: 0.08491698]) [Noise stdev: 0.037923630054416686]
Episode 1094/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0718 ; (Q Loss Avg: [Agent 0: 0.00067214, Agent 1: 0.00025045] ; E(V) Avg: [Agent 0: 0.15695681, Agent 1: 0.08248096]) [Noise stdev: 0.03784778279430785]
Episode 1095/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0718 ; (Q Loss Avg: [Agent 0: 0.00068224, Agent 1: 0.00028702] ; E(V) Avg: [Agent 0: 0.15725270, Agent 1: 0.08063735]) [Noise stdev: 0.037772087228719234]
Episode 1096/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0728 ; (Q Loss Avg: [Agent 0: 0.00056573, Agent 1: 0.00029212] ; E(V) Avg: [Agent 0: 0.15869437, Agent 1: 0.08086638]) [Noise stdev: 0.037696543054261795]
Episode 1097/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0728 ; (Q Loss Avg: [Agent 0: 0.00065650, Agent 1: 0.00030375] ; E(V) Avg: [Agent 0: 0.16240139, Agent 1: 0.08228868]) [Noise stdev: 0.03762114996815327]
Episode 1098/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0728 ; (Q Loss Avg: [Agent 0: 0.00057855, Agent 1: 0.00027685] ; E(V) Avg: [Agent 0: 0.16134173, Agent 1: 0.08084654]) [Noise stdev: 0.037545907668216964]
Episode 1099/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0718 ; (Q Loss Avg: [Agent 0: 0.00057547, Agent 1: 0.00025629] ; E(V) Avg: [Agent 0: 0.15689616, Agent 1: 0.08222450]) [Noise stdev: 0.03747081585288053]
Episode 1100/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0708 ; (Q Loss Avg: [Agent 0: 0.00054787, Agent 1: 0.00028152] ; E(V) Avg: [Agent 0: 0.15687007, Agent 1: 0.08250743]) [Noise stdev: 0.03739587422117477]
Episode 1101/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0708 ; (Q Loss Avg: [Agent 0: 0.00060421, Agent 1: 0.00024597] ; E(V) Avg: [Agent 0: 0.15631875, Agent 1: 0.08338432]) [Noise stdev: 0.03732108247273242]
Episode 1102/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0707 ; (Q Loss Avg: [Agent 0: 0.00064930, Agent 1: 0.00022715] ; E(V) Avg: [Agent 0: 0.15953569, Agent 1: 0.08280375]) [Noise stdev: 0.037246440307786954]
Episode 1103/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0707 ; (Q Loss Avg: [Agent 0: 0.00063393, Agent 1: 0.00021946] ; E(V) Avg: [Agent 0: 0.16031281, Agent 1: 0.08323807]) [Noise stdev: 0.03717194742717138]
Episode 1104/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0717 ; (Q Loss Avg: [Agent 0: 0.00070936, Agent 1: 0.00025806] ; E(V) Avg: [Agent 0: 0.15983074, Agent 1: 0.08134096]) [Noise stdev: 0.03709760353231704]
Episode 1105/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0707 ; (Q Loss Avg: [Agent 0: 0.00056601, Agent 1: 0.00023937] ; E(V) Avg: [Agent 0: 0.15971119, Agent 1: 0.08282407]) [Noise stdev: 0.0370234083252524]
Episode 1106/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0717 ; (Q Loss Avg: [Agent 0: 0.00060699, Agent 1: 0.00027704] ; E(V) Avg: [Agent 0: 0.16024083, Agent 1: 0.08376672]) [Noise stdev: 0.0369493615086019]
Episode 1107/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0717 ; (Q Loss Avg: [Agent 0: 0.00056693, Agent 1: 0.00023479] ; E(V) Avg: [Agent 0: 0.15958024, Agent 1: 0.08345578]) [Noise stdev: 0.03687546278558469]
Episode 1108/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0717 ; (Q Loss Avg: [Agent 0: 0.00070864, Agent 1: 0.00021365] ; E(V) Avg: [Agent 0: 0.15826531, Agent 1: 0.08036425]) [Noise stdev: 0.03680171186001352]
Episode 1109/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0718 ; (Q Loss Avg: [Agent 0: 0.00064816, Agent 1: 0.00024951] ; E(V) Avg: [Agent 0: 0.16037522, Agent 1: 0.08284575]) [Noise stdev: 0.03672810843629349]
Episode 1110/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0718 ; (Q Loss Avg: [Agent 0: 0.00054736, Agent 1: 0.00024405] ; E(V) Avg: [Agent 0: 0.15637099, Agent 1: 0.08060748]) [Noise stdev: 0.0366546522194209]
Episode 1111/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0718 ; (Q Loss Avg: [Agent 0: 0.00062479, Agent 1: 0.00023839] ; E(V) Avg: [Agent 0: 0.15936739, Agent 1: 0.08013338]) [Noise stdev: 0.03658134291498206]
Episode 1112/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0718 ; (Q Loss Avg: [Agent 0: 0.00055759, Agent 1: 0.00022315] ; E(V) Avg: [Agent 0: 0.16243692, Agent 1: 0.08105590]) [Noise stdev: 0.036508180229152094]
Episode 1113/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0719 ; (Q Loss Avg: [Agent 0: 0.00068987, Agent 1: 0.00024663] ; E(V) Avg: [Agent 0: 0.15891298, Agent 1: 0.08211559]) [Noise stdev: 0.03643516386869379]
Episode 1114/5000 (20 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0709 ; (Q Loss Avg: [Agent 0: 0.00091051, Agent 1: 0.00028140] ; E(V) Avg: [Agent 0: 0.15999525, Agent 1: 0.08095969]) [Noise stdev: 0.0363622935409564]
Episode 1115/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0699 ; (Q Loss Avg: [Agent 0: 0.00086014, Agent 1: 0.00024851] ; E(V) Avg: [Agent 0: 0.16245228, Agent 1: 0.08112120]) [Noise stdev: 0.03628956895387449]
Episode 1116/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0689 ; (Q Loss Avg: [Agent 0: 0.00084103, Agent 1: 0.00027411] ; E(V) Avg: [Agent 0: 0.16325503, Agent 1: 0.08038827]) [Noise stdev: 0.03621698981596674]
Episode 1117/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0689 ; (Q Loss Avg: [Agent 0: 0.00072871, Agent 1: 0.00021915] ; E(V) Avg: [Agent 0: 0.15551979, Agent 1: 0.07966693]) [Noise stdev: 0.0361445558363348]
Episode 1118/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0680 ; (Q Loss Avg: [Agent 0: 0.00063109, Agent 1: 0.00021572] ; E(V) Avg: [Agent 0: 0.15546930, Agent 1: 0.08099226]) [Noise stdev: 0.036072266724662135]
Episode 1119/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0681 ; (Q Loss Avg: [Agent 0: 0.00061889, Agent 1: 0.00023510] ; E(V) Avg: [Agent 0: 0.15748257, Agent 1: 0.08125446]) [Noise stdev: 0.03600012219121281]
Episode 1120/5000 (33 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0680 ; (Q Loss Avg: [Agent 0: 0.00063941, Agent 1: 0.00021491] ; E(V) Avg: [Agent 0: 0.16005108, Agent 1: 0.08011982]) [Noise stdev: 0.035928121946830385]
Episode 1121/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0680 ; (Q Loss Avg: [Agent 0: 0.00064288, Agent 1: 0.00024658] ; E(V) Avg: [Agent 0: 0.15688199, Agent 1: 0.08209919]) [Noise stdev: 0.03585626570293672]
Episode 1122/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0681 ; (Q Loss Avg: [Agent 0: 0.00066651, Agent 1: 0.00022256] ; E(V) Avg: [Agent 0: 0.15670946, Agent 1: 0.08124805]) [Noise stdev: 0.03578455317153085]
Episode 1123/5000 (37 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0681 ; (Q Loss Avg: [Agent 0: 0.00065083, Agent 1: 0.00023611] ; E(V) Avg: [Agent 0: 0.15829708, Agent 1: 0.08213718]) [Noise stdev: 0.03571298406518779]
Episode 1124/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0681 ; (Q Loss Avg: [Agent 0: 0.00079528, Agent 1: 0.00025547] ; E(V) Avg: [Agent 0: 0.15797216, Agent 1: 0.08120719]) [Noise stdev: 0.035641558097057414]
Episode 1125/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0681 ; (Q Loss Avg: [Agent 0: 0.00074716, Agent 1: 0.00021022] ; E(V) Avg: [Agent 0: 0.15753737, Agent 1: 0.08134103]) [Noise stdev: 0.0355702749808633]
Episode 1126/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00085794, Agent 1: 0.00024883] ; E(V) Avg: [Agent 0: 0.15120913, Agent 1: 0.08030271]) [Noise stdev: 0.03549913443090157]
Episode 1127/5000 (32 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00070193, Agent 1: 0.00023044] ; E(V) Avg: [Agent 0: 0.15469875, Agent 1: 0.07989796]) [Noise stdev: 0.03542813616203977]
Episode 1128/5000 (89 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0671 ; (Q Loss Avg: [Agent 0: 0.00067409, Agent 1: 0.00026685] ; E(V) Avg: [Agent 0: 0.15682176, Agent 1: 0.08086664]) [Noise stdev: 0.03535727988971569]
Episode 1129/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0671 ; (Q Loss Avg: [Agent 0: 0.00070885, Agent 1: 0.00026868] ; E(V) Avg: [Agent 0: 0.16224573, Agent 1: 0.08037300]) [Noise stdev: 0.03528656532993626]
Episode 1130/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0671 ; (Q Loss Avg: [Agent 0: 0.00055326, Agent 1: 0.00022877] ; E(V) Avg: [Agent 0: 0.15754815, Agent 1: 0.08097116]) [Noise stdev: 0.03521599219927639]
Episode 1131/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0671 ; (Q Loss Avg: [Agent 0: 0.00073473, Agent 1: 0.00021183] ; E(V) Avg: [Agent 0: 0.15794087, Agent 1: 0.07993603]) [Noise stdev: 0.03514556021487784]
Episode 1132/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0671 ; (Q Loss Avg: [Agent 0: 0.00065285, Agent 1: 0.00022178] ; E(V) Avg: [Agent 0: 0.15819094, Agent 1: 0.08058594]) [Noise stdev: 0.03507526909444808]
Episode 1133/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0671 ; (Q Loss Avg: [Agent 0: 0.00069335, Agent 1: 0.00022117] ; E(V) Avg: [Agent 0: 0.15603531, Agent 1: 0.08170841]) [Noise stdev: 0.035005118556259186]
Episode 1134/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00078281, Agent 1: 0.00022318] ; E(V) Avg: [Agent 0: 0.15895862, Agent 1: 0.08259875]) [Noise stdev: 0.03493510831914667]
Episode 1135/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0651 ; (Q Loss Avg: [Agent 0: 0.00069400, Agent 1: 0.00020253] ; E(V) Avg: [Agent 0: 0.15441861, Agent 1: 0.08192706]) [Noise stdev: 0.03486523810250838]
Episode 1136/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0651 ; (Q Loss Avg: [Agent 0: 0.00064637, Agent 1: 0.00021978] ; E(V) Avg: [Agent 0: 0.15820287, Agent 1: 0.08194958]) [Noise stdev: 0.03479550762630336]
Episode 1137/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0651 ; (Q Loss Avg: [Agent 0: 0.00064260, Agent 1: 0.00021789] ; E(V) Avg: [Agent 0: 0.15939617, Agent 1: 0.07939442]) [Noise stdev: 0.034725916611050754]
Episode 1138/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0641 ; (Q Loss Avg: [Agent 0: 0.00067047, Agent 1: 0.00023488] ; E(V) Avg: [Agent 0: 0.16187759, Agent 1: 0.07941290]) [Noise stdev: 0.034656464777828654]
Episode 1139/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00067406, Agent 1: 0.00022284] ; E(V) Avg: [Agent 0: 0.16313954, Agent 1: 0.07897028]) [Noise stdev: 0.034587151848273]
Episode 1140/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0621 ; (Q Loss Avg: [Agent 0: 0.00068556, Agent 1: 0.00024920] ; E(V) Avg: [Agent 0: 0.15680902, Agent 1: 0.08131038]) [Noise stdev: 0.03451797754457645]
Episode 1141/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0621 ; (Q Loss Avg: [Agent 0: 0.00063482, Agent 1: 0.00024221] ; E(V) Avg: [Agent 0: 0.15967077, Agent 1: 0.08061191]) [Noise stdev: 0.034448941589487296]
Episode 1142/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00065535, Agent 1: 0.00024103] ; E(V) Avg: [Agent 0: 0.15485473, Agent 1: 0.08000195]) [Noise stdev: 0.03438004370630832]
Episode 1143/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0622 ; (Q Loss Avg: [Agent 0: 0.00061385, Agent 1: 0.00021370] ; E(V) Avg: [Agent 0: 0.16132289, Agent 1: 0.08151723]) [Noise stdev: 0.034311283618895703]
Episode 1144/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0622 ; (Q Loss Avg: [Agent 0: 0.00068510, Agent 1: 0.00018868] ; E(V) Avg: [Agent 0: 0.15300640, Agent 1: 0.08138855]) [Noise stdev: 0.034242661051657915]
Episode 1145/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0612 ; (Q Loss Avg: [Agent 0: 0.00078612, Agent 1: 0.00026115] ; E(V) Avg: [Agent 0: 0.15628231, Agent 1: 0.07888296]) [Noise stdev: 0.0341741757295546]
Episode 1146/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0602 ; (Q Loss Avg: [Agent 0: 0.00071765, Agent 1: 0.00025124] ; E(V) Avg: [Agent 0: 0.15485085, Agent 1: 0.07956152]) [Noise stdev: 0.03410582737809549]
Episode 1147/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0602 ; (Q Loss Avg: [Agent 0: 0.00062467, Agent 1: 0.00018496] ; E(V) Avg: [Agent 0: 0.15688061, Agent 1: 0.08173314]) [Noise stdev: 0.0340376157233393]
Episode 1148/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0612 ; (Q Loss Avg: [Agent 0: 0.00058385, Agent 1: 0.00021207] ; E(V) Avg: [Agent 0: 0.15597703, Agent 1: 0.08054620]) [Noise stdev: 0.033969540491892616]
Episode 1149/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0602 ; (Q Loss Avg: [Agent 0: 0.00077978, Agent 1: 0.00026313] ; E(V) Avg: [Agent 0: 0.15121485, Agent 1: 0.08072630]) [Noise stdev: 0.03390160141090883]
Episode 1150/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0602 ; (Q Loss Avg: [Agent 0: 0.00068885, Agent 1: 0.00024346] ; E(V) Avg: [Agent 0: 0.15494259, Agent 1: 0.08053407]) [Noise stdev: 0.03383379820808701]
Episode 1151/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0602 ; (Q Loss Avg: [Agent 0: 0.00061375, Agent 1: 0.00021081] ; E(V) Avg: [Agent 0: 0.15735826, Agent 1: 0.08045232]) [Noise stdev: 0.03376613061167084]
Episode 1152/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0612 ; (Q Loss Avg: [Agent 0: 0.00060559, Agent 1: 0.00020782] ; E(V) Avg: [Agent 0: 0.15618753, Agent 1: 0.07906145]) [Noise stdev: 0.033698598350447494]
Episode 1153/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0602 ; (Q Loss Avg: [Agent 0: 0.00091786, Agent 1: 0.00020206] ; E(V) Avg: [Agent 0: 0.16065235, Agent 1: 0.07706909]) [Noise stdev: 0.0336312011537466]
Episode 1154/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0611 ; (Q Loss Avg: [Agent 0: 0.00073713, Agent 1: 0.00022064] ; E(V) Avg: [Agent 0: 0.15812706, Agent 1: 0.07968650]) [Noise stdev: 0.03356393875143911]
Episode 1155/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0611 ; (Q Loss Avg: [Agent 0: 0.00065104, Agent 1: 0.00020313] ; E(V) Avg: [Agent 0: 0.15695917, Agent 1: 0.08022422]) [Noise stdev: 0.03349681087393623]
Episode 1156/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0611 ; (Q Loss Avg: [Agent 0: 0.00063857, Agent 1: 0.00024634] ; E(V) Avg: [Agent 0: 0.15734481, Agent 1: 0.07983466]) [Noise stdev: 0.03342981725218836]
Episode 1157/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0601 ; (Q Loss Avg: [Agent 0: 0.00054642, Agent 1: 0.00024355] ; E(V) Avg: [Agent 0: 0.15655605, Agent 1: 0.08006719]) [Noise stdev: 0.033362957617683986]
Episode 1158/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0611 ; (Q Loss Avg: [Agent 0: 0.00063326, Agent 1: 0.00020086] ; E(V) Avg: [Agent 0: 0.15742905, Agent 1: 0.08009289]) [Noise stdev: 0.033296231702448616]
Episode 1159/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0621 ; (Q Loss Avg: [Agent 0: 0.00063351, Agent 1: 0.00019287] ; E(V) Avg: [Agent 0: 0.15696163, Agent 1: 0.08019806]) [Noise stdev: 0.03322963923904372]
Episode 1160/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00060745, Agent 1: 0.00024704] ; E(V) Avg: [Agent 0: 0.15536781, Agent 1: 0.08048729]) [Noise stdev: 0.033163179960565635]
Episode 1161/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00064573, Agent 1: 0.00022257] ; E(V) Avg: [Agent 0: 0.15708011, Agent 1: 0.08076272]) [Noise stdev: 0.0330968536006445]
Episode 1162/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00065003, Agent 1: 0.00020936] ; E(V) Avg: [Agent 0: 0.16093629, Agent 1: 0.07961497]) [Noise stdev: 0.03303065989344321]
Episode 1163/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0621 ; (Q Loss Avg: [Agent 0: 0.00050632, Agent 1: 0.00019434] ; E(V) Avg: [Agent 0: 0.16409107, Agent 1: 0.08138834]) [Noise stdev: 0.03296459857365633]
Episode 1164/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0611 ; (Q Loss Avg: [Agent 0: 0.00062469, Agent 1: 0.00017639] ; E(V) Avg: [Agent 0: 0.15650167, Agent 1: 0.08111731]) [Noise stdev: 0.03289866937650902]
Episode 1165/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0601 ; (Q Loss Avg: [Agent 0: 0.00052757, Agent 1: 0.00019718] ; E(V) Avg: [Agent 0: 0.16112509, Agent 1: 0.08028462]) [Noise stdev: 0.032832872037756]
Episode 1166/5000 (68 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0621 ; (Q Loss Avg: [Agent 0: 0.00057237, Agent 1: 0.00019371] ; E(V) Avg: [Agent 0: 0.15670960, Agent 1: 0.07951221]) [Noise stdev: 0.03276720629368049]
Episode 1167/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00062970, Agent 1: 0.00020691] ; E(V) Avg: [Agent 0: 0.15964130, Agent 1: 0.08065111]) [Noise stdev: 0.03270167188109313]
Episode 1168/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0621 ; (Q Loss Avg: [Agent 0: 0.00064865, Agent 1: 0.00020869] ; E(V) Avg: [Agent 0: 0.15798972, Agent 1: 0.08227073]) [Noise stdev: 0.03263626853733095]
Episode 1169/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0621 ; (Q Loss Avg: [Agent 0: 0.00068555, Agent 1: 0.00023011] ; E(V) Avg: [Agent 0: 0.15722503, Agent 1: 0.08074061]) [Noise stdev: 0.032570996000256286]
Episode 1170/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00056835, Agent 1: 0.00021986] ; E(V) Avg: [Agent 0: 0.16191981, Agent 1: 0.08284580]) [Noise stdev: 0.03250585400825577]
Episode 1171/5000 (26 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0621 ; (Q Loss Avg: [Agent 0: 0.00063609, Agent 1: 0.00022612] ; E(V) Avg: [Agent 0: 0.16163971, Agent 1: 0.08076954]) [Noise stdev: 0.03244084230023926]
Episode 1172/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00080101, Agent 1: 0.00021329] ; E(V) Avg: [Agent 0: 0.16005655, Agent 1: 0.07969641]) [Noise stdev: 0.032375960615638785]
Episode 1173/5000 (29 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0640 ; (Q Loss Avg: [Agent 0: 0.00061170, Agent 1: 0.00019500] ; E(V) Avg: [Agent 0: 0.15612859, Agent 1: 0.07885758]) [Noise stdev: 0.032311208694407505]
Episode 1174/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0650 ; (Q Loss Avg: [Agent 0: 0.00075428, Agent 1: 0.00021671] ; E(V) Avg: [Agent 0: 0.16111347, Agent 1: 0.08149373]) [Noise stdev: 0.032246586277018686]
Episode 1175/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0640 ; (Q Loss Avg: [Agent 0: 0.00068799, Agent 1: 0.00020148] ; E(V) Avg: [Agent 0: 0.15737708, Agent 1: 0.08210381]) [Noise stdev: 0.03218209310446465]
Episode 1176/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00060802, Agent 1: 0.00018609] ; E(V) Avg: [Agent 0: 0.16327126, Agent 1: 0.07996742]) [Noise stdev: 0.03211772891825572]
Episode 1177/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0641 ; (Q Loss Avg: [Agent 0: 0.00065219, Agent 1: 0.00019898] ; E(V) Avg: [Agent 0: 0.15982456, Agent 1: 0.07863152]) [Noise stdev: 0.032053493460419205]
Episode 1178/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00073057, Agent 1: 0.00021882] ; E(V) Avg: [Agent 0: 0.16004037, Agent 1: 0.08055722]) [Noise stdev: 0.03198938647349837]
Episode 1179/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0641 ; (Q Loss Avg: [Agent 0: 0.00060685, Agent 1: 0.00023293] ; E(V) Avg: [Agent 0: 0.16007169, Agent 1: 0.08032703]) [Noise stdev: 0.03192540770055137]
Episode 1180/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0631 ; (Q Loss Avg: [Agent 0: 0.00063575, Agent 1: 0.00023760] ; E(V) Avg: [Agent 0: 0.16071986, Agent 1: 0.07923566]) [Noise stdev: 0.031861556885150265]
Episode 1181/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0622 ; (Q Loss Avg: [Agent 0: 0.00053937, Agent 1: 0.00023062] ; E(V) Avg: [Agent 0: 0.15501730, Agent 1: 0.08058330]) [Noise stdev: 0.031797833771379964]
Episode 1182/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0632 ; (Q Loss Avg: [Agent 0: 0.00064883, Agent 1: 0.00022744] ; E(V) Avg: [Agent 0: 0.15910584, Agent 1: 0.07999051]) [Noise stdev: 0.03173423810383721]
Episode 1183/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0632 ; (Q Loss Avg: [Agent 0: 0.00067871, Agent 1: 0.00019275] ; E(V) Avg: [Agent 0: 0.15766442, Agent 1: 0.07803944]) [Noise stdev: 0.03167076962762953]
Episode 1184/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0632 ; (Q Loss Avg: [Agent 0: 0.00067230, Agent 1: 0.00020379] ; E(V) Avg: [Agent 0: 0.15308457, Agent 1: 0.08096562]) [Noise stdev: 0.03160742808837427]
Episode 1185/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0632 ; (Q Loss Avg: [Agent 0: 0.00069792, Agent 1: 0.00020378] ; E(V) Avg: [Agent 0: 0.15749522, Agent 1: 0.07950949]) [Noise stdev: 0.03154421323219753]
Episode 1186/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0632 ; (Q Loss Avg: [Agent 0: 0.00056946, Agent 1: 0.00018522] ; E(V) Avg: [Agent 0: 0.15788084, Agent 1: 0.08076596]) [Noise stdev: 0.03148112480573313]
Episode 1187/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0622 ; (Q Loss Avg: [Agent 0: 0.00067245, Agent 1: 0.00018886] ; E(V) Avg: [Agent 0: 0.15606847, Agent 1: 0.08022455]) [Noise stdev: 0.03141816255612167]
Episode 1188/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0622 ; (Q Loss Avg: [Agent 0: 0.00067096, Agent 1: 0.00019347] ; E(V) Avg: [Agent 0: 0.15547207, Agent 1: 0.07958138]) [Noise stdev: 0.03135532623100942]
Episode 1189/5000 (67 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0632 ; (Q Loss Avg: [Agent 0: 0.00065011, Agent 1: 0.00020119] ; E(V) Avg: [Agent 0: 0.15840462, Agent 1: 0.07983447]) [Noise stdev: 0.03129261557854741]
Episode 1190/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0633 ; (Q Loss Avg: [Agent 0: 0.00073381, Agent 1: 0.00019060] ; E(V) Avg: [Agent 0: 0.16131656, Agent 1: 0.08076629]) [Noise stdev: 0.031230030347390313]
Episode 1191/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0634 ; (Q Loss Avg: [Agent 0: 0.00073745, Agent 1: 0.00020229] ; E(V) Avg: [Agent 0: 0.16148812, Agent 1: 0.08156842]) [Noise stdev: 0.031167570286695534]
Episode 1192/5000 (18 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0625 ; (Q Loss Avg: [Agent 0: 0.00079620, Agent 1: 0.00020283] ; E(V) Avg: [Agent 0: 0.15906758, Agent 1: 0.08045144]) [Noise stdev: 0.031105235146122144]
Episode 1193/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0625 ; (Q Loss Avg: [Agent 0: 0.00066940, Agent 1: 0.00019490] ; E(V) Avg: [Agent 0: 0.16112617, Agent 1: 0.08086909]) [Noise stdev: 0.0310430246758299]
Episode 1194/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0635 ; (Q Loss Avg: [Agent 0: 0.00077661, Agent 1: 0.00021095] ; E(V) Avg: [Agent 0: 0.15773025, Agent 1: 0.08044947]) [Noise stdev: 0.03098093862647824]
Episode 1195/5000 (29 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0644 ; (Q Loss Avg: [Agent 0: 0.00072221, Agent 1: 0.00019446] ; E(V) Avg: [Agent 0: 0.16051163, Agent 1: 0.08015477]) [Noise stdev: 0.030918976749225283]
Episode 1196/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0644 ; (Q Loss Avg: [Agent 0: 0.00065464, Agent 1: 0.00020633] ; E(V) Avg: [Agent 0: 0.16025197, Agent 1: 0.08195549]) [Noise stdev: 0.030857138795726834]
Episode 1197/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0644 ; (Q Loss Avg: [Agent 0: 0.00068344, Agent 1: 0.00020648] ; E(V) Avg: [Agent 0: 0.15712612, Agent 1: 0.07972866]) [Noise stdev: 0.03079542451813538]
Episode 1198/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0654 ; (Q Loss Avg: [Agent 0: 0.00064096, Agent 1: 0.00020482] ; E(V) Avg: [Agent 0: 0.15743702, Agent 1: 0.08005274]) [Noise stdev: 0.030733833669099108]
Episode 1199/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0664 ; (Q Loss Avg: [Agent 0: 0.00066398, Agent 1: 0.00020460] ; E(V) Avg: [Agent 0: 0.15936465, Agent 1: 0.07894816]) [Noise stdev: 0.03067236600176091]
Episode 1200/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0664 ; (Q Loss Avg: [Agent 0: 0.00075204, Agent 1: 0.00021398] ; E(V) Avg: [Agent 0: 0.16393037, Agent 1: 0.07932417]) [Noise stdev: 0.03061102126975739]
Episode 1201/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0664 ; (Q Loss Avg: [Agent 0: 0.00068976, Agent 1: 0.00019042] ; E(V) Avg: [Agent 0: 0.15733845, Agent 1: 0.08056892]) [Noise stdev: 0.030549799227217875]
Episode 1202/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0665 ; (Q Loss Avg: [Agent 0: 0.00072169, Agent 1: 0.00018980] ; E(V) Avg: [Agent 0: 0.15762971, Agent 1: 0.07894751]) [Noise stdev: 0.030488699628763438]
Episode 1203/5000 (33 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0655 ; (Q Loss Avg: [Agent 0: 0.00073861, Agent 1: 0.00019119] ; E(V) Avg: [Agent 0: 0.15791395, Agent 1: 0.08159941]) [Noise stdev: 0.03042772222950591]
Episode 1204/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0655 ; (Q Loss Avg: [Agent 0: 0.00076661, Agent 1: 0.00019210] ; E(V) Avg: [Agent 0: 0.15666296, Agent 1: 0.08068904]) [Noise stdev: 0.0303668667850469]
Episode 1205/5000 (29 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0664 ; (Q Loss Avg: [Agent 0: 0.00075387, Agent 1: 0.00019047] ; E(V) Avg: [Agent 0: 0.16169345, Agent 1: 0.08202919]) [Noise stdev: 0.030306133051476804]
Episode 1206/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0664 ; (Q Loss Avg: [Agent 0: 0.00062287, Agent 1: 0.00019761] ; E(V) Avg: [Agent 0: 0.16071606, Agent 1: 0.08051299]) [Noise stdev: 0.03024552078537385]
Episode 1207/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0664 ; (Q Loss Avg: [Agent 0: 0.00065958, Agent 1: 0.00020124] ; E(V) Avg: [Agent 0: 0.16112750, Agent 1: 0.08024149]) [Noise stdev: 0.030185029743803105]
Episode 1208/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0664 ; (Q Loss Avg: [Agent 0: 0.00077711, Agent 1: 0.00022771] ; E(V) Avg: [Agent 0: 0.16058567, Agent 1: 0.07960626]) [Noise stdev: 0.030124659684315498]
Episode 1209/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0664 ; (Q Loss Avg: [Agent 0: 0.00075627, Agent 1: 0.00024594] ; E(V) Avg: [Agent 0: 0.16077549, Agent 1: 0.08079455]) [Noise stdev: 0.030064410364946868]
Episode 1210/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0654 ; (Q Loss Avg: [Agent 0: 0.00070084, Agent 1: 0.00019750] ; E(V) Avg: [Agent 0: 0.16009949, Agent 1: 0.08108848]) [Noise stdev: 0.030004281544216974]
Episode 1211/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0644 ; (Q Loss Avg: [Agent 0: 0.00064632, Agent 1: 0.00019681] ; E(V) Avg: [Agent 0: 0.16517351, Agent 1: 0.07953932]) [Noise stdev: 0.02994427298112854]
Episode 1212/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0644 ; (Q Loss Avg: [Agent 0: 0.00074351, Agent 1: 0.00019476] ; E(V) Avg: [Agent 0: 0.16221666, Agent 1: 0.07986292]) [Noise stdev: 0.029884384435166285]
Episode 1213/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0634 ; (Q Loss Avg: [Agent 0: 0.00069112, Agent 1: 0.00017856] ; E(V) Avg: [Agent 0: 0.15580371, Agent 1: 0.07994638]) [Noise stdev: 0.02982461566629595]
Episode 1214/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0643 ; (Q Loss Avg: [Agent 0: 0.00064293, Agent 1: 0.00019093] ; E(V) Avg: [Agent 0: 0.15929209, Agent 1: 0.07973053]) [Noise stdev: 0.02976496643496336]
Episode 1215/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0653 ; (Q Loss Avg: [Agent 0: 0.00065269, Agent 1: 0.00018609] ; E(V) Avg: [Agent 0: 0.16223125, Agent 1: 0.08064680]) [Noise stdev: 0.029705436502093435]
Episode 1216/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0653 ; (Q Loss Avg: [Agent 0: 0.00055555, Agent 1: 0.00018247] ; E(V) Avg: [Agent 0: 0.16181812, Agent 1: 0.07999785]) [Noise stdev: 0.02964602562908925]
Episode 1217/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0643 ; (Q Loss Avg: [Agent 0: 0.00061662, Agent 1: 0.00020779] ; E(V) Avg: [Agent 0: 0.16345004, Agent 1: 0.08089897]) [Noise stdev: 0.02958673357783107]
Episode 1218/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0653 ; (Q Loss Avg: [Agent 0: 0.00060222, Agent 1: 0.00017598] ; E(V) Avg: [Agent 0: 0.15897374, Agent 1: 0.08108277]) [Noise stdev: 0.02952756011067541]
Episode 1219/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0653 ; (Q Loss Avg: [Agent 0: 0.00058771, Agent 1: 0.00018472] ; E(V) Avg: [Agent 0: 0.16029270, Agent 1: 0.07861663]) [Noise stdev: 0.02946850499045406]
Episode 1220/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0654 ; (Q Loss Avg: [Agent 0: 0.00063834, Agent 1: 0.00019512] ; E(V) Avg: [Agent 0: 0.16404058, Agent 1: 0.08169259]) [Noise stdev: 0.02940956798047315]
Episode 1221/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0654 ; (Q Loss Avg: [Agent 0: 0.00070790, Agent 1: 0.00020938] ; E(V) Avg: [Agent 0: 0.16242388, Agent 1: 0.07833931]) [Noise stdev: 0.029350748844512206]
Episode 1222/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0644 ; (Q Loss Avg: [Agent 0: 0.00058937, Agent 1: 0.00015922] ; E(V) Avg: [Agent 0: 0.16180117, Agent 1: 0.07979910]) [Noise stdev: 0.029292047346823183]
Episode 1223/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0634 ; (Q Loss Avg: [Agent 0: 0.00061369, Agent 1: 0.00019075] ; E(V) Avg: [Agent 0: 0.15978051, Agent 1: 0.07915873]) [Noise stdev: 0.029233463252129537]
Episode 1224/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0634 ; (Q Loss Avg: [Agent 0: 0.00064449, Agent 1: 0.00017880] ; E(V) Avg: [Agent 0: 0.16112142, Agent 1: 0.07948711]) [Noise stdev: 0.02917499632562528]
Episode 1225/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0634 ; (Q Loss Avg: [Agent 0: 0.00060072, Agent 1: 0.00018643] ; E(V) Avg: [Agent 0: 0.16414044, Agent 1: 0.08036663]) [Noise stdev: 0.029116646332974026]
Episode 1226/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0654 ; (Q Loss Avg: [Agent 0: 0.00072004, Agent 1: 0.00020919] ; E(V) Avg: [Agent 0: 0.16493019, Agent 1: 0.07839392]) [Noise stdev: 0.029058413040308078]
Episode 1227/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0655 ; (Q Loss Avg: [Agent 0: 0.00060964, Agent 1: 0.00020247] ; E(V) Avg: [Agent 0: 0.16168260, Agent 1: 0.07917138]) [Noise stdev: 0.02900029621422746]
Episode 1228/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0635 ; (Q Loss Avg: [Agent 0: 0.00085751, Agent 1: 0.00019967] ; E(V) Avg: [Agent 0: 0.16130836, Agent 1: 0.07751501]) [Noise stdev: 0.028942295621799004]
Episode 1229/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0635 ; (Q Loss Avg: [Agent 0: 0.00075621, Agent 1: 0.00022456] ; E(V) Avg: [Agent 0: 0.16585141, Agent 1: 0.08104875]) [Noise stdev: 0.028884411030555408]
Episode 1230/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0635 ; (Q Loss Avg: [Agent 0: 0.00077042, Agent 1: 0.00020295] ; E(V) Avg: [Agent 0: 0.16540196, Agent 1: 0.08023443]) [Noise stdev: 0.028826642208494298]
Episode 1231/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0625 ; (Q Loss Avg: [Agent 0: 0.00078677, Agent 1: 0.00018962] ; E(V) Avg: [Agent 0: 0.16102853, Agent 1: 0.07852552]) [Noise stdev: 0.02876898892407731]
Episode 1232/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0615 ; (Q Loss Avg: [Agent 0: 0.00083797, Agent 1: 0.00019211] ; E(V) Avg: [Agent 0: 0.16546728, Agent 1: 0.07888312]) [Noise stdev: 0.028711450946229157]
Episode 1233/5000 (57 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0615 ; (Q Loss Avg: [Agent 0: 0.00073741, Agent 1: 0.00018525] ; E(V) Avg: [Agent 0: 0.16241696, Agent 1: 0.08029669]) [Noise stdev: 0.0286540280443367]
Episode 1234/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0615 ; (Q Loss Avg: [Agent 0: 0.00067512, Agent 1: 0.00020000] ; E(V) Avg: [Agent 0: 0.15748105, Agent 1: 0.07849676]) [Noise stdev: 0.028596719988248025]
Episode 1235/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0615 ; (Q Loss Avg: [Agent 0: 0.00070021, Agent 1: 0.00021091] ; E(V) Avg: [Agent 0: 0.16372408, Agent 1: 0.07923794]) [Noise stdev: 0.028539526548271528]
Episode 1236/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0605 ; (Q Loss Avg: [Agent 0: 0.00056097, Agent 1: 0.00017308] ; E(V) Avg: [Agent 0: 0.15592209, Agent 1: 0.08079866]) [Noise stdev: 0.028482447495174985]
Episode 1237/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0605 ; (Q Loss Avg: [Agent 0: 0.00068469, Agent 1: 0.00017764] ; E(V) Avg: [Agent 0: 0.16341977, Agent 1: 0.07895571]) [Noise stdev: 0.028425482600184637]
Episode 1238/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0605 ; (Q Loss Avg: [Agent 0: 0.00071428, Agent 1: 0.00022582] ; E(V) Avg: [Agent 0: 0.16571624, Agent 1: 0.07828209]) [Noise stdev: 0.02836863163498427]
Episode 1239/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0605 ; (Q Loss Avg: [Agent 0: 0.00069398, Agent 1: 0.00019262] ; E(V) Avg: [Agent 0: 0.16487005, Agent 1: 0.07872898]) [Noise stdev: 0.0283118943717143]
Episode 1240/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0615 ; (Q Loss Avg: [Agent 0: 0.00067458, Agent 1: 0.00017574] ; E(V) Avg: [Agent 0: 0.16303870, Agent 1: 0.07830800]) [Noise stdev: 0.02825527058297087]
Episode 1241/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0605 ; (Q Loss Avg: [Agent 0: 0.00086839, Agent 1: 0.00018152] ; E(V) Avg: [Agent 0: 0.16667654, Agent 1: 0.07929533]) [Noise stdev: 0.028198760041804927]
Episode 1242/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0595 ; (Q Loss Avg: [Agent 0: 0.00084541, Agent 1: 0.00019391] ; E(V) Avg: [Agent 0: 0.17250491, Agent 1: 0.07615821]) [Noise stdev: 0.028142362521721316]
Episode 1243/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0595 ; (Q Loss Avg: [Agent 0: 0.00069445, Agent 1: 0.00017112] ; E(V) Avg: [Agent 0: 0.16446172, Agent 1: 0.07826512]) [Noise stdev: 0.028086077796677874]
Episode 1244/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0605 ; (Q Loss Avg: [Agent 0: 0.00078027, Agent 1: 0.00018442] ; E(V) Avg: [Agent 0: 0.16371875, Agent 1: 0.07974673]) [Noise stdev: 0.02802990564108452]
Episode 1245/5000 (29 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0614 ; (Q Loss Avg: [Agent 0: 0.00074110, Agent 1: 0.00017507] ; E(V) Avg: [Agent 0: 0.16596522, Agent 1: 0.07784389]) [Noise stdev: 0.02797384582980235]
Episode 1246/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0623 ; (Q Loss Avg: [Agent 0: 0.00078957, Agent 1: 0.00019341] ; E(V) Avg: [Agent 0: 0.16639719, Agent 1: 0.07885781]) [Noise stdev: 0.027917898138142744]
Episode 1247/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0623 ; (Q Loss Avg: [Agent 0: 0.00059751, Agent 1: 0.00024193] ; E(V) Avg: [Agent 0: 0.16125562, Agent 1: 0.07909187]) [Noise stdev: 0.02786206234186646]
Episode 1248/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0623 ; (Q Loss Avg: [Agent 0: 0.00063878, Agent 1: 0.00022392] ; E(V) Avg: [Agent 0: 0.16532678, Agent 1: 0.07809921]) [Noise stdev: 0.027806338217182727]
Episode 1249/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0633 ; (Q Loss Avg: [Agent 0: 0.00067336, Agent 1: 0.00022035] ; E(V) Avg: [Agent 0: 0.16200268, Agent 1: 0.07946966]) [Noise stdev: 0.02775072554074836]
Episode 1250/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0633 ; (Q Loss Avg: [Agent 0: 0.00070135, Agent 1: 0.00020135] ; E(V) Avg: [Agent 0: 0.16182912, Agent 1: 0.07727091]) [Noise stdev: 0.02769522408966686]
Episode 1251/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0633 ; (Q Loss Avg: [Agent 0: 0.00071050, Agent 1: 0.00019795] ; E(V) Avg: [Agent 0: 0.16292879, Agent 1: 0.07882559]) [Noise stdev: 0.02763983364148753]
Episode 1252/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0633 ; (Q Loss Avg: [Agent 0: 0.00072455, Agent 1: 0.00018734] ; E(V) Avg: [Agent 0: 0.16504417, Agent 1: 0.07862064]) [Noise stdev: 0.027584553974204555]
Episode 1253/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0642 ; (Q Loss Avg: [Agent 0: 0.00076750, Agent 1: 0.00020863] ; E(V) Avg: [Agent 0: 0.16410641, Agent 1: 0.07859695]) [Noise stdev: 0.027529384866256147]
Episode 1254/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0643 ; (Q Loss Avg: [Agent 0: 0.00071855, Agent 1: 0.00022128] ; E(V) Avg: [Agent 0: 0.16338354, Agent 1: 0.07853146]) [Noise stdev: 0.027474326096523633]
Episode 1255/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0652 ; (Q Loss Avg: [Agent 0: 0.00080671, Agent 1: 0.00021872] ; E(V) Avg: [Agent 0: 0.16320223, Agent 1: 0.07819271]) [Noise stdev: 0.027419377444330584]
Episode 1256/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0651 ; (Q Loss Avg: [Agent 0: 0.00075962, Agent 1: 0.00023270] ; E(V) Avg: [Agent 0: 0.16200916, Agent 1: 0.07773475]) [Noise stdev: 0.027364538689441924]
Episode 1257/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00064219, Agent 1: 0.00023336] ; E(V) Avg: [Agent 0: 0.16502593, Agent 1: 0.07924716]) [Noise stdev: 0.02730980961206304]
Episode 1258/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00069388, Agent 1: 0.00024119] ; E(V) Avg: [Agent 0: 0.16610002, Agent 1: 0.07842619]) [Noise stdev: 0.027255189992838914]
Episode 1259/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00064343, Agent 1: 0.00023303] ; E(V) Avg: [Agent 0: 0.16465174, Agent 1: 0.07719342]) [Noise stdev: 0.027200679612853235]
Episode 1260/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00060577, Agent 1: 0.00018108] ; E(V) Avg: [Agent 0: 0.16515372, Agent 1: 0.07827415]) [Noise stdev: 0.027146278253627528]
Episode 1261/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00073617, Agent 1: 0.00021122] ; E(V) Avg: [Agent 0: 0.16509331, Agent 1: 0.07830101]) [Noise stdev: 0.027091985697120274]
Episode 1262/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0661 ; (Q Loss Avg: [Agent 0: 0.00062582, Agent 1: 0.00021164] ; E(V) Avg: [Agent 0: 0.16657007, Agent 1: 0.07944781]) [Noise stdev: 0.02703780172572603]
Episode 1263/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0670 ; (Q Loss Avg: [Agent 0: 0.00062074, Agent 1: 0.00023279] ; E(V) Avg: [Agent 0: 0.16453181, Agent 1: 0.07745014]) [Noise stdev: 0.02698372612227458]
Episode 1264/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0679 ; (Q Loss Avg: [Agent 0: 0.00063177, Agent 1: 0.00022509] ; E(V) Avg: [Agent 0: 0.16678629, Agent 1: 0.07861274]) [Noise stdev: 0.02692975867003003]
Episode 1265/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0678 ; (Q Loss Avg: [Agent 0: 0.00071265, Agent 1: 0.00018807] ; E(V) Avg: [Agent 0: 0.16861271, Agent 1: 0.07541861]) [Noise stdev: 0.02687589915268997]
Episode 1266/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0667 ; (Q Loss Avg: [Agent 0: 0.00064146, Agent 1: 0.00022314] ; E(V) Avg: [Agent 0: 0.16317994, Agent 1: 0.07638492]) [Noise stdev: 0.02682214735438459]
Episode 1267/5000 (57 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0667 ; (Q Loss Avg: [Agent 0: 0.00063463, Agent 1: 0.00020005] ; E(V) Avg: [Agent 0: 0.16778348, Agent 1: 0.07683313]) [Noise stdev: 0.02676850305967582]
Episode 1268/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0677 ; (Q Loss Avg: [Agent 0: 0.00064411, Agent 1: 0.00020669] ; E(V) Avg: [Agent 0: 0.16816595, Agent 1: 0.07788476]) [Noise stdev: 0.026714966053556468]
Episode 1269/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0667 ; (Q Loss Avg: [Agent 0: 0.00064755, Agent 1: 0.00023653] ; E(V) Avg: [Agent 0: 0.17197573, Agent 1: 0.07815558]) [Noise stdev: 0.026661536121449354]
Episode 1270/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0667 ; (Q Loss Avg: [Agent 0: 0.00067196, Agent 1: 0.00018554] ; E(V) Avg: [Agent 0: 0.16779347, Agent 1: 0.07939187]) [Noise stdev: 0.026608213049206453]
Episode 1271/5000 (25 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0677 ; (Q Loss Avg: [Agent 0: 0.00075788, Agent 1: 0.00019543] ; E(V) Avg: [Agent 0: 0.16739305, Agent 1: 0.07695658]) [Noise stdev: 0.02655499662310804]
Episode 1272/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0677 ; (Q Loss Avg: [Agent 0: 0.00067180, Agent 1: 0.00021486] ; E(V) Avg: [Agent 0: 0.16857471, Agent 1: 0.07782853]) [Noise stdev: 0.026501886629861825]
Episode 1273/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0678 ; (Q Loss Avg: [Agent 0: 0.00067719, Agent 1: 0.00021757] ; E(V) Avg: [Agent 0: 0.16815632, Agent 1: 0.07684054]) [Noise stdev: 0.0264488828566021]
Episode 1274/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0678 ; (Q Loss Avg: [Agent 0: 0.00067944, Agent 1: 0.00019738] ; E(V) Avg: [Agent 0: 0.16955366, Agent 1: 0.07836889]) [Noise stdev: 0.026395985090888897]
Episode 1275/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0688 ; (Q Loss Avg: [Agent 0: 0.00069464, Agent 1: 0.00020254] ; E(V) Avg: [Agent 0: 0.16772363, Agent 1: 0.07705397]) [Noise stdev: 0.02634319312070712]
Episode 1276/5000 (65 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0698 ; (Q Loss Avg: [Agent 0: 0.00072071, Agent 1: 0.00019422] ; E(V) Avg: [Agent 0: 0.17231117, Agent 1: 0.07832238]) [Noise stdev: 0.026290506734465705]
Episode 1277/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0698 ; (Q Loss Avg: [Agent 0: 0.00074217, Agent 1: 0.00017902] ; E(V) Avg: [Agent 0: 0.16916283, Agent 1: 0.07648647]) [Noise stdev: 0.026237925720996775]
Episode 1278/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0708 ; (Q Loss Avg: [Agent 0: 0.00079354, Agent 1: 0.00019049] ; E(V) Avg: [Agent 0: 0.16617700, Agent 1: 0.07694400]) [Noise stdev: 0.026185449869554782]
Episode 1279/5000 (114 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.0728 ; (Q Loss Avg: [Agent 0: 0.00075738, Agent 1: 0.00019944] ; E(V) Avg: [Agent 0: 0.16962683, Agent 1: 0.07766864]) [Noise stdev: 0.026133078969815673]
Episode 1280/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0738 ; (Q Loss Avg: [Agent 0: 0.00072665, Agent 1: 0.00023182] ; E(V) Avg: [Agent 0: 0.16756896, Agent 1: 0.07656293]) [Noise stdev: 0.02608081281187604]
Episode 1281/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0748 ; (Q Loss Avg: [Agent 0: 0.00061123, Agent 1: 0.00021728] ; E(V) Avg: [Agent 0: 0.17245949, Agent 1: 0.07644416]) [Noise stdev: 0.026028651186252288]
Episode 1282/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0748 ; (Q Loss Avg: [Agent 0: 0.00068647, Agent 1: 0.00019406] ; E(V) Avg: [Agent 0: 0.17147131, Agent 1: 0.07715682]) [Noise stdev: 0.025976593883879785]
Episode 1283/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0747 ; (Q Loss Avg: [Agent 0: 0.00066703, Agent 1: 0.00020304] ; E(V) Avg: [Agent 0: 0.16961152, Agent 1: 0.07696619]) [Noise stdev: 0.025924640696112024]
Episode 1284/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0757 ; (Q Loss Avg: [Agent 0: 0.00070439, Agent 1: 0.00019382] ; E(V) Avg: [Agent 0: 0.17516859, Agent 1: 0.07814597]) [Noise stdev: 0.0258727914147198]
Episode 1285/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0757 ; (Q Loss Avg: [Agent 0: 0.00087924, Agent 1: 0.00017506] ; E(V) Avg: [Agent 0: 0.17380279, Agent 1: 0.07634416]) [Noise stdev: 0.02582104583189036]
Episode 1286/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0767 ; (Q Loss Avg: [Agent 0: 0.00079443, Agent 1: 0.00020189] ; E(V) Avg: [Agent 0: 0.17343678, Agent 1: 0.07722251]) [Noise stdev: 0.025769403740226578]
Episode 1287/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0776 ; (Q Loss Avg: [Agent 0: 0.00076251, Agent 1: 0.00018987] ; E(V) Avg: [Agent 0: 0.17179748, Agent 1: 0.07649471]) [Noise stdev: 0.025717864932746123]
Episode 1288/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0786 ; (Q Loss Avg: [Agent 0: 0.00081268, Agent 1: 0.00021400] ; E(V) Avg: [Agent 0: 0.17244289, Agent 1: 0.07926674]) [Noise stdev: 0.02566642920288063]
Episode 1289/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0776 ; (Q Loss Avg: [Agent 0: 0.00073421, Agent 1: 0.00018694] ; E(V) Avg: [Agent 0: 0.17172164, Agent 1: 0.07695316]) [Noise stdev: 0.02561509634447487]
Episode 1290/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0776 ; (Q Loss Avg: [Agent 0: 0.00079929, Agent 1: 0.00019486] ; E(V) Avg: [Agent 0: 0.17452526, Agent 1: 0.07689382]) [Noise stdev: 0.025563866151785918]
Episode 1291/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0776 ; (Q Loss Avg: [Agent 0: 0.00078830, Agent 1: 0.00018758] ; E(V) Avg: [Agent 0: 0.17419047, Agent 1: 0.07799061]) [Noise stdev: 0.025512738419482345]
Episode 1292/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0785 ; (Q Loss Avg: [Agent 0: 0.00077949, Agent 1: 0.00021105] ; E(V) Avg: [Agent 0: 0.17159603, Agent 1: 0.07832596]) [Noise stdev: 0.02546171294264338]
Episode 1293/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0795 ; (Q Loss Avg: [Agent 0: 0.00070180, Agent 1: 0.00018908] ; E(V) Avg: [Agent 0: 0.17355019, Agent 1: 0.07671104]) [Noise stdev: 0.025410789516758094]
Episode 1294/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0794 ; (Q Loss Avg: [Agent 0: 0.00062865, Agent 1: 0.00020256] ; E(V) Avg: [Agent 0: 0.17143125, Agent 1: 0.07744894]) [Noise stdev: 0.02535996793772458]
Episode 1295/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0794 ; (Q Loss Avg: [Agent 0: 0.00072825, Agent 1: 0.00021253] ; E(V) Avg: [Agent 0: 0.17378391, Agent 1: 0.07709795]) [Noise stdev: 0.02530924800184913]
Episode 1296/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0794 ; (Q Loss Avg: [Agent 0: 0.00069507, Agent 1: 0.00021580] ; E(V) Avg: [Agent 0: 0.17037048, Agent 1: 0.07687957]) [Noise stdev: 0.025258629505845433]
Episode 1297/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0804 ; (Q Loss Avg: [Agent 0: 0.00067561, Agent 1: 0.00022831] ; E(V) Avg: [Agent 0: 0.17517327, Agent 1: 0.07959882]) [Noise stdev: 0.025208112246833742]
Episode 1298/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0804 ; (Q Loss Avg: [Agent 0: 0.00066203, Agent 1: 0.00021919] ; E(V) Avg: [Agent 0: 0.17228803, Agent 1: 0.07704533]) [Noise stdev: 0.025157696022340074]
Episode 1299/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0804 ; (Q Loss Avg: [Agent 0: 0.00064769, Agent 1: 0.00021536] ; E(V) Avg: [Agent 0: 0.17368233, Agent 1: 0.07604253]) [Noise stdev: 0.025107380630295394]
Episode 1300/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0814 ; (Q Loss Avg: [Agent 0: 0.00066311, Agent 1: 0.00018181] ; E(V) Avg: [Agent 0: 0.17573951, Agent 1: 0.07577049]) [Noise stdev: 0.025057165869034805]
Episode 1301/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0814 ; (Q Loss Avg: [Agent 0: 0.00070296, Agent 1: 0.00018941] ; E(V) Avg: [Agent 0: 0.17635119, Agent 1: 0.07630894]) [Noise stdev: 0.025007051537296737]
Episode 1302/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0814 ; (Q Loss Avg: [Agent 0: 0.00068646, Agent 1: 0.00021514] ; E(V) Avg: [Agent 0: 0.17345430, Agent 1: 0.07743406]) [Noise stdev: 0.024957037434222142]
Episode 1303/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0824 ; (Q Loss Avg: [Agent 0: 0.00071460, Agent 1: 0.00018932] ; E(V) Avg: [Agent 0: 0.17322166, Agent 1: 0.07697366]) [Noise stdev: 0.0249071233593537]
Episode 1304/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0824 ; (Q Loss Avg: [Agent 0: 0.00063590, Agent 1: 0.00020249] ; E(V) Avg: [Agent 0: 0.17725722, Agent 1: 0.07583335]) [Noise stdev: 0.024857309112634993]
Episode 1305/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0825 ; (Q Loss Avg: [Agent 0: 0.00077295, Agent 1: 0.00020596] ; E(V) Avg: [Agent 0: 0.17779156, Agent 1: 0.07626388]) [Noise stdev: 0.024807594494409724]
Episode 1306/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0825 ; (Q Loss Avg: [Agent 0: 0.00077155, Agent 1: 0.00018374] ; E(V) Avg: [Agent 0: 0.17339639, Agent 1: 0.07487759]) [Noise stdev: 0.024757979305420903]
Episode 1307/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0825 ; (Q Loss Avg: [Agent 0: 0.00070634, Agent 1: 0.00019788] ; E(V) Avg: [Agent 0: 0.17598393, Agent 1: 0.07471673]) [Noise stdev: 0.02470846334681006]
Episode 1308/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0815 ; (Q Loss Avg: [Agent 0: 0.00074956, Agent 1: 0.00020846] ; E(V) Avg: [Agent 0: 0.17610954, Agent 1: 0.07829676]) [Noise stdev: 0.02465904642011644]
Episode 1309/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0805 ; (Q Loss Avg: [Agent 0: 0.00098472, Agent 1: 0.00023746] ; E(V) Avg: [Agent 0: 0.17244712, Agent 1: 0.07914412]) [Noise stdev: 0.024609728327276207]
Episode 1310/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0805 ; (Q Loss Avg: [Agent 0: 0.00066918, Agent 1: 0.00022906] ; E(V) Avg: [Agent 0: 0.17543763, Agent 1: 0.07634085]) [Noise stdev: 0.024560508870621653]
Episode 1311/5000 (29 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0814 ; (Q Loss Avg: [Agent 0: 0.00070386, Agent 1: 0.00019497] ; E(V) Avg: [Agent 0: 0.17326821, Agent 1: 0.07518682]) [Noise stdev: 0.02451138785288041]
Episode 1312/5000 (58 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0814 ; (Q Loss Avg: [Agent 0: 0.00073058, Agent 1: 0.00020264] ; E(V) Avg: [Agent 0: 0.17416510, Agent 1: 0.07683649]) [Noise stdev: 0.024462365077174652]
Episode 1313/5000 (165 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.0854 ; (Q Loss Avg: [Agent 0: 0.00074062, Agent 1: 0.00020776] ; E(V) Avg: [Agent 0: 0.17689360, Agent 1: 0.07565955]) [Noise stdev: 0.024413440347020303]
Episode 1314/5000 (96 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0865 ; (Q Loss Avg: [Agent 0: 0.00079290, Agent 1: 0.00022956] ; E(V) Avg: [Agent 0: 0.17896417, Agent 1: 0.07460422]) [Noise stdev: 0.024364613466326263]
Episode 1315/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0865 ; (Q Loss Avg: [Agent 0: 0.00077635, Agent 1: 0.00022525] ; E(V) Avg: [Agent 0: 0.17759608, Agent 1: 0.07642238]) [Noise stdev: 0.02431588423939361]
Episode 1316/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0875 ; (Q Loss Avg: [Agent 0: 0.00084847, Agent 1: 0.00023419] ; E(V) Avg: [Agent 0: 0.17875748, Agent 1: 0.07541656]) [Noise stdev: 0.02426725247091482]
Episode 1317/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0885 ; (Q Loss Avg: [Agent 0: 0.00074573, Agent 1: 0.00019882] ; E(V) Avg: [Agent 0: 0.17642772, Agent 1: 0.07437758]) [Noise stdev: 0.02421871796597299]
Episode 1318/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0885 ; (Q Loss Avg: [Agent 0: 0.00070506, Agent 1: 0.00019951] ; E(V) Avg: [Agent 0: 0.17906144, Agent 1: 0.07426927]) [Noise stdev: 0.024170280530041045]
Episode 1319/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0885 ; (Q Loss Avg: [Agent 0: 0.00082569, Agent 1: 0.00023781] ; E(V) Avg: [Agent 0: 0.17737411, Agent 1: 0.07452407]) [Noise stdev: 0.024121939968980963]
Episode 1320/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0885 ; (Q Loss Avg: [Agent 0: 0.00074023, Agent 1: 0.00019092] ; E(V) Avg: [Agent 0: 0.17742185, Agent 1: 0.07529451]) [Noise stdev: 0.024073696089043]
Episode 1321/5000 (29 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0884 ; (Q Loss Avg: [Agent 0: 0.00079871, Agent 1: 0.00019823] ; E(V) Avg: [Agent 0: 0.18222373, Agent 1: 0.07784689]) [Noise stdev: 0.024025548696864914]
Episode 1322/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0893 ; (Q Loss Avg: [Agent 0: 0.00072465, Agent 1: 0.00022808] ; E(V) Avg: [Agent 0: 0.17640227, Agent 1: 0.07595918]) [Noise stdev: 0.023977497599471185]
Episode 1323/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0902 ; (Q Loss Avg: [Agent 0: 0.00069145, Agent 1: 0.00022510] ; E(V) Avg: [Agent 0: 0.17996670, Agent 1: 0.07467819]) [Noise stdev: 0.02392954260427224]
Episode 1324/5000 (29 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0901 ; (Q Loss Avg: [Agent 0: 0.00076165, Agent 1: 0.00021436] ; E(V) Avg: [Agent 0: 0.18039191, Agent 1: 0.07574421]) [Noise stdev: 0.023881683519063696]
Episode 1325/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0901 ; (Q Loss Avg: [Agent 0: 0.00076585, Agent 1: 0.00023924] ; E(V) Avg: [Agent 0: 0.17812041, Agent 1: 0.07443446]) [Noise stdev: 0.023833920152025567]
Episode 1326/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0891 ; (Q Loss Avg: [Agent 0: 0.00077765, Agent 1: 0.00021332] ; E(V) Avg: [Agent 0: 0.17696619, Agent 1: 0.07344533]) [Noise stdev: 0.023786252311721517]
Episode 1327/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0891 ; (Q Loss Avg: [Agent 0: 0.00081135, Agent 1: 0.00023107] ; E(V) Avg: [Agent 0: 0.18130846, Agent 1: 0.07348008]) [Noise stdev: 0.023738679807098075]
Episode 1328/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0901 ; (Q Loss Avg: [Agent 0: 0.00068458, Agent 1: 0.00024740] ; E(V) Avg: [Agent 0: 0.18099492, Agent 1: 0.07528160]) [Noise stdev: 0.023691202447483878]
Episode 1329/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0911 ; (Q Loss Avg: [Agent 0: 0.00062658, Agent 1: 0.00023247] ; E(V) Avg: [Agent 0: 0.17762334, Agent 1: 0.07412987]) [Noise stdev: 0.02364382004258891]
Episode 1330/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0911 ; (Q Loss Avg: [Agent 0: 0.00077445, Agent 1: 0.00019782] ; E(V) Avg: [Agent 0: 0.18262229, Agent 1: 0.07517175]) [Noise stdev: 0.02359653240250373]
Episode 1331/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0911 ; (Q Loss Avg: [Agent 0: 0.00070106, Agent 1: 0.00021750] ; E(V) Avg: [Agent 0: 0.17905691, Agent 1: 0.07275370]) [Noise stdev: 0.023549339337698726]
Episode 1332/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0911 ; (Q Loss Avg: [Agent 0: 0.00069736, Agent 1: 0.00020936] ; E(V) Avg: [Agent 0: 0.18793549, Agent 1: 0.07244552]) [Noise stdev: 0.02350224065902333]
Episode 1333/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0910 ; (Q Loss Avg: [Agent 0: 0.00066606, Agent 1: 0.00022273] ; E(V) Avg: [Agent 0: 0.17773304, Agent 1: 0.07423022]) [Noise stdev: 0.023455236177705282]
Episode 1334/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0920 ; (Q Loss Avg: [Agent 0: 0.00065723, Agent 1: 0.00027671] ; E(V) Avg: [Agent 0: 0.18242699, Agent 1: 0.07453919]) [Noise stdev: 0.023408325705349873]
Episode 1335/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0920 ; (Q Loss Avg: [Agent 0: 0.00064849, Agent 1: 0.00025111] ; E(V) Avg: [Agent 0: 0.17602441, Agent 1: 0.07548989]) [Noise stdev: 0.02336150905393917]
Episode 1336/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0920 ; (Q Loss Avg: [Agent 0: 0.00071785, Agent 1: 0.00023755] ; E(V) Avg: [Agent 0: 0.18121429, Agent 1: 0.07538338]) [Noise stdev: 0.023314786035831293]
Episode 1337/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0919 ; (Q Loss Avg: [Agent 0: 0.00069388, Agent 1: 0.00023455] ; E(V) Avg: [Agent 0: 0.18081076, Agent 1: 0.07418595]) [Noise stdev: 0.02326815646375963]
Episode 1338/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0929 ; (Q Loss Avg: [Agent 0: 0.00067590, Agent 1: 0.00021084] ; E(V) Avg: [Agent 0: 0.17605068, Agent 1: 0.07425064]) [Noise stdev: 0.02322162015083211]
Episode 1339/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0919 ; (Q Loss Avg: [Agent 0: 0.00061477, Agent 1: 0.00021200] ; E(V) Avg: [Agent 0: 0.17608870, Agent 1: 0.07523636]) [Noise stdev: 0.023175176910530445]
Episode 1340/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0919 ; (Q Loss Avg: [Agent 0: 0.00065495, Agent 1: 0.00021815] ; E(V) Avg: [Agent 0: 0.17848813, Agent 1: 0.07469297]) [Noise stdev: 0.023128826556709385]
Episode 1341/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0929 ; (Q Loss Avg: [Agent 0: 0.00066874, Agent 1: 0.00019823] ; E(V) Avg: [Agent 0: 0.18350652, Agent 1: 0.07425027]) [Noise stdev: 0.023082568903595967]
Episode 1342/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0939 ; (Q Loss Avg: [Agent 0: 0.00076680, Agent 1: 0.00022335] ; E(V) Avg: [Agent 0: 0.17896130, Agent 1: 0.07408067]) [Noise stdev: 0.023036403765788773]
Episode 1343/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0949 ; (Q Loss Avg: [Agent 0: 0.00079798, Agent 1: 0.00023706] ; E(V) Avg: [Agent 0: 0.17907650, Agent 1: 0.07456535]) [Noise stdev: 0.022990330958257196]
Episode 1344/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0949 ; (Q Loss Avg: [Agent 0: 0.00074417, Agent 1: 0.00022743] ; E(V) Avg: [Agent 0: 0.18201175, Agent 1: 0.07379191]) [Noise stdev: 0.02294435029634068]
Episode 1345/5000 (58 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0950 ; (Q Loss Avg: [Agent 0: 0.00072118, Agent 1: 0.00022754] ; E(V) Avg: [Agent 0: 0.17890125, Agent 1: 0.07534573]) [Noise stdev: 0.022898461595747998]
Episode 1346/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0941 ; (Q Loss Avg: [Agent 0: 0.00072134, Agent 1: 0.00021573] ; E(V) Avg: [Agent 0: 0.18213140, Agent 1: 0.07428633]) [Noise stdev: 0.022852664672556502]
Episode 1347/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0941 ; (Q Loss Avg: [Agent 0: 0.00072911, Agent 1: 0.00020557] ; E(V) Avg: [Agent 0: 0.18225823, Agent 1: 0.07410501]) [Noise stdev: 0.022806959343211388]
Episode 1348/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0941 ; (Q Loss Avg: [Agent 0: 0.00076653, Agent 1: 0.00018596] ; E(V) Avg: [Agent 0: 0.18492585, Agent 1: 0.07391105]) [Noise stdev: 0.022761345424524966]
Episode 1349/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0940 ; (Q Loss Avg: [Agent 0: 0.00072154, Agent 1: 0.00019912] ; E(V) Avg: [Agent 0: 0.17953893, Agent 1: 0.07519204]) [Noise stdev: 0.022715822733675915]
Episode 1350/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0940 ; (Q Loss Avg: [Agent 0: 0.00065437, Agent 1: 0.00020934] ; E(V) Avg: [Agent 0: 0.17921635, Agent 1: 0.07404365]) [Noise stdev: 0.022670391088208564]
Episode 1351/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0940 ; (Q Loss Avg: [Agent 0: 0.00067915, Agent 1: 0.00020987] ; E(V) Avg: [Agent 0: 0.18283349, Agent 1: 0.07491512]) [Noise stdev: 0.022625050306032145]
Episode 1352/5000 (106 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.0960 ; (Q Loss Avg: [Agent 0: 0.00066894, Agent 1: 0.00021961] ; E(V) Avg: [Agent 0: 0.18096924, Agent 1: 0.07447754]) [Noise stdev: 0.02257980020542008]
Episode 1353/5000 (91 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0971 ; (Q Loss Avg: [Agent 0: 0.00079874, Agent 1: 0.00020965] ; E(V) Avg: [Agent 0: 0.18157078, Agent 1: 0.07400560]) [Noise stdev: 0.022534640605009242]
Episode 1354/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0971 ; (Q Loss Avg: [Agent 0: 0.00073105, Agent 1: 0.00022167] ; E(V) Avg: [Agent 0: 0.18054286, Agent 1: 0.07450767]) [Noise stdev: 0.022489571323799223]
Episode 1355/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0972 ; (Q Loss Avg: [Agent 0: 0.00074494, Agent 1: 0.00020371] ; E(V) Avg: [Agent 0: 0.18142002, Agent 1: 0.07495647]) [Noise stdev: 0.022444592181151624]
Episode 1356/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0973 ; (Q Loss Avg: [Agent 0: 0.00076405, Agent 1: 0.00020313] ; E(V) Avg: [Agent 0: 0.17871219, Agent 1: 0.07603209]) [Noise stdev: 0.02239970299678932]
Episode 1357/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0973 ; (Q Loss Avg: [Agent 0: 0.00077943, Agent 1: 0.00019555] ; E(V) Avg: [Agent 0: 0.18069101, Agent 1: 0.07518607]) [Noise stdev: 0.022354903590795745]
Episode 1358/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0963 ; (Q Loss Avg: [Agent 0: 0.00074645, Agent 1: 0.00018951] ; E(V) Avg: [Agent 0: 0.18439494, Agent 1: 0.07419847]) [Noise stdev: 0.02231019378361415]
Episode 1359/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0963 ; (Q Loss Avg: [Agent 0: 0.00071332, Agent 1: 0.00017785] ; E(V) Avg: [Agent 0: 0.18064244, Agent 1: 0.07381360]) [Noise stdev: 0.022265573396046925]
Episode 1360/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0963 ; (Q Loss Avg: [Agent 0: 0.00071571, Agent 1: 0.00020609] ; E(V) Avg: [Agent 0: 0.18207865, Agent 1: 0.07475652]) [Noise stdev: 0.022221042249254832]
Episode 1361/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0963 ; (Q Loss Avg: [Agent 0: 0.00071562, Agent 1: 0.00024196] ; E(V) Avg: [Agent 0: 0.18424883, Agent 1: 0.07382196]) [Noise stdev: 0.02217660016475632]
Episode 1362/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0963 ; (Q Loss Avg: [Agent 0: 0.00075238, Agent 1: 0.00024013] ; E(V) Avg: [Agent 0: 0.18263336, Agent 1: 0.07568352]) [Noise stdev: 0.022132246964426807]
Episode 1363/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0954 ; (Q Loss Avg: [Agent 0: 0.00072021, Agent 1: 0.00026066] ; E(V) Avg: [Agent 0: 0.18481715, Agent 1: 0.07307630]) [Noise stdev: 0.022087982470497952]
Episode 1364/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0955 ; (Q Loss Avg: [Agent 0: 0.00069485, Agent 1: 0.00023749] ; E(V) Avg: [Agent 0: 0.18224898, Agent 1: 0.07444713]) [Noise stdev: 0.022043806505556957]
Episode 1365/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0946 ; (Q Loss Avg: [Agent 0: 0.00069903, Agent 1: 0.00026406] ; E(V) Avg: [Agent 0: 0.17999958, Agent 1: 0.07475727]) [Noise stdev: 0.021999718892545844]
Episode 1366/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0947 ; (Q Loss Avg: [Agent 0: 0.00070729, Agent 1: 0.00024211] ; E(V) Avg: [Agent 0: 0.17767591, Agent 1: 0.07420500]) [Noise stdev: 0.021955719454760752]
Episode 1367/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0937 ; (Q Loss Avg: [Agent 0: 0.00081393, Agent 1: 0.00021120] ; E(V) Avg: [Agent 0: 0.18698616, Agent 1: 0.07247501]) [Noise stdev: 0.02191180801585123]
Episode 1368/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0937 ; (Q Loss Avg: [Agent 0: 0.00077234, Agent 1: 0.00022083] ; E(V) Avg: [Agent 0: 0.18497065, Agent 1: 0.07369739]) [Noise stdev: 0.021867984399819528]
Episode 1369/5000 (91 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0957 ; (Q Loss Avg: [Agent 0: 0.00069360, Agent 1: 0.00021155] ; E(V) Avg: [Agent 0: 0.18233949, Agent 1: 0.07328255]) [Noise stdev: 0.021824248431019887]
Episode 1370/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0957 ; (Q Loss Avg: [Agent 0: 0.00061371, Agent 1: 0.00028436] ; E(V) Avg: [Agent 0: 0.18078696, Agent 1: 0.07405089]) [Noise stdev: 0.021780599934157846]
Episode 1371/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0957 ; (Q Loss Avg: [Agent 0: 0.00065588, Agent 1: 0.00028392] ; E(V) Avg: [Agent 0: 0.18409200, Agent 1: 0.07297722]) [Noise stdev: 0.02173703873428953]
Episode 1372/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0947 ; (Q Loss Avg: [Agent 0: 0.00072824, Agent 1: 0.00023133] ; E(V) Avg: [Agent 0: 0.18135139, Agent 1: 0.07303266]) [Noise stdev: 0.02169356465682095]
Episode 1373/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0947 ; (Q Loss Avg: [Agent 0: 0.00072671, Agent 1: 0.00021373] ; E(V) Avg: [Agent 0: 0.18198880, Agent 1: 0.07479131]) [Noise stdev: 0.02165017752750731]
Episode 1374/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0947 ; (Q Loss Avg: [Agent 0: 0.00072252, Agent 1: 0.00023247] ; E(V) Avg: [Agent 0: 0.18630936, Agent 1: 0.07339348]) [Noise stdev: 0.021606877172452292]
Episode 1375/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0947 ; (Q Loss Avg: [Agent 0: 0.00070680, Agent 1: 0.00021775] ; E(V) Avg: [Agent 0: 0.18630948, Agent 1: 0.07347347]) [Noise stdev: 0.021563663418107386]
Episode 1376/5000 (91 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0957 ; (Q Loss Avg: [Agent 0: 0.00064019, Agent 1: 0.00023600] ; E(V) Avg: [Agent 0: 0.18450023, Agent 1: 0.07290503]) [Noise stdev: 0.02152053609127117]
Episode 1377/5000 (87 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0967 ; (Q Loss Avg: [Agent 0: 0.00069974, Agent 1: 0.00023381] ; E(V) Avg: [Agent 0: 0.18357908, Agent 1: 0.07481288]) [Noise stdev: 0.02147749501908863]
Episode 1378/5000 (74 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0977 ; (Q Loss Avg: [Agent 0: 0.00070080, Agent 1: 0.00025872] ; E(V) Avg: [Agent 0: 0.18357072, Agent 1: 0.07331733]) [Noise stdev: 0.02143454002905045]
Episode 1379/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0957 ; (Q Loss Avg: [Agent 0: 0.00080056, Agent 1: 0.00020937] ; E(V) Avg: [Agent 0: 0.18197862, Agent 1: 0.07383646]) [Noise stdev: 0.02139167094899235]
Episode 1380/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0947 ; (Q Loss Avg: [Agent 0: 0.00069292, Agent 1: 0.00022668] ; E(V) Avg: [Agent 0: 0.18464320, Agent 1: 0.07396758]) [Noise stdev: 0.021348887607094368]
Episode 1381/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0947 ; (Q Loss Avg: [Agent 0: 0.00065402, Agent 1: 0.00025054] ; E(V) Avg: [Agent 0: 0.18220628, Agent 1: 0.07387842]) [Noise stdev: 0.02130618983188018]
Episode 1382/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0947 ; (Q Loss Avg: [Agent 0: 0.00068177, Agent 1: 0.00024074] ; E(V) Avg: [Agent 0: 0.18500557, Agent 1: 0.07544960]) [Noise stdev: 0.02126357745221642]
Episode 1383/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0948 ; (Q Loss Avg: [Agent 0: 0.00087917, Agent 1: 0.00023808] ; E(V) Avg: [Agent 0: 0.18373104, Agent 1: 0.07368443]) [Noise stdev: 0.021221050297311986]
Episode 1384/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0948 ; (Q Loss Avg: [Agent 0: 0.00069408, Agent 1: 0.00020846] ; E(V) Avg: [Agent 0: 0.18598227, Agent 1: 0.07396887]) [Noise stdev: 0.021178608196717363]
Episode 1385/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0948 ; (Q Loss Avg: [Agent 0: 0.00074290, Agent 1: 0.00021152] ; E(V) Avg: [Agent 0: 0.18283289, Agent 1: 0.07457014]) [Noise stdev: 0.02113625098032393]
Episode 1386/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0948 ; (Q Loss Avg: [Agent 0: 0.00070980, Agent 1: 0.00022469] ; E(V) Avg: [Agent 0: 0.18339696, Agent 1: 0.07297589]) [Noise stdev: 0.02109397847836328]
Episode 1387/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0949 ; (Q Loss Avg: [Agent 0: 0.00062511, Agent 1: 0.00025575] ; E(V) Avg: [Agent 0: 0.18530691, Agent 1: 0.07537854]) [Noise stdev: 0.02105179052140655]
Episode 1388/5000 (67 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.0958 ; (Q Loss Avg: [Agent 0: 0.00066296, Agent 1: 0.00023108] ; E(V) Avg: [Agent 0: 0.18611212, Agent 1: 0.07449009]) [Noise stdev: 0.021009686940363738]
Episode 1389/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0958 ; (Q Loss Avg: [Agent 0: 0.00071333, Agent 1: 0.00023883] ; E(V) Avg: [Agent 0: 0.18668948, Agent 1: 0.07472383]) [Noise stdev: 0.02096766756648301]
Episode 1390/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0958 ; (Q Loss Avg: [Agent 0: 0.00067228, Agent 1: 0.00022204] ; E(V) Avg: [Agent 0: 0.18575435, Agent 1: 0.07436239]) [Noise stdev: 0.020925732231350042]
Episode 1391/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0958 ; (Q Loss Avg: [Agent 0: 0.00068174, Agent 1: 0.00023524] ; E(V) Avg: [Agent 0: 0.18340789, Agent 1: 0.07369875]) [Noise stdev: 0.020883880766887342]
Episode 1392/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0959 ; (Q Loss Avg: [Agent 0: 0.00069520, Agent 1: 0.00020759] ; E(V) Avg: [Agent 0: 0.18675945, Agent 1: 0.07384754]) [Noise stdev: 0.020842113005353567]
Episode 1393/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0959 ; (Q Loss Avg: [Agent 0: 0.00069215, Agent 1: 0.00022181] ; E(V) Avg: [Agent 0: 0.18122151, Agent 1: 0.07383058]) [Noise stdev: 0.02080042877934286]
Episode 1394/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0960 ; (Q Loss Avg: [Agent 0: 0.00068706, Agent 1: 0.00023093] ; E(V) Avg: [Agent 0: 0.18684105, Agent 1: 0.07435386]) [Noise stdev: 0.020758827921784174]
Episode 1395/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0951 ; (Q Loss Avg: [Agent 0: 0.00073877, Agent 1: 0.00020620] ; E(V) Avg: [Agent 0: 0.18334688, Agent 1: 0.07707578]) [Noise stdev: 0.020717310265940606]
Episode 1396/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0951 ; (Q Loss Avg: [Agent 0: 0.00067324, Agent 1: 0.00023910] ; E(V) Avg: [Agent 0: 0.18435783, Agent 1: 0.07325679]) [Noise stdev: 0.020675875645408723]
Episode 1397/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0951 ; (Q Loss Avg: [Agent 0: 0.00075721, Agent 1: 0.00024317] ; E(V) Avg: [Agent 0: 0.18771794, Agent 1: 0.07532438]) [Noise stdev: 0.020634523894117907]
Episode 1398/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0951 ; (Q Loss Avg: [Agent 0: 0.00069166, Agent 1: 0.00022978] ; E(V) Avg: [Agent 0: 0.18635022, Agent 1: 0.07475822]) [Noise stdev: 0.02059325484632967]
Episode 1399/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0951 ; (Q Loss Avg: [Agent 0: 0.00066605, Agent 1: 0.00020898] ; E(V) Avg: [Agent 0: 0.18503547, Agent 1: 0.07313018]) [Noise stdev: 0.020552068336637013]
Episode 1400/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0951 ; (Q Loss Avg: [Agent 0: 0.00066707, Agent 1: 0.00020609] ; E(V) Avg: [Agent 0: 0.18644819, Agent 1: 0.07376139]) [Noise stdev: 0.02051096419996374]
Episode 1401/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0941 ; (Q Loss Avg: [Agent 0: 0.00058780, Agent 1: 0.00026678] ; E(V) Avg: [Agent 0: 0.18241707, Agent 1: 0.07680857]) [Noise stdev: 0.02046994227156381]
Episode 1402/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0941 ; (Q Loss Avg: [Agent 0: 0.00065717, Agent 1: 0.00021503] ; E(V) Avg: [Agent 0: 0.18405720, Agent 1: 0.07432275]) [Noise stdev: 0.020429002387020685]
Episode 1403/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0941 ; (Q Loss Avg: [Agent 0: 0.00072297, Agent 1: 0.00020852] ; E(V) Avg: [Agent 0: 0.18374966, Agent 1: 0.07534417]) [Noise stdev: 0.020388144382246644]
Episode 1404/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0941 ; (Q Loss Avg: [Agent 0: 0.00070718, Agent 1: 0.00022524] ; E(V) Avg: [Agent 0: 0.18598136, Agent 1: 0.07529845]) [Noise stdev: 0.02034736809348215]
Episode 1405/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.0940 ; (Q Loss Avg: [Agent 0: 0.00071447, Agent 1: 0.00022493] ; E(V) Avg: [Agent 0: 0.18634009, Agent 1: 0.07388111]) [Noise stdev: 0.020306673357295187]
Episode 1406/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0930 ; (Q Loss Avg: [Agent 0: 0.00063022, Agent 1: 0.00026195] ; E(V) Avg: [Agent 0: 0.18322230, Agent 1: 0.07461429]) [Noise stdev: 0.020266060010580598]
Episode 1407/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0920 ; (Q Loss Avg: [Agent 0: 0.00081461, Agent 1: 0.00020630] ; E(V) Avg: [Agent 0: 0.18448857, Agent 1: 0.07583187]) [Noise stdev: 0.020225527890559435]
Episode 1408/5000 (72 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0940 ; (Q Loss Avg: [Agent 0: 0.00067503, Agent 1: 0.00023738] ; E(V) Avg: [Agent 0: 0.18714608, Agent 1: 0.07407019]) [Noise stdev: 0.020185076834778316]
Episode 1409/5000 (67 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.0959 ; (Q Loss Avg: [Agent 0: 0.00064519, Agent 1: 0.00023239] ; E(V) Avg: [Agent 0: 0.18226206, Agent 1: 0.07498933]) [Noise stdev: 0.020144706681108758]
Episode 1410/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0959 ; (Q Loss Avg: [Agent 0: 0.00053534, Agent 1: 0.00025382] ; E(V) Avg: [Agent 0: 0.18496218, Agent 1: 0.07603158]) [Noise stdev: 0.02010441726774654]
Episode 1411/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0960 ; (Q Loss Avg: [Agent 0: 0.00060139, Agent 1: 0.00023020] ; E(V) Avg: [Agent 0: 0.18284906, Agent 1: 0.07455900]) [Noise stdev: 0.020064208433211047]
Episode 1412/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0960 ; (Q Loss Avg: [Agent 0: 0.00069988, Agent 1: 0.00026062] ; E(V) Avg: [Agent 0: 0.18531492, Agent 1: 0.07512485]) [Noise stdev: 0.020024080016344623]
Episode 1413/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0920 ; (Q Loss Avg: [Agent 0: 0.00067200, Agent 1: 0.00026516] ; E(V) Avg: [Agent 0: 0.18189195, Agent 1: 0.07579745]) [Noise stdev: 0.019984031856311934]
Episode 1414/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0900 ; (Q Loss Avg: [Agent 0: 0.00071778, Agent 1: 0.00020178] ; E(V) Avg: [Agent 0: 0.18466783, Agent 1: 0.07467787]) [Noise stdev: 0.01994406379259931]
Episode 1415/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0900 ; (Q Loss Avg: [Agent 0: 0.00070782, Agent 1: 0.00024144] ; E(V) Avg: [Agent 0: 0.18508028, Agent 1: 0.07562339]) [Noise stdev: 0.019904175665014113]
Episode 1416/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0900 ; (Q Loss Avg: [Agent 0: 0.00071594, Agent 1: 0.00027698] ; E(V) Avg: [Agent 0: 0.18353608, Agent 1: 0.07384665]) [Noise stdev: 0.019864367313684083]
Episode 1417/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0900 ; (Q Loss Avg: [Agent 0: 0.00070944, Agent 1: 0.00024851] ; E(V) Avg: [Agent 0: 0.18291777, Agent 1: 0.07449936]) [Noise stdev: 0.019824638579056714]
Episode 1418/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0900 ; (Q Loss Avg: [Agent 0: 0.00067673, Agent 1: 0.00022674] ; E(V) Avg: [Agent 0: 0.18608935, Agent 1: 0.07467489]) [Noise stdev: 0.0197849893018986]
Episode 1419/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0900 ; (Q Loss Avg: [Agent 0: 0.00068555, Agent 1: 0.00023452] ; E(V) Avg: [Agent 0: 0.18705881, Agent 1: 0.07561909]) [Noise stdev: 0.0197454193232948]
Episode 1420/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0900 ; (Q Loss Avg: [Agent 0: 0.00067299, Agent 1: 0.00021963] ; E(V) Avg: [Agent 0: 0.18501371, Agent 1: 0.07562271]) [Noise stdev: 0.01970592848464821]
Episode 1421/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0901 ; (Q Loss Avg: [Agent 0: 0.00065348, Agent 1: 0.00023042] ; E(V) Avg: [Agent 0: 0.18418727, Agent 1: 0.07548116]) [Noise stdev: 0.019666516627678914]
Episode 1422/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0892 ; (Q Loss Avg: [Agent 0: 0.00068745, Agent 1: 0.00024265] ; E(V) Avg: [Agent 0: 0.18610851, Agent 1: 0.07284831]) [Noise stdev: 0.019627183594423555]
Episode 1423/5000 (68 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0903 ; (Q Loss Avg: [Agent 0: 0.00070277, Agent 1: 0.00023036] ; E(V) Avg: [Agent 0: 0.18466370, Agent 1: 0.07592302]) [Noise stdev: 0.019587929227234707]
Episode 1424/5000 (108 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.0924 ; (Q Loss Avg: [Agent 0: 0.00068484, Agent 1: 0.00026015] ; E(V) Avg: [Agent 0: 0.18483769, Agent 1: 0.07579453]) [Noise stdev: 0.019548753368780238]
Episode 1425/5000 (67 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0934 ; (Q Loss Avg: [Agent 0: 0.00063620, Agent 1: 0.00024610] ; E(V) Avg: [Agent 0: 0.18498393, Agent 1: 0.07574100]) [Noise stdev: 0.019509655862042678]
Episode 1426/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0934 ; (Q Loss Avg: [Agent 0: 0.00061991, Agent 1: 0.00022366] ; E(V) Avg: [Agent 0: 0.18639365, Agent 1: 0.07548143]) [Noise stdev: 0.019470636550318592]
Episode 1427/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0924 ; (Q Loss Avg: [Agent 0: 0.00065752, Agent 1: 0.00024591] ; E(V) Avg: [Agent 0: 0.18882800, Agent 1: 0.07644423]) [Noise stdev: 0.019431695277217956]
Episode 1428/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0914 ; (Q Loss Avg: [Agent 0: 0.00077219, Agent 1: 0.00022688] ; E(V) Avg: [Agent 0: 0.18353035, Agent 1: 0.07636420]) [Noise stdev: 0.01939283188666352]
Episode 1429/5000 (91 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0924 ; (Q Loss Avg: [Agent 0: 0.00068882, Agent 1: 0.00023923] ; E(V) Avg: [Agent 0: 0.18490867, Agent 1: 0.07678322]) [Noise stdev: 0.019354046222890192]
Episode 1430/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0924 ; (Q Loss Avg: [Agent 0: 0.00067399, Agent 1: 0.00022811] ; E(V) Avg: [Agent 0: 0.18837846, Agent 1: 0.07550934]) [Noise stdev: 0.01931533813044441]
Episode 1431/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0934 ; (Q Loss Avg: [Agent 0: 0.00070616, Agent 1: 0.00024695] ; E(V) Avg: [Agent 0: 0.18575707, Agent 1: 0.07743570]) [Noise stdev: 0.01927670745418352]
Episode 1432/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0944 ; (Q Loss Avg: [Agent 0: 0.00072212, Agent 1: 0.00025825] ; E(V) Avg: [Agent 0: 0.18938090, Agent 1: 0.07649852]) [Noise stdev: 0.019238154039275153]
Episode 1433/5000 (108 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.0965 ; (Q Loss Avg: [Agent 0: 0.00066812, Agent 1: 0.00023933] ; E(V) Avg: [Agent 0: 0.18815934, Agent 1: 0.07665351]) [Noise stdev: 0.0191996777311966]
Episode 1434/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0965 ; (Q Loss Avg: [Agent 0: 0.00067975, Agent 1: 0.00027685] ; E(V) Avg: [Agent 0: 0.18665838, Agent 1: 0.07660807]) [Noise stdev: 0.019161278375734208]
Episode 1435/5000 (123 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.0995 ; (Q Loss Avg: [Agent 0: 0.00069291, Agent 1: 0.00024965] ; E(V) Avg: [Agent 0: 0.18390119, Agent 1: 0.07764221]) [Noise stdev: 0.01912295581898274]
Episode 1436/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0995 ; (Q Loss Avg: [Agent 0: 0.00068494, Agent 1: 0.00025871] ; E(V) Avg: [Agent 0: 0.17878045, Agent 1: 0.07782002]) [Noise stdev: 0.019084709907344774]
Episode 1437/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0986 ; (Q Loss Avg: [Agent 0: 0.00067010, Agent 1: 0.00024409] ; E(V) Avg: [Agent 0: 0.18806716, Agent 1: 0.07627162]) [Noise stdev: 0.019046540487530083]
Episode 1438/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0976 ; (Q Loss Avg: [Agent 0: 0.00064002, Agent 1: 0.00024717] ; E(V) Avg: [Agent 0: 0.18407759, Agent 1: 0.07860228]) [Noise stdev: 0.019008447406555024]
Episode 1439/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0986 ; (Q Loss Avg: [Agent 0: 0.00066640, Agent 1: 0.00025659] ; E(V) Avg: [Agent 0: 0.18541254, Agent 1: 0.07783733]) [Noise stdev: 0.018970430511741913]
Episode 1440/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0996 ; (Q Loss Avg: [Agent 0: 0.00063840, Agent 1: 0.00027309] ; E(V) Avg: [Agent 0: 0.18277350, Agent 1: 0.07833979]) [Noise stdev: 0.018932489650718427]
Episode 1441/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1006 ; (Q Loss Avg: [Agent 0: 0.00070160, Agent 1: 0.00030571] ; E(V) Avg: [Agent 0: 0.18560435, Agent 1: 0.07791887]) [Noise stdev: 0.01889462467141699]
Episode 1442/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1006 ; (Q Loss Avg: [Agent 0: 0.00064190, Agent 1: 0.00027988] ; E(V) Avg: [Agent 0: 0.18338101, Agent 1: 0.07751263]) [Noise stdev: 0.018856835422074156]
Episode 1443/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1006 ; (Q Loss Avg: [Agent 0: 0.00068814, Agent 1: 0.00025101] ; E(V) Avg: [Agent 0: 0.18300056, Agent 1: 0.07700945]) [Noise stdev: 0.018819121751230006]
Episode 1444/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0996 ; (Q Loss Avg: [Agent 0: 0.00080896, Agent 1: 0.00021267] ; E(V) Avg: [Agent 0: 0.18610177, Agent 1: 0.07912378]) [Noise stdev: 0.018781483507727546]
Episode 1445/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0986 ; (Q Loss Avg: [Agent 0: 0.00069349, Agent 1: 0.00026262] ; E(V) Avg: [Agent 0: 0.18487544, Agent 1: 0.07748402]) [Noise stdev: 0.01874392054071209]
Episode 1446/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0986 ; (Q Loss Avg: [Agent 0: 0.00070642, Agent 1: 0.00025767] ; E(V) Avg: [Agent 0: 0.18857625, Agent 1: 0.07805564]) [Noise stdev: 0.018706432699630668]
Episode 1447/5000 (91 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0996 ; (Q Loss Avg: [Agent 0: 0.00066873, Agent 1: 0.00024935] ; E(V) Avg: [Agent 0: 0.18694793, Agent 1: 0.07887756]) [Noise stdev: 0.018669019834231408]
Episode 1448/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0996 ; (Q Loss Avg: [Agent 0: 0.00065879, Agent 1: 0.00024799] ; E(V) Avg: [Agent 0: 0.18631634, Agent 1: 0.07899307]) [Noise stdev: 0.018631681794562944]
Episode 1449/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0997 ; (Q Loss Avg: [Agent 0: 0.00064231, Agent 1: 0.00022479] ; E(V) Avg: [Agent 0: 0.18048636, Agent 1: 0.07846305]) [Noise stdev: 0.018594418430973817]
Episode 1450/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0997 ; (Q Loss Avg: [Agent 0: 0.00058787, Agent 1: 0.00027397] ; E(V) Avg: [Agent 0: 0.18692600, Agent 1: 0.07694767]) [Noise stdev: 0.01855722959411187]
Episode 1451/5000 (88 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1007 ; (Q Loss Avg: [Agent 0: 0.00063759, Agent 1: 0.00028782] ; E(V) Avg: [Agent 0: 0.18382372, Agent 1: 0.07807817]) [Noise stdev: 0.018520115134923645]
Episode 1452/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0977 ; (Q Loss Avg: [Agent 0: 0.00060425, Agent 1: 0.00025672] ; E(V) Avg: [Agent 0: 0.18918664, Agent 1: 0.07880552]) [Noise stdev: 0.0184830749046538]
Episode 1453/5000 (71 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0977 ; (Q Loss Avg: [Agent 0: 0.00062346, Agent 1: 0.00026901] ; E(V) Avg: [Agent 0: 0.18392804, Agent 1: 0.07818996]) [Noise stdev: 0.01844610875484449]
Episode 1454/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0967 ; (Q Loss Avg: [Agent 0: 0.00060360, Agent 1: 0.00025860] ; E(V) Avg: [Agent 0: 0.18759457, Agent 1: 0.07903668]) [Noise stdev: 0.018409216537334804]
Episode 1455/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0967 ; (Q Loss Avg: [Agent 0: 0.00058467, Agent 1: 0.00027352] ; E(V) Avg: [Agent 0: 0.18733023, Agent 1: 0.07939811]) [Noise stdev: 0.018372398104260133]
Episode 1456/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0977 ; (Q Loss Avg: [Agent 0: 0.00056332, Agent 1: 0.00028121] ; E(V) Avg: [Agent 0: 0.18397818, Agent 1: 0.07911459]) [Noise stdev: 0.018335653308051612]
Episode 1457/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0967 ; (Q Loss Avg: [Agent 0: 0.00060567, Agent 1: 0.00028049] ; E(V) Avg: [Agent 0: 0.18363692, Agent 1: 0.07826330]) [Noise stdev: 0.01829898200143551]
Episode 1458/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0967 ; (Q Loss Avg: [Agent 0: 0.00059396, Agent 1: 0.00028928] ; E(V) Avg: [Agent 0: 0.18958877, Agent 1: 0.07874513]) [Noise stdev: 0.01826238403743264]
Episode 1459/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0957 ; (Q Loss Avg: [Agent 0: 0.00064143, Agent 1: 0.00028983] ; E(V) Avg: [Agent 0: 0.18229088, Agent 1: 0.07674160]) [Noise stdev: 0.018225859269357776]
Episode 1460/5000 (17 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.0947 ; (Q Loss Avg: [Agent 0: 0.00062917, Agent 1: 0.00032001] ; E(V) Avg: [Agent 0: 0.18119779, Agent 1: 0.07607915]) [Noise stdev: 0.01818940755081906]
Episode 1461/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0947 ; (Q Loss Avg: [Agent 0: 0.00060829, Agent 1: 0.00026744] ; E(V) Avg: [Agent 0: 0.18580412, Agent 1: 0.07854837]) [Noise stdev: 0.018153028735717423]
Episode 1462/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0947 ; (Q Loss Avg: [Agent 0: 0.00056972, Agent 1: 0.00028600] ; E(V) Avg: [Agent 0: 0.18232368, Agent 1: 0.07772976]) [Noise stdev: 0.01811672267824599]
Episode 1463/5000 (106 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.0977 ; (Q Loss Avg: [Agent 0: 0.00062497, Agent 1: 0.00027973] ; E(V) Avg: [Agent 0: 0.18443460, Agent 1: 0.07907350]) [Noise stdev: 0.018080489232889498]
Episode 1464/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0977 ; (Q Loss Avg: [Agent 0: 0.00064022, Agent 1: 0.00026180] ; E(V) Avg: [Agent 0: 0.18190187, Agent 1: 0.08038634]) [Noise stdev: 0.01804432825442372]
Episode 1465/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0987 ; (Q Loss Avg: [Agent 0: 0.00060363, Agent 1: 0.00026736] ; E(V) Avg: [Agent 0: 0.18278884, Agent 1: 0.07940606]) [Noise stdev: 0.018008239597914873]
Episode 1466/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0987 ; (Q Loss Avg: [Agent 0: 0.00052858, Agent 1: 0.00026993] ; E(V) Avg: [Agent 0: 0.18420022, Agent 1: 0.08030625]) [Noise stdev: 0.017972223118719044]
Episode 1467/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0997 ; (Q Loss Avg: [Agent 0: 0.00058898, Agent 1: 0.00027318] ; E(V) Avg: [Agent 0: 0.18242584, Agent 1: 0.07990572]) [Noise stdev: 0.017936278672481605]
Episode 1468/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0997 ; (Q Loss Avg: [Agent 0: 0.00053533, Agent 1: 0.00024731] ; E(V) Avg: [Agent 0: 0.18565144, Agent 1: 0.07948155]) [Noise stdev: 0.01790040611513664]
Episode 1469/5000 (66 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.0997 ; (Q Loss Avg: [Agent 0: 0.00054122, Agent 1: 0.00027200] ; E(V) Avg: [Agent 0: 0.18369908, Agent 1: 0.07968862]) [Noise stdev: 0.01786460530290637]
Episode 1470/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.0997 ; (Q Loss Avg: [Agent 0: 0.00057336, Agent 1: 0.00026989] ; E(V) Avg: [Agent 0: 0.18487096, Agent 1: 0.08018607]) [Noise stdev: 0.017828876092300557]
Episode 1471/5000 (167 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.1027 ; (Q Loss Avg: [Agent 0: 0.00057251, Agent 1: 0.00027453] ; E(V) Avg: [Agent 0: 0.18501405, Agent 1: 0.07988072]) [Noise stdev: 0.017793218340115956]
Episode 1472/5000 (112 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1057 ; (Q Loss Avg: [Agent 0: 0.00055998, Agent 1: 0.00026304] ; E(V) Avg: [Agent 0: 0.18314694, Agent 1: 0.08009468]) [Noise stdev: 0.017757631903435725]
Episode 1473/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1057 ; (Q Loss Avg: [Agent 0: 0.00061637, Agent 1: 0.00025956] ; E(V) Avg: [Agent 0: 0.18277026, Agent 1: 0.07887652]) [Noise stdev: 0.017722116639628853]
Episode 1474/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1057 ; (Q Loss Avg: [Agent 0: 0.00056285, Agent 1: 0.00027047] ; E(V) Avg: [Agent 0: 0.18175740, Agent 1: 0.07917606]) [Noise stdev: 0.017686672406349597]
Episode 1475/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1067 ; (Q Loss Avg: [Agent 0: 0.00056565, Agent 1: 0.00025657] ; E(V) Avg: [Agent 0: 0.18002593, Agent 1: 0.07969969]) [Noise stdev: 0.0176512990615369]
Episode 1476/5000 (202 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.1097 ; (Q Loss Avg: [Agent 0: 0.00057251, Agent 1: 0.00027103] ; E(V) Avg: [Agent 0: 0.18186199, Agent 1: 0.08020363]) [Noise stdev: 0.017615996463413826]
Episode 1477/5000 (34 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1087 ; (Q Loss Avg: [Agent 0: 0.00057074, Agent 1: 0.00029611] ; E(V) Avg: [Agent 0: 0.18243337, Agent 1: 0.08046436]) [Noise stdev: 0.017580764470487]
Episode 1478/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1077 ; (Q Loss Avg: [Agent 0: 0.00055468, Agent 1: 0.00028229] ; E(V) Avg: [Agent 0: 0.18377612, Agent 1: 0.08017943]) [Noise stdev: 0.017545602941546026]
Episode 1479/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1077 ; (Q Loss Avg: [Agent 0: 0.00059864, Agent 1: 0.00030545] ; E(V) Avg: [Agent 0: 0.18486622, Agent 1: 0.08174241]) [Noise stdev: 0.017510511735662933]
Episode 1480/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1087 ; (Q Loss Avg: [Agent 0: 0.00059375, Agent 1: 0.00025907] ; E(V) Avg: [Agent 0: 0.18279406, Agent 1: 0.08122530]) [Noise stdev: 0.017475490712191608]
Episode 1481/5000 (91 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1097 ; (Q Loss Avg: [Agent 0: 0.00053043, Agent 1: 0.00024563] ; E(V) Avg: [Agent 0: 0.18084987, Agent 1: 0.07987692]) [Noise stdev: 0.017440539730767226]
Episode 1482/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1097 ; (Q Loss Avg: [Agent 0: 0.00053023, Agent 1: 0.00025405] ; E(V) Avg: [Agent 0: 0.18026094, Agent 1: 0.08049485]) [Noise stdev: 0.01740565865130569]
Episode 1483/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1097 ; (Q Loss Avg: [Agent 0: 0.00056340, Agent 1: 0.00023815] ; E(V) Avg: [Agent 0: 0.18146364, Agent 1: 0.08001382]) [Noise stdev: 0.01737084733400308]
Episode 1484/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1097 ; (Q Loss Avg: [Agent 0: 0.00056445, Agent 1: 0.00024190] ; E(V) Avg: [Agent 0: 0.18152875, Agent 1: 0.08224991]) [Noise stdev: 0.017336105639335075]
Episode 1485/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1097 ; (Q Loss Avg: [Agent 0: 0.00054899, Agent 1: 0.00023636] ; E(V) Avg: [Agent 0: 0.18212530, Agent 1: 0.08111534]) [Noise stdev: 0.017301433428056404]
Episode 1486/5000 (71 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1107 ; (Q Loss Avg: [Agent 0: 0.00054262, Agent 1: 0.00028006] ; E(V) Avg: [Agent 0: 0.18083472, Agent 1: 0.08124086]) [Noise stdev: 0.01726683056120029]
Episode 1487/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1107 ; (Q Loss Avg: [Agent 0: 0.00055525, Agent 1: 0.00027959] ; E(V) Avg: [Agent 0: 0.17928337, Agent 1: 0.08252355]) [Noise stdev: 0.017232296900077892]
Episode 1488/5000 (73 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1108 ; (Q Loss Avg: [Agent 0: 0.00059958, Agent 1: 0.00027371] ; E(V) Avg: [Agent 0: 0.18114645, Agent 1: 0.08053439]) [Noise stdev: 0.017197832306277736]
Episode 1489/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1108 ; (Q Loss Avg: [Agent 0: 0.00055522, Agent 1: 0.00026984] ; E(V) Avg: [Agent 0: 0.18053988, Agent 1: 0.08095197]) [Noise stdev: 0.01716343664166518]
Episode 1490/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1108 ; (Q Loss Avg: [Agent 0: 0.00058053, Agent 1: 0.00029172] ; E(V) Avg: [Agent 0: 0.18016829, Agent 1: 0.08123750]) [Noise stdev: 0.01712910976838185]
Episode 1491/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1108 ; (Q Loss Avg: [Agent 0: 0.00055896, Agent 1: 0.00024452] ; E(V) Avg: [Agent 0: 0.18121901, Agent 1: 0.08132763]) [Noise stdev: 0.01709485154884509]
Episode 1492/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1108 ; (Q Loss Avg: [Agent 0: 0.00056216, Agent 1: 0.00026246] ; E(V) Avg: [Agent 0: 0.17794069, Agent 1: 0.08289958]) [Noise stdev: 0.017060661845747397]
Episode 1493/5000 (72 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1118 ; (Q Loss Avg: [Agent 0: 0.00050551, Agent 1: 0.00029463] ; E(V) Avg: [Agent 0: 0.17847756, Agent 1: 0.08218003]) [Noise stdev: 0.0170265405220559]
Episode 1494/5000 (57 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1118 ; (Q Loss Avg: [Agent 0: 0.00055744, Agent 1: 0.00024794] ; E(V) Avg: [Agent 0: 0.17939219, Agent 1: 0.08140854]) [Noise stdev: 0.01699248744101179]
Episode 1495/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1128 ; (Q Loss Avg: [Agent 0: 0.00054509, Agent 1: 0.00026727] ; E(V) Avg: [Agent 0: 0.17842756, Agent 1: 0.08207796]) [Noise stdev: 0.016958502466129767]
Episode 1496/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1128 ; (Q Loss Avg: [Agent 0: 0.00051885, Agent 1: 0.00026676] ; E(V) Avg: [Agent 0: 0.17770192, Agent 1: 0.08279562]) [Noise stdev: 0.016924585461197506]
Episode 1497/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1127 ; (Q Loss Avg: [Agent 0: 0.00056758, Agent 1: 0.00023377] ; E(V) Avg: [Agent 0: 0.17896903, Agent 1: 0.08252115]) [Noise stdev: 0.01689073629027511]
Episode 1498/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1127 ; (Q Loss Avg: [Agent 0: 0.00055387, Agent 1: 0.00025714] ; E(V) Avg: [Agent 0: 0.17952670, Agent 1: 0.08430001]) [Noise stdev: 0.01685695481769456]
Episode 1499/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1127 ; (Q Loss Avg: [Agent 0: 0.00054823, Agent 1: 0.00026375] ; E(V) Avg: [Agent 0: 0.17659941, Agent 1: 0.08228730]) [Noise stdev: 0.01682324090805917]
Episode 1500/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1126 ; (Q Loss Avg: [Agent 0: 0.00061770, Agent 1: 0.00025620] ; E(V) Avg: [Agent 0: 0.17690057, Agent 1: 0.08380656]) [Noise stdev: 0.01678959442624305]
Episode 1501/5000 (110 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1156 ; (Q Loss Avg: [Agent 0: 0.00051954, Agent 1: 0.00026420] ; E(V) Avg: [Agent 0: 0.17729899, Agent 1: 0.08216284]) [Noise stdev: 0.016756015237390565]
Episode 1502/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1156 ; (Q Loss Avg: [Agent 0: 0.00054618, Agent 1: 0.00026553] ; E(V) Avg: [Agent 0: 0.17977407, Agent 1: 0.08298576]) [Noise stdev: 0.016722503206915783]
Episode 1503/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1155 ; (Q Loss Avg: [Agent 0: 0.00053817, Agent 1: 0.00026895] ; E(V) Avg: [Agent 0: 0.17658298, Agent 1: 0.08286915]) [Noise stdev: 0.01668905820050195]
Episode 1504/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1155 ; (Q Loss Avg: [Agent 0: 0.00052448, Agent 1: 0.00022437] ; E(V) Avg: [Agent 0: 0.17755334, Agent 1: 0.08503076]) [Noise stdev: 0.016655680084100948]
Episode 1505/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1156 ; (Q Loss Avg: [Agent 0: 0.00051519, Agent 1: 0.00024991] ; E(V) Avg: [Agent 0: 0.17494822, Agent 1: 0.08288014]) [Noise stdev: 0.016622368723932746]
Episode 1506/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1166 ; (Q Loss Avg: [Agent 0: 0.00051531, Agent 1: 0.00024115] ; E(V) Avg: [Agent 0: 0.17772844, Agent 1: 0.08407700]) [Noise stdev: 0.01658912398648488]
Episode 1507/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1176 ; (Q Loss Avg: [Agent 0: 0.00048962, Agent 1: 0.00026134] ; E(V) Avg: [Agent 0: 0.17608019, Agent 1: 0.08308394]) [Noise stdev: 0.01655594573851191]
Episode 1508/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1166 ; (Q Loss Avg: [Agent 0: 0.00054915, Agent 1: 0.00026112] ; E(V) Avg: [Agent 0: 0.18028348, Agent 1: 0.08270150]) [Noise stdev: 0.016522833847034887]
Episode 1509/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1157 ; (Q Loss Avg: [Agent 0: 0.00053504, Agent 1: 0.00026535] ; E(V) Avg: [Agent 0: 0.17605313, Agent 1: 0.08352990]) [Noise stdev: 0.016489788179340818]
Episode 1510/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1167 ; (Q Loss Avg: [Agent 0: 0.00058081, Agent 1: 0.00029320] ; E(V) Avg: [Agent 0: 0.17590381, Agent 1: 0.08278165]) [Noise stdev: 0.016456808602982136]
Episode 1511/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1167 ; (Q Loss Avg: [Agent 0: 0.00050389, Agent 1: 0.00027849] ; E(V) Avg: [Agent 0: 0.17407769, Agent 1: 0.08355301]) [Noise stdev: 0.01642389498577617]
Episode 1512/5000 (71 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1177 ; (Q Loss Avg: [Agent 0: 0.00050106, Agent 1: 0.00024680] ; E(V) Avg: [Agent 0: 0.17550801, Agent 1: 0.08317453]) [Noise stdev: 0.016391047195804618]
Episode 1513/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1186 ; (Q Loss Avg: [Agent 0: 0.00049425, Agent 1: 0.00023980] ; E(V) Avg: [Agent 0: 0.17603263, Agent 1: 0.08575300]) [Noise stdev: 0.01635826510141301]
Episode 1514/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1196 ; (Q Loss Avg: [Agent 0: 0.00050015, Agent 1: 0.00026084] ; E(V) Avg: [Agent 0: 0.17651481, Agent 1: 0.08412856]) [Noise stdev: 0.016325548571210186]
Episode 1515/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1195 ; (Q Loss Avg: [Agent 0: 0.00052612, Agent 1: 0.00024947] ; E(V) Avg: [Agent 0: 0.17469379, Agent 1: 0.08282704]) [Noise stdev: 0.016292897474067765]
Episode 1516/5000 (104 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1205 ; (Q Loss Avg: [Agent 0: 0.00052721, Agent 1: 0.00024920] ; E(V) Avg: [Agent 0: 0.17586838, Agent 1: 0.08448489]) [Noise stdev: 0.016260311679119628]
Episode 1517/5000 (58 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1205 ; (Q Loss Avg: [Agent 0: 0.00058143, Agent 1: 0.00025557] ; E(V) Avg: [Agent 0: 0.17390553, Agent 1: 0.08377217]) [Noise stdev: 0.01622779105576139]
Episode 1518/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1205 ; (Q Loss Avg: [Agent 0: 0.00056757, Agent 1: 0.00025030] ; E(V) Avg: [Agent 0: 0.17296267, Agent 1: 0.08266286]) [Noise stdev: 0.016195335473649868]
Episode 1519/5000 (71 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1215 ; (Q Loss Avg: [Agent 0: 0.00053982, Agent 1: 0.00025709] ; E(V) Avg: [Agent 0: 0.17422721, Agent 1: 0.08386074]) [Noise stdev: 0.016162944802702566]
Episode 1520/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.1205 ; (Q Loss Avg: [Agent 0: 0.00049991, Agent 1: 0.00029404] ; E(V) Avg: [Agent 0: 0.17669166, Agent 1: 0.08238947]) [Noise stdev: 0.01613061891309716]
Episode 1521/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1205 ; (Q Loss Avg: [Agent 0: 0.00046429, Agent 1: 0.00026345] ; E(V) Avg: [Agent 0: 0.17514449, Agent 1: 0.08580977]) [Noise stdev: 0.016098357675270965]
Episode 1522/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1214 ; (Q Loss Avg: [Agent 0: 0.00049963, Agent 1: 0.00027406] ; E(V) Avg: [Agent 0: 0.17398798, Agent 1: 0.08435731]) [Noise stdev: 0.016066160959920424]
Episode 1523/5000 (57 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1204 ; (Q Loss Avg: [Agent 0: 0.00052080, Agent 1: 0.00027192] ; E(V) Avg: [Agent 0: 0.17224131, Agent 1: 0.08521150]) [Noise stdev: 0.01603402863800058]
Episode 1524/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1184 ; (Q Loss Avg: [Agent 0: 0.00049281, Agent 1: 0.00024922] ; E(V) Avg: [Agent 0: 0.17036542, Agent 1: 0.08415380]) [Noise stdev: 0.016001960580724582]
Episode 1525/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1174 ; (Q Loss Avg: [Agent 0: 0.00047401, Agent 1: 0.00023050] ; E(V) Avg: [Agent 0: 0.17528338, Agent 1: 0.08308069]) [Noise stdev: 0.015969956659563134]
Episode 1526/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1173 ; (Q Loss Avg: [Agent 0: 0.00048515, Agent 1: 0.00023756] ; E(V) Avg: [Agent 0: 0.17222990, Agent 1: 0.08466620]) [Noise stdev: 0.015938016746244007]
Episode 1527/5000 (88 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1193 ; (Q Loss Avg: [Agent 0: 0.00054066, Agent 1: 0.00027386] ; E(V) Avg: [Agent 0: 0.17329716, Agent 1: 0.08414019]) [Noise stdev: 0.01590614071275152]
Episode 1528/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1203 ; (Q Loss Avg: [Agent 0: 0.00055454, Agent 1: 0.00028198] ; E(V) Avg: [Agent 0: 0.17089489, Agent 1: 0.08558403]) [Noise stdev: 0.015874328431326017]
Episode 1529/5000 (138 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1213 ; (Q Loss Avg: [Agent 0: 0.00051154, Agent 1: 0.00025851] ; E(V) Avg: [Agent 0: 0.17017001, Agent 1: 0.08439741]) [Noise stdev: 0.015842579774463364]
Episode 1530/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1213 ; (Q Loss Avg: [Agent 0: 0.00047845, Agent 1: 0.00027867] ; E(V) Avg: [Agent 0: 0.17287711, Agent 1: 0.08469217]) [Noise stdev: 0.015810894614914438]
Episode 1531/5000 (70 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1222 ; (Q Loss Avg: [Agent 0: 0.00049894, Agent 1: 0.00025069] ; E(V) Avg: [Agent 0: 0.17089093, Agent 1: 0.08437986]) [Noise stdev: 0.015779272825684608]
Episode 1532/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1222 ; (Q Loss Avg: [Agent 0: 0.00054461, Agent 1: 0.00025217] ; E(V) Avg: [Agent 0: 0.16915656, Agent 1: 0.08552441]) [Noise stdev: 0.01574771428003324]
Episode 1533/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1202 ; (Q Loss Avg: [Agent 0: 0.00049125, Agent 1: 0.00030293] ; E(V) Avg: [Agent 0: 0.16765446, Agent 1: 0.08450630]) [Noise stdev: 0.015716218851473174]
Episode 1534/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1202 ; (Q Loss Avg: [Agent 0: 0.00054243, Agent 1: 0.00032071] ; E(V) Avg: [Agent 0: 0.16772170, Agent 1: 0.08635821]) [Noise stdev: 0.01568478641377023]
Episode 1535/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1182 ; (Q Loss Avg: [Agent 0: 0.00051938, Agent 1: 0.00026758] ; E(V) Avg: [Agent 0: 0.17055262, Agent 1: 0.08548593]) [Noise stdev: 0.015653416840942687]
Episode 1536/5000 (59 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1192 ; (Q Loss Avg: [Agent 0: 0.00048078, Agent 1: 0.00027102] ; E(V) Avg: [Agent 0: 0.16868610, Agent 1: 0.08586283]) [Noise stdev: 0.015622110007260801]
Episode 1537/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1202 ; (Q Loss Avg: [Agent 0: 0.00051289, Agent 1: 0.00027799] ; E(V) Avg: [Agent 0: 0.16917686, Agent 1: 0.08562464]) [Noise stdev: 0.01559086578724628]
Episode 1538/5000 (78 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1222 ; (Q Loss Avg: [Agent 0: 0.00047494, Agent 1: 0.00026005] ; E(V) Avg: [Agent 0: 0.16994634, Agent 1: 0.08410744]) [Noise stdev: 0.015559684055671787]
Episode 1539/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1222 ; (Q Loss Avg: [Agent 0: 0.00047108, Agent 1: 0.00026144] ; E(V) Avg: [Agent 0: 0.16802960, Agent 1: 0.08510562]) [Noise stdev: 0.015528564687560444]
Episode 1540/5000 (51 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1212 ; (Q Loss Avg: [Agent 0: 0.00052098, Agent 1: 0.00027645] ; E(V) Avg: [Agent 0: 0.16844860, Agent 1: 0.08600754]) [Noise stdev: 0.015497507558185323]
Episode 1541/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1202 ; (Q Loss Avg: [Agent 0: 0.00052235, Agent 1: 0.00025369] ; E(V) Avg: [Agent 0: 0.16852701, Agent 1: 0.08496829]) [Noise stdev: 0.015466512543068953]
Episode 1542/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1202 ; (Q Loss Avg: [Agent 0: 0.00049126, Agent 1: 0.00028291] ; E(V) Avg: [Agent 0: 0.16601273, Agent 1: 0.08390195]) [Noise stdev: 0.015435579517982814]
Episode 1543/5000 (36 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1202 ; (Q Loss Avg: [Agent 0: 0.00047676, Agent 1: 0.00029165] ; E(V) Avg: [Agent 0: 0.16631842, Agent 1: 0.08544984]) [Noise stdev: 0.01540470835894685]
Episode 1544/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1212 ; (Q Loss Avg: [Agent 0: 0.00050274, Agent 1: 0.00028977] ; E(V) Avg: [Agent 0: 0.17001511, Agent 1: 0.08556138]) [Noise stdev: 0.015373898942228955]
Episode 1545/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1222 ; (Q Loss Avg: [Agent 0: 0.00049191, Agent 1: 0.00025747] ; E(V) Avg: [Agent 0: 0.16662610, Agent 1: 0.08377592]) [Noise stdev: 0.015343151144344497]
Episode 1546/5000 (58 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1232 ; (Q Loss Avg: [Agent 0: 0.00045590, Agent 1: 0.00023982] ; E(V) Avg: [Agent 0: 0.16681392, Agent 1: 0.08566624]) [Noise stdev: 0.015312464842055808]
Episode 1547/5000 (112 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1242 ; (Q Loss Avg: [Agent 0: 0.00048248, Agent 1: 0.00024332] ; E(V) Avg: [Agent 0: 0.16631490, Agent 1: 0.08523534]) [Noise stdev: 0.015281839912371697]
Episode 1548/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1242 ; (Q Loss Avg: [Agent 0: 0.00049083, Agent 1: 0.00025408] ; E(V) Avg: [Agent 0: 0.16551205, Agent 1: 0.08603104]) [Noise stdev: 0.015251276232546953]
Episode 1549/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1242 ; (Q Loss Avg: [Agent 0: 0.00047961, Agent 1: 0.00026467] ; E(V) Avg: [Agent 0: 0.16567635, Agent 1: 0.08567711]) [Noise stdev: 0.015220773680081859]
Episode 1550/5000 (71 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1252 ; (Q Loss Avg: [Agent 0: 0.00046545, Agent 1: 0.00025181] ; E(V) Avg: [Agent 0: 0.16389691, Agent 1: 0.08497692]) [Noise stdev: 0.015190332132721696]
Episode 1551/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1242 ; (Q Loss Avg: [Agent 0: 0.00046513, Agent 1: 0.00026210] ; E(V) Avg: [Agent 0: 0.16631334, Agent 1: 0.08607396]) [Noise stdev: 0.015159951468456253]
Episode 1552/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1252 ; (Q Loss Avg: [Agent 0: 0.00042357, Agent 1: 0.00026273] ; E(V) Avg: [Agent 0: 0.16668377, Agent 1: 0.08654402]) [Noise stdev: 0.01512963156551934]
Episode 1553/5000 (72 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1252 ; (Q Loss Avg: [Agent 0: 0.00044810, Agent 1: 0.00025444] ; E(V) Avg: [Agent 0: 0.16416562, Agent 1: 0.08440962]) [Noise stdev: 0.015099372302388302]
Episode 1554/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1261 ; (Q Loss Avg: [Agent 0: 0.00052145, Agent 1: 0.00026570] ; E(V) Avg: [Agent 0: 0.16323812, Agent 1: 0.08577660]) [Noise stdev: 0.015069173557783526]
Episode 1555/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1261 ; (Q Loss Avg: [Agent 0: 0.00047206, Agent 1: 0.00027743] ; E(V) Avg: [Agent 0: 0.16316815, Agent 1: 0.08516518]) [Noise stdev: 0.015039035210667958]
Episode 1556/5000 (71 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1261 ; (Q Loss Avg: [Agent 0: 0.00045176, Agent 1: 0.00026703] ; E(V) Avg: [Agent 0: 0.16215676, Agent 1: 0.08651797]) [Noise stdev: 0.015008957140246621]
Episode 1557/5000 (51 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1271 ; (Q Loss Avg: [Agent 0: 0.00042700, Agent 1: 0.00029895] ; E(V) Avg: [Agent 0: 0.16332710, Agent 1: 0.08599673]) [Noise stdev: 0.014978939225966128]
Episode 1558/5000 (35 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1281 ; (Q Loss Avg: [Agent 0: 0.00040124, Agent 1: 0.00027542] ; E(V) Avg: [Agent 0: 0.16385197, Agent 1: 0.08623069]) [Noise stdev: 0.014948981347514196]
Episode 1559/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1291 ; (Q Loss Avg: [Agent 0: 0.00044273, Agent 1: 0.00024637] ; E(V) Avg: [Agent 0: 0.16201881, Agent 1: 0.08553533]) [Noise stdev: 0.014919083384819168]
Episode 1560/5000 (67 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1311 ; (Q Loss Avg: [Agent 0: 0.00047262, Agent 1: 0.00023878] ; E(V) Avg: [Agent 0: 0.16291584, Agent 1: 0.08658963]) [Noise stdev: 0.014889245218049529]
Episode 1561/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1311 ; (Q Loss Avg: [Agent 0: 0.00046254, Agent 1: 0.00026034] ; E(V) Avg: [Agent 0: 0.16097731, Agent 1: 0.08574326]) [Noise stdev: 0.01485946672761343]
Episode 1562/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1311 ; (Q Loss Avg: [Agent 0: 0.00044330, Agent 1: 0.00021902] ; E(V) Avg: [Agent 0: 0.16134172, Agent 1: 0.08571269]) [Noise stdev: 0.014829747794158204]
Episode 1563/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1291 ; (Q Loss Avg: [Agent 0: 0.00041790, Agent 1: 0.00024285] ; E(V) Avg: [Agent 0: 0.15932076, Agent 1: 0.08538374]) [Noise stdev: 0.014800088298569887]
Episode 1564/5000 (71 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1301 ; (Q Loss Avg: [Agent 0: 0.00043765, Agent 1: 0.00027520] ; E(V) Avg: [Agent 0: 0.16251594, Agent 1: 0.08584317]) [Noise stdev: 0.014770488121972748]
Episode 1565/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1301 ; (Q Loss Avg: [Agent 0: 0.00044646, Agent 1: 0.00024996] ; E(V) Avg: [Agent 0: 0.16018459, Agent 1: 0.08690501]) [Noise stdev: 0.014740947145728803]
Episode 1566/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1301 ; (Q Loss Avg: [Agent 0: 0.00043783, Agent 1: 0.00026728] ; E(V) Avg: [Agent 0: 0.16136750, Agent 1: 0.08559300]) [Noise stdev: 0.014711465251437345]
Episode 1567/5000 (51 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1301 ; (Q Loss Avg: [Agent 0: 0.00047610, Agent 1: 0.00027269] ; E(V) Avg: [Agent 0: 0.16073565, Agent 1: 0.08572426]) [Noise stdev: 0.01468204232093447]
Episode 1568/5000 (74 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1311 ; (Q Loss Avg: [Agent 0: 0.00048295, Agent 1: 0.00026600] ; E(V) Avg: [Agent 0: 0.16166019, Agent 1: 0.08639882]) [Noise stdev: 0.0146526782362926]
Episode 1569/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1301 ; (Q Loss Avg: [Agent 0: 0.00043182, Agent 1: 0.00026060] ; E(V) Avg: [Agent 0: 0.16229543, Agent 1: 0.08590381]) [Noise stdev: 0.014623372879820016]
Episode 1570/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1311 ; (Q Loss Avg: [Agent 0: 0.00046113, Agent 1: 0.00026979] ; E(V) Avg: [Agent 0: 0.16192178, Agent 1: 0.08722097]) [Noise stdev: 0.014594126134060376]
Episode 1571/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1291 ; (Q Loss Avg: [Agent 0: 0.00046411, Agent 1: 0.00026677] ; E(V) Avg: [Agent 0: 0.16125448, Agent 1: 0.08675390]) [Noise stdev: 0.014564937881792256]
Episode 1572/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1281 ; (Q Loss Avg: [Agent 0: 0.00046261, Agent 1: 0.00025850] ; E(V) Avg: [Agent 0: 0.16116786, Agent 1: 0.08554915]) [Noise stdev: 0.014535808006028671]
Episode 1573/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1281 ; (Q Loss Avg: [Agent 0: 0.00041846, Agent 1: 0.00024235] ; E(V) Avg: [Agent 0: 0.15867152, Agent 1: 0.08659612]) [Noise stdev: 0.014506736390016614]
Episode 1574/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1281 ; (Q Loss Avg: [Agent 0: 0.00041791, Agent 1: 0.00023390] ; E(V) Avg: [Agent 0: 0.16030881, Agent 1: 0.08631178]) [Noise stdev: 0.01447772291723658]
Episode 1575/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1271 ; (Q Loss Avg: [Agent 0: 0.00043371, Agent 1: 0.00026976] ; E(V) Avg: [Agent 0: 0.15737823, Agent 1: 0.08666835]) [Noise stdev: 0.014448767471402108]
Episode 1576/5000 (112 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1251 ; (Q Loss Avg: [Agent 0: 0.00044425, Agent 1: 0.00027729] ; E(V) Avg: [Agent 0: 0.16055088, Agent 1: 0.08642445]) [Noise stdev: 0.014419869936459304]
Episode 1577/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1251 ; (Q Loss Avg: [Agent 0: 0.00038089, Agent 1: 0.00025988] ; E(V) Avg: [Agent 0: 0.15850021, Agent 1: 0.08488300]) [Noise stdev: 0.014391030196586385]
Episode 1578/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1251 ; (Q Loss Avg: [Agent 0: 0.00041788, Agent 1: 0.00029137] ; E(V) Avg: [Agent 0: 0.16076359, Agent 1: 0.08648611]) [Noise stdev: 0.014362248136193212]
Episode 1579/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1251 ; (Q Loss Avg: [Agent 0: 0.00041641, Agent 1: 0.00027693] ; E(V) Avg: [Agent 0: 0.15928220, Agent 1: 0.08696550]) [Noise stdev: 0.014333523639920825]
Episode 1580/5000 (71 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1261 ; (Q Loss Avg: [Agent 0: 0.00044316, Agent 1: 0.00026185] ; E(V) Avg: [Agent 0: 0.16067431, Agent 1: 0.08684046]) [Noise stdev: 0.014304856592640984]
Episode 1581/5000 (80 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1260 ; (Q Loss Avg: [Agent 0: 0.00041115, Agent 1: 0.00026893] ; E(V) Avg: [Agent 0: 0.15858810, Agent 1: 0.08666652]) [Noise stdev: 0.014276246879455702]
Episode 1582/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1260 ; (Q Loss Avg: [Agent 0: 0.00038489, Agent 1: 0.00026456] ; E(V) Avg: [Agent 0: 0.15861659, Agent 1: 0.08655403]) [Noise stdev: 0.01424769438569679]
Episode 1583/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1260 ; (Q Loss Avg: [Agent 0: 0.00039537, Agent 1: 0.00023948] ; E(V) Avg: [Agent 0: 0.15728888, Agent 1: 0.08566745]) [Noise stdev: 0.014219198996925397]
Episode 1584/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1270 ; (Q Loss Avg: [Agent 0: 0.00042273, Agent 1: 0.00026421] ; E(V) Avg: [Agent 0: 0.15757398, Agent 1: 0.08692113]) [Noise stdev: 0.014190760598931547]
Episode 1585/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1270 ; (Q Loss Avg: [Agent 0: 0.00038085, Agent 1: 0.00027342] ; E(V) Avg: [Agent 0: 0.15816204, Agent 1: 0.08747748]) [Noise stdev: 0.014162379077733683]
Episode 1586/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1260 ; (Q Loss Avg: [Agent 0: 0.00038954, Agent 1: 0.00025550] ; E(V) Avg: [Agent 0: 0.15600756, Agent 1: 0.08617943]) [Noise stdev: 0.014134054319578216]
Episode 1587/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1260 ; (Q Loss Avg: [Agent 0: 0.00038994, Agent 1: 0.00028882] ; E(V) Avg: [Agent 0: 0.15699530, Agent 1: 0.08706868]) [Noise stdev: 0.01410578621093906]
Episode 1588/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1260 ; (Q Loss Avg: [Agent 0: 0.00039978, Agent 1: 0.00025740] ; E(V) Avg: [Agent 0: 0.15730594, Agent 1: 0.08605974]) [Noise stdev: 0.014077574638517183]
Episode 1589/5000 (70 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1269 ; (Q Loss Avg: [Agent 0: 0.00040077, Agent 1: 0.00023846] ; E(V) Avg: [Agent 0: 0.15811039, Agent 1: 0.08550799]) [Noise stdev: 0.014049419489240149]
Episode 1590/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1269 ; (Q Loss Avg: [Agent 0: 0.00039604, Agent 1: 0.00025518] ; E(V) Avg: [Agent 0: 0.15460230, Agent 1: 0.08678840]) [Noise stdev: 0.014021320650261668]
Episode 1591/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1269 ; (Q Loss Avg: [Agent 0: 0.00042370, Agent 1: 0.00024982] ; E(V) Avg: [Agent 0: 0.15542634, Agent 1: 0.08738768]) [Noise stdev: 0.013993278008961144]
Episode 1592/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1269 ; (Q Loss Avg: [Agent 0: 0.00045408, Agent 1: 0.00027484] ; E(V) Avg: [Agent 0: 0.15640699, Agent 1: 0.08568340]) [Noise stdev: 0.013965291452943222]
Episode 1593/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1259 ; (Q Loss Avg: [Agent 0: 0.00044339, Agent 1: 0.00027228] ; E(V) Avg: [Agent 0: 0.15439067, Agent 1: 0.08660394]) [Noise stdev: 0.013937360870037337]
Episode 1594/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1259 ; (Q Loss Avg: [Agent 0: 0.00042855, Agent 1: 0.00026357] ; E(V) Avg: [Agent 0: 0.15645672, Agent 1: 0.08722512]) [Noise stdev: 0.013909486148297262]
Episode 1595/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1259 ; (Q Loss Avg: [Agent 0: 0.00040022, Agent 1: 0.00028432] ; E(V) Avg: [Agent 0: 0.15508092, Agent 1: 0.08581826]) [Noise stdev: 0.013881667176000667]
Episode 1596/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1259 ; (Q Loss Avg: [Agent 0: 0.00036380, Agent 1: 0.00024795] ; E(V) Avg: [Agent 0: 0.15693379, Agent 1: 0.08556297]) [Noise stdev: 0.013853903841648666]
Episode 1597/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1270 ; (Q Loss Avg: [Agent 0: 0.00037656, Agent 1: 0.00024931] ; E(V) Avg: [Agent 0: 0.15523814, Agent 1: 0.08671778]) [Noise stdev: 0.01382619603396537]
Episode 1598/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1269 ; (Q Loss Avg: [Agent 0: 0.00038809, Agent 1: 0.00028860] ; E(V) Avg: [Agent 0: 0.15653997, Agent 1: 0.08584063]) [Noise stdev: 0.013798543641897438]
Episode 1599/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1269 ; (Q Loss Avg: [Agent 0: 0.00036047, Agent 1: 0.00024673] ; E(V) Avg: [Agent 0: 0.15393940, Agent 1: 0.08544435]) [Noise stdev: 0.013770946554613642]
Episode 1600/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1270 ; (Q Loss Avg: [Agent 0: 0.00040631, Agent 1: 0.00024487] ; E(V) Avg: [Agent 0: 0.15664455, Agent 1: 0.08591404]) [Noise stdev: 0.013743404661504414]
Episode 1601/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1250 ; (Q Loss Avg: [Agent 0: 0.00035910, Agent 1: 0.00026570] ; E(V) Avg: [Agent 0: 0.15421831, Agent 1: 0.08562098]) [Noise stdev: 0.013715917852181406]
Episode 1602/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.1240 ; (Q Loss Avg: [Agent 0: 0.00040515, Agent 1: 0.00024705] ; E(V) Avg: [Agent 0: 0.15545961, Agent 1: 0.08829474]) [Noise stdev: 0.013688486016477043]
Episode 1603/5000 (93 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1251 ; (Q Loss Avg: [Agent 0: 0.00041560, Agent 1: 0.00024237] ; E(V) Avg: [Agent 0: 0.15601314, Agent 1: 0.08609876]) [Noise stdev: 0.013661109044444088]
Episode 1604/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1251 ; (Q Loss Avg: [Agent 0: 0.00038704, Agent 1: 0.00026643] ; E(V) Avg: [Agent 0: 0.15495418, Agent 1: 0.08611506]) [Noise stdev: 0.0136337868263552]
Episode 1605/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1251 ; (Q Loss Avg: [Agent 0: 0.00040450, Agent 1: 0.00024288] ; E(V) Avg: [Agent 0: 0.15430011, Agent 1: 0.08442294]) [Noise stdev: 0.01360651925270249]
Episode 1606/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1250 ; (Q Loss Avg: [Agent 0: 0.00041024, Agent 1: 0.00026409] ; E(V) Avg: [Agent 0: 0.15617181, Agent 1: 0.08588311]) [Noise stdev: 0.013579306214197085]
Episode 1607/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1250 ; (Q Loss Avg: [Agent 0: 0.00036657, Agent 1: 0.00025758] ; E(V) Avg: [Agent 0: 0.15461414, Agent 1: 0.08604297]) [Noise stdev: 0.013552147601768691]
Episode 1608/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1250 ; (Q Loss Avg: [Agent 0: 0.00035226, Agent 1: 0.00026566] ; E(V) Avg: [Agent 0: 0.15574518, Agent 1: 0.08645602]) [Noise stdev: 0.013525043306565154]
Episode 1609/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1250 ; (Q Loss Avg: [Agent 0: 0.00042622, Agent 1: 0.00023870] ; E(V) Avg: [Agent 0: 0.15537851, Agent 1: 0.08395807]) [Noise stdev: 0.013497993219952023]
Episode 1610/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1250 ; (Q Loss Avg: [Agent 0: 0.00038509, Agent 1: 0.00026506] ; E(V) Avg: [Agent 0: 0.15454463, Agent 1: 0.08675820]) [Noise stdev: 0.01347099723351212]
Episode 1611/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1250 ; (Q Loss Avg: [Agent 0: 0.00040327, Agent 1: 0.00024616] ; E(V) Avg: [Agent 0: 0.15342685, Agent 1: 0.08587852]) [Noise stdev: 0.013444055239045096]
Episode 1612/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1239 ; (Q Loss Avg: [Agent 0: 0.00039051, Agent 1: 0.00026300] ; E(V) Avg: [Agent 0: 0.15550193, Agent 1: 0.08573814]) [Noise stdev: 0.013417167128567007]
Episode 1613/5000 (34 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1240 ; (Q Loss Avg: [Agent 0: 0.00039822, Agent 1: 0.00026572] ; E(V) Avg: [Agent 0: 0.15448065, Agent 1: 0.08622625]) [Noise stdev: 0.013390332794309873]
Episode 1614/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1250 ; (Q Loss Avg: [Agent 0: 0.00042832, Agent 1: 0.00023227] ; E(V) Avg: [Agent 0: 0.15410471, Agent 1: 0.08540717]) [Noise stdev: 0.013363552128721253]
Episode 1615/5000 (188 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.1291 ; (Q Loss Avg: [Agent 0: 0.00038864, Agent 1: 0.00023418] ; E(V) Avg: [Agent 0: 0.15305721, Agent 1: 0.08592781]) [Noise stdev: 0.013336825024463811]
Episode 1616/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1280 ; (Q Loss Avg: [Agent 0: 0.00038540, Agent 1: 0.00021759] ; E(V) Avg: [Agent 0: 0.15288612, Agent 1: 0.08591857]) [Noise stdev: 0.013310151374414884]
Episode 1617/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1280 ; (Q Loss Avg: [Agent 0: 0.00040097, Agent 1: 0.00023064] ; E(V) Avg: [Agent 0: 0.15497841, Agent 1: 0.08455571]) [Noise stdev: 0.013283531071666054]
Episode 1618/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1290 ; (Q Loss Avg: [Agent 0: 0.00036086, Agent 1: 0.00025517] ; E(V) Avg: [Agent 0: 0.15168463, Agent 1: 0.08588643]) [Noise stdev: 0.013256964009522722]
Episode 1619/5000 (38 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1280 ; (Q Loss Avg: [Agent 0: 0.00036625, Agent 1: 0.00028260] ; E(V) Avg: [Agent 0: 0.15342884, Agent 1: 0.08367162]) [Noise stdev: 0.013230450081503677]
Episode 1620/5000 (68 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1300 ; (Q Loss Avg: [Agent 0: 0.00037515, Agent 1: 0.00027017] ; E(V) Avg: [Agent 0: 0.15163199, Agent 1: 0.08516718]) [Noise stdev: 0.01320398918134067]
Episode 1621/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1300 ; (Q Loss Avg: [Agent 0: 0.00038377, Agent 1: 0.00022996] ; E(V) Avg: [Agent 0: 0.15137770, Agent 1: 0.08643885]) [Noise stdev: 0.013177581202977988]
Episode 1622/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1301 ; (Q Loss Avg: [Agent 0: 0.00036024, Agent 1: 0.00025085] ; E(V) Avg: [Agent 0: 0.15098022, Agent 1: 0.08638338]) [Noise stdev: 0.013151226040572032]
Episode 1623/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1301 ; (Q Loss Avg: [Agent 0: 0.00035435, Agent 1: 0.00024696] ; E(V) Avg: [Agent 0: 0.15168179, Agent 1: 0.08532743]) [Noise stdev: 0.013124923588490888]
Episode 1624/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1301 ; (Q Loss Avg: [Agent 0: 0.00039506, Agent 1: 0.00024779] ; E(V) Avg: [Agent 0: 0.15284954, Agent 1: 0.08628187]) [Noise stdev: 0.013098673741313906]
Episode 1625/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1301 ; (Q Loss Avg: [Agent 0: 0.00039549, Agent 1: 0.00027130] ; E(V) Avg: [Agent 0: 0.15197569, Agent 1: 0.08555111]) [Noise stdev: 0.013072476393831278]
Episode 1626/5000 (149 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.1332 ; (Q Loss Avg: [Agent 0: 0.00038044, Agent 1: 0.00025051] ; E(V) Avg: [Agent 0: 0.14989530, Agent 1: 0.08591298]) [Noise stdev: 0.013046331441043616]
Episode 1627/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1321 ; (Q Loss Avg: [Agent 0: 0.00036778, Agent 1: 0.00027287] ; E(V) Avg: [Agent 0: 0.15031329, Agent 1: 0.08701756]) [Noise stdev: 0.013020238778161528]
Episode 1628/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1321 ; (Q Loss Avg: [Agent 0: 0.00043155, Agent 1: 0.00025980] ; E(V) Avg: [Agent 0: 0.14990860, Agent 1: 0.08691759]) [Noise stdev: 0.012994198300605206]
Episode 1629/5000 (29 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1300 ; (Q Loss Avg: [Agent 0: 0.00036381, Agent 1: 0.00022030] ; E(V) Avg: [Agent 0: 0.15107529, Agent 1: 0.08651864]) [Noise stdev: 0.012968209904003995]
Episode 1630/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1300 ; (Q Loss Avg: [Agent 0: 0.00033984, Agent 1: 0.00022183] ; E(V) Avg: [Agent 0: 0.15085971, Agent 1: 0.08549750]) [Noise stdev: 0.012942273484195986]
Episode 1631/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1291 ; (Q Loss Avg: [Agent 0: 0.00035716, Agent 1: 0.00023100] ; E(V) Avg: [Agent 0: 0.14897164, Agent 1: 0.08611516]) [Noise stdev: 0.012916388937227593]
Episode 1632/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1290 ; (Q Loss Avg: [Agent 0: 0.00037981, Agent 1: 0.00027836] ; E(V) Avg: [Agent 0: 0.14765556, Agent 1: 0.08614954]) [Noise stdev: 0.012890556159353138]
Episode 1633/5000 (41 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1290 ; (Q Loss Avg: [Agent 0: 0.00036648, Agent 1: 0.00021724] ; E(V) Avg: [Agent 0: 0.14836665, Agent 1: 0.08551236]) [Noise stdev: 0.012864775047034432]
Episode 1634/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1290 ; (Q Loss Avg: [Agent 0: 0.00035966, Agent 1: 0.00020464] ; E(V) Avg: [Agent 0: 0.14758030, Agent 1: 0.08634279]) [Noise stdev: 0.012839045496940362]
Episode 1635/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1289 ; (Q Loss Avg: [Agent 0: 0.00034924, Agent 1: 0.00022299] ; E(V) Avg: [Agent 0: 0.14668571, Agent 1: 0.08568734]) [Noise stdev: 0.012813367405946482]
Episode 1636/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1299 ; (Q Loss Avg: [Agent 0: 0.00035745, Agent 1: 0.00027666] ; E(V) Avg: [Agent 0: 0.14814248, Agent 1: 0.08718685]) [Noise stdev: 0.01278774067113459]
Episode 1637/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1299 ; (Q Loss Avg: [Agent 0: 0.00036170, Agent 1: 0.00025759] ; E(V) Avg: [Agent 0: 0.14758236, Agent 1: 0.08680619]) [Noise stdev: 0.01276216518979232]
Episode 1638/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1299 ; (Q Loss Avg: [Agent 0: 0.00035946, Agent 1: 0.00026452] ; E(V) Avg: [Agent 0: 0.14767039, Agent 1: 0.08594549]) [Noise stdev: 0.012736640859412734]
Episode 1639/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1299 ; (Q Loss Avg: [Agent 0: 0.00038993, Agent 1: 0.00025382] ; E(V) Avg: [Agent 0: 0.14824706, Agent 1: 0.08466866]) [Noise stdev: 0.012711167577693909]
Episode 1640/5000 (29 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1298 ; (Q Loss Avg: [Agent 0: 0.00035130, Agent 1: 0.00023938] ; E(V) Avg: [Agent 0: 0.14858576, Agent 1: 0.08639446]) [Noise stdev: 0.01268574524253852]
Episode 1641/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1298 ; (Q Loss Avg: [Agent 0: 0.00036345, Agent 1: 0.00024556] ; E(V) Avg: [Agent 0: 0.14515621, Agent 1: 0.08579157]) [Noise stdev: 0.012660373752053443]
Episode 1642/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.1288 ; (Q Loss Avg: [Agent 0: 0.00038803, Agent 1: 0.00027183] ; E(V) Avg: [Agent 0: 0.14484784, Agent 1: 0.08399203]) [Noise stdev: 0.012635053004549335]
Episode 1643/5000 (131 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1308 ; (Q Loss Avg: [Agent 0: 0.00033915, Agent 1: 0.00024823] ; E(V) Avg: [Agent 0: 0.14690094, Agent 1: 0.08577856]) [Noise stdev: 0.012609782898540237]
Episode 1644/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1308 ; (Q Loss Avg: [Agent 0: 0.00033201, Agent 1: 0.00023708] ; E(V) Avg: [Agent 0: 0.14498795, Agent 1: 0.08489702]) [Noise stdev: 0.012584563332743156]
Episode 1645/5000 (31 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1307 ; (Q Loss Avg: [Agent 0: 0.00032989, Agent 1: 0.00023533] ; E(V) Avg: [Agent 0: 0.14527791, Agent 1: 0.08709014]) [Noise stdev: 0.01255939420607767]
Episode 1646/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1307 ; (Q Loss Avg: [Agent 0: 0.00032068, Agent 1: 0.00023146] ; E(V) Avg: [Agent 0: 0.14408450, Agent 1: 0.08420881]) [Noise stdev: 0.012534275417665514]
Episode 1647/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1287 ; (Q Loss Avg: [Agent 0: 0.00036553, Agent 1: 0.00025469] ; E(V) Avg: [Agent 0: 0.14542186, Agent 1: 0.08501600]) [Noise stdev: 0.012509206866830182]
Episode 1648/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1287 ; (Q Loss Avg: [Agent 0: 0.00033417, Agent 1: 0.00025429] ; E(V) Avg: [Agent 0: 0.14547901, Agent 1: 0.08539621]) [Noise stdev: 0.012484188453096522]
Episode 1649/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1287 ; (Q Loss Avg: [Agent 0: 0.00032734, Agent 1: 0.00026450] ; E(V) Avg: [Agent 0: 0.14378037, Agent 1: 0.08515459]) [Noise stdev: 0.01245922007619033]
Episode 1650/5000 (64 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1277 ; (Q Loss Avg: [Agent 0: 0.00035423, Agent 1: 0.00026615] ; E(V) Avg: [Agent 0: 0.14416082, Agent 1: 0.08517444]) [Noise stdev: 0.01243430163603795]
Episode 1651/5000 (101 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1287 ; (Q Loss Avg: [Agent 0: 0.00034672, Agent 1: 0.00025559] ; E(V) Avg: [Agent 0: 0.14440532, Agent 1: 0.08624098]) [Noise stdev: 0.012409433032765874]
Episode 1652/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1286 ; (Q Loss Avg: [Agent 0: 0.00035345, Agent 1: 0.00024729] ; E(V) Avg: [Agent 0: 0.14383813, Agent 1: 0.08495485]) [Noise stdev: 0.012384614166700343]
Episode 1653/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1276 ; (Q Loss Avg: [Agent 0: 0.00037289, Agent 1: 0.00025272] ; E(V) Avg: [Agent 0: 0.14409688, Agent 1: 0.08436049]) [Noise stdev: 0.012359844938366943]
Episode 1654/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1277 ; (Q Loss Avg: [Agent 0: 0.00038893, Agent 1: 0.00028193] ; E(V) Avg: [Agent 0: 0.14351502, Agent 1: 0.08528343]) [Noise stdev: 0.01233512524849021]
Episode 1655/5000 (224 steps): Episode reward: 0.5900 ; Average reward (last 100): 0.1326 ; (Q Loss Avg: [Agent 0: 0.00035002, Agent 1: 0.00025204] ; E(V) Avg: [Agent 0: 0.14244496, Agent 1: 0.08571757]) [Noise stdev: 0.01231045499799323]
Episode 1656/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1316 ; (Q Loss Avg: [Agent 0: 0.00036339, Agent 1: 0.00025761] ; E(V) Avg: [Agent 0: 0.14131317, Agent 1: 0.08530107]) [Noise stdev: 0.012285834087997244]
Episode 1657/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1316 ; (Q Loss Avg: [Agent 0: 0.00035158, Agent 1: 0.00024149] ; E(V) Avg: [Agent 0: 0.14113076, Agent 1: 0.08530077]) [Noise stdev: 0.01226126241982125]
Episode 1658/5000 (92 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1326 ; (Q Loss Avg: [Agent 0: 0.00034974, Agent 1: 0.00023250] ; E(V) Avg: [Agent 0: 0.14240857, Agent 1: 0.08500921]) [Noise stdev: 0.012236739894981606]
Episode 1659/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1325 ; (Q Loss Avg: [Agent 0: 0.00034486, Agent 1: 0.00022264] ; E(V) Avg: [Agent 0: 0.13912139, Agent 1: 0.08372387]) [Noise stdev: 0.012212266415191643]
Episode 1660/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1315 ; (Q Loss Avg: [Agent 0: 0.00040957, Agent 1: 0.00021891] ; E(V) Avg: [Agent 0: 0.14150376, Agent 1: 0.08409789]) [Noise stdev: 0.01218784188236126]
Episode 1661/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1315 ; (Q Loss Avg: [Agent 0: 0.00034397, Agent 1: 0.00022294] ; E(V) Avg: [Agent 0: 0.14145218, Agent 1: 0.08517970]) [Noise stdev: 0.012163466198596538]
Episode 1662/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1315 ; (Q Loss Avg: [Agent 0: 0.00034475, Agent 1: 0.00022934] ; E(V) Avg: [Agent 0: 0.13935656, Agent 1: 0.08598506]) [Noise stdev: 0.012139139266199346]
Episode 1663/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1315 ; (Q Loss Avg: [Agent 0: 0.00034106, Agent 1: 0.00024535] ; E(V) Avg: [Agent 0: 0.14118423, Agent 1: 0.08450384]) [Noise stdev: 0.012114860987666946]
Episode 1664/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1305 ; (Q Loss Avg: [Agent 0: 0.00031573, Agent 1: 0.00023934] ; E(V) Avg: [Agent 0: 0.13951933, Agent 1: 0.08728997]) [Noise stdev: 0.012090631265691612]
Episode 1665/5000 (108 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1325 ; (Q Loss Avg: [Agent 0: 0.00032740, Agent 1: 0.00024258] ; E(V) Avg: [Agent 0: 0.14045690, Agent 1: 0.08626842]) [Noise stdev: 0.012066450003160228]
Episode 1666/5000 (108 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1345 ; (Q Loss Avg: [Agent 0: 0.00032807, Agent 1: 0.00024338] ; E(V) Avg: [Agent 0: 0.13956419, Agent 1: 0.08493913]) [Noise stdev: 0.012042317103153908]
Episode 1667/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1345 ; (Q Loss Avg: [Agent 0: 0.00032122, Agent 1: 0.00023224] ; E(V) Avg: [Agent 0: 0.13884498, Agent 1: 0.08406899]) [Noise stdev: 0.0120182324689476]
Episode 1668/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1335 ; (Q Loss Avg: [Agent 0: 0.00035689, Agent 1: 0.00024004] ; E(V) Avg: [Agent 0: 0.13998325, Agent 1: 0.08384611]) [Noise stdev: 0.011994196004009704]
Episode 1669/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1335 ; (Q Loss Avg: [Agent 0: 0.00033722, Agent 1: 0.00023418] ; E(V) Avg: [Agent 0: 0.13973224, Agent 1: 0.08372158]) [Noise stdev: 0.011970207612001686]
Episode 1670/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1325 ; (Q Loss Avg: [Agent 0: 0.00032179, Agent 1: 0.00021583] ; E(V) Avg: [Agent 0: 0.13751755, Agent 1: 0.08466801]) [Noise stdev: 0.011946267196777683]
Episode 1671/5000 (90 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1325 ; (Q Loss Avg: [Agent 0: 0.00032629, Agent 1: 0.00022891] ; E(V) Avg: [Agent 0: 0.13838387, Agent 1: 0.08428684]) [Noise stdev: 0.011922374662384128]
Episode 1672/5000 (68 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1325 ; (Q Loss Avg: [Agent 0: 0.00034070, Agent 1: 0.00022957] ; E(V) Avg: [Agent 0: 0.13785381, Agent 1: 0.08518959]) [Noise stdev: 0.01189852991305936]
Episode 1673/5000 (67 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1335 ; (Q Loss Avg: [Agent 0: 0.00032186, Agent 1: 0.00024421] ; E(V) Avg: [Agent 0: 0.13762801, Agent 1: 0.08432645]) [Noise stdev: 0.011874732853233242]
Episode 1674/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1335 ; (Q Loss Avg: [Agent 0: 0.00032430, Agent 1: 0.00023601] ; E(V) Avg: [Agent 0: 0.13670992, Agent 1: 0.08493292]) [Noise stdev: 0.011850983387526775]
Episode 1675/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1335 ; (Q Loss Avg: [Agent 0: 0.00033293, Agent 1: 0.00024798] ; E(V) Avg: [Agent 0: 0.13728310, Agent 1: 0.08405843]) [Noise stdev: 0.011827281420751722]
Episode 1676/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1314 ; (Q Loss Avg: [Agent 0: 0.00030362, Agent 1: 0.00024023] ; E(V) Avg: [Agent 0: 0.13648030, Agent 1: 0.08360983]) [Noise stdev: 0.011803626857910218]
Episode 1677/5000 (68 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1324 ; (Q Loss Avg: [Agent 0: 0.00034231, Agent 1: 0.00023168] ; E(V) Avg: [Agent 0: 0.13703838, Agent 1: 0.08419642]) [Noise stdev: 0.011780019604194397]
Episode 1678/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1324 ; (Q Loss Avg: [Agent 0: 0.00035539, Agent 1: 0.00023676] ; E(V) Avg: [Agent 0: 0.13586362, Agent 1: 0.08544322]) [Noise stdev: 0.011756459564986008]
Episode 1679/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1324 ; (Q Loss Avg: [Agent 0: 0.00034507, Agent 1: 0.00025056] ; E(V) Avg: [Agent 0: 0.13728423, Agent 1: 0.08407964]) [Noise stdev: 0.011732946645856036]
Episode 1680/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1314 ; (Q Loss Avg: [Agent 0: 0.00034826, Agent 1: 0.00022816] ; E(V) Avg: [Agent 0: 0.13646104, Agent 1: 0.08423340]) [Noise stdev: 0.011709480752564325]
Episode 1681/5000 (69 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1314 ; (Q Loss Avg: [Agent 0: 0.00035231, Agent 1: 0.00022417] ; E(V) Avg: [Agent 0: 0.13798536, Agent 1: 0.08409565]) [Noise stdev: 0.011686061791059197]
Episode 1682/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1314 ; (Q Loss Avg: [Agent 0: 0.00034155, Agent 1: 0.00023331] ; E(V) Avg: [Agent 0: 0.13701543, Agent 1: 0.08414994]) [Noise stdev: 0.011662689667477077]
Episode 1683/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1314 ; (Q Loss Avg: [Agent 0: 0.00031822, Agent 1: 0.00025157] ; E(V) Avg: [Agent 0: 0.13719363, Agent 1: 0.08318673]) [Noise stdev: 0.011639364288142124]
Episode 1684/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1304 ; (Q Loss Avg: [Agent 0: 0.00032119, Agent 1: 0.00023309] ; E(V) Avg: [Agent 0: 0.13590278, Agent 1: 0.08528508]) [Noise stdev: 0.01161608555956584]
Episode 1685/5000 (168 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.1334 ; (Q Loss Avg: [Agent 0: 0.00031304, Agent 1: 0.00022151] ; E(V) Avg: [Agent 0: 0.13530815, Agent 1: 0.08418615]) [Noise stdev: 0.011592853388446707]
Episode 1686/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1334 ; (Q Loss Avg: [Agent 0: 0.00032233, Agent 1: 0.00021513] ; E(V) Avg: [Agent 0: 0.13453342, Agent 1: 0.08327746]) [Noise stdev: 0.011569667681669813]
Episode 1687/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1344 ; (Q Loss Avg: [Agent 0: 0.00031245, Agent 1: 0.00021834] ; E(V) Avg: [Agent 0: 0.13456638, Agent 1: 0.08381845]) [Noise stdev: 0.011546528346306473]
Episode 1688/5000 (224 steps): Episode reward: 0.5900 ; Average reward (last 100): 0.1383 ; (Q Loss Avg: [Agent 0: 0.00032299, Agent 1: 0.00022720] ; E(V) Avg: [Agent 0: 0.13482772, Agent 1: 0.08433310]) [Noise stdev: 0.011523435289613861]
Episode 1689/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1374 ; (Q Loss Avg: [Agent 0: 0.00030007, Agent 1: 0.00023899] ; E(V) Avg: [Agent 0: 0.13467294, Agent 1: 0.08461625]) [Noise stdev: 0.011500388419034634]
Episode 1690/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1374 ; (Q Loss Avg: [Agent 0: 0.00029565, Agent 1: 0.00024631] ; E(V) Avg: [Agent 0: 0.13293250, Agent 1: 0.08368604]) [Noise stdev: 0.011477387642196564]
Episode 1691/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1374 ; (Q Loss Avg: [Agent 0: 0.00029365, Agent 1: 0.00021535] ; E(V) Avg: [Agent 0: 0.13486385, Agent 1: 0.08338937]) [Noise stdev: 0.011454432866912171]
Episode 1692/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1373 ; (Q Loss Avg: [Agent 0: 0.00031525, Agent 1: 0.00022673] ; E(V) Avg: [Agent 0: 0.13341915, Agent 1: 0.08462610]) [Noise stdev: 0.011431524001178346]
Episode 1693/5000 (70 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1382 ; (Q Loss Avg: [Agent 0: 0.00028763, Agent 1: 0.00023057] ; E(V) Avg: [Agent 0: 0.13336898, Agent 1: 0.08354833]) [Noise stdev: 0.011408660953175989]
Episode 1694/5000 (69 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1391 ; (Q Loss Avg: [Agent 0: 0.00030538, Agent 1: 0.00024063] ; E(V) Avg: [Agent 0: 0.13237224, Agent 1: 0.08365820]) [Noise stdev: 0.011385843631269637]
Episode 1695/5000 (80 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1401 ; (Q Loss Avg: [Agent 0: 0.00029798, Agent 1: 0.00022145] ; E(V) Avg: [Agent 0: 0.13214165, Agent 1: 0.08339057]) [Noise stdev: 0.011363071944007097]
Episode 1696/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1401 ; (Q Loss Avg: [Agent 0: 0.00029372, Agent 1: 0.00020177] ; E(V) Avg: [Agent 0: 0.13366892, Agent 1: 0.08424779]) [Noise stdev: 0.011340345800119082]
Episode 1697/5000 (68 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1400 ; (Q Loss Avg: [Agent 0: 0.00033567, Agent 1: 0.00022203] ; E(V) Avg: [Agent 0: 0.13227538, Agent 1: 0.08247584]) [Noise stdev: 0.011317665108518844]
Episode 1698/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1401 ; (Q Loss Avg: [Agent 0: 0.00029664, Agent 1: 0.00022496] ; E(V) Avg: [Agent 0: 0.13335237, Agent 1: 0.08289890]) [Noise stdev: 0.011295029778301807]
Episode 1699/5000 (34 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1401 ; (Q Loss Avg: [Agent 0: 0.00027589, Agent 1: 0.00021796] ; E(V) Avg: [Agent 0: 0.13354493, Agent 1: 0.08450818]) [Noise stdev: 0.011272439718745203]
Episode 1700/5000 (92 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1411 ; (Q Loss Avg: [Agent 0: 0.00031353, Agent 1: 0.00020630] ; E(V) Avg: [Agent 0: 0.13293661, Agent 1: 0.08407866]) [Noise stdev: 0.011249894839307712]
Episode 1701/5000 (71 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1420 ; (Q Loss Avg: [Agent 0: 0.00029995, Agent 1: 0.00021663] ; E(V) Avg: [Agent 0: 0.13310485, Agent 1: 0.08422152]) [Noise stdev: 0.011227395049629097]
Episode 1702/5000 (131 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1450 ; (Q Loss Avg: [Agent 0: 0.00030632, Agent 1: 0.00021760] ; E(V) Avg: [Agent 0: 0.13258858, Agent 1: 0.08366899]) [Noise stdev: 0.011204940259529839]
Episode 1703/5000 (68 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1449 ; (Q Loss Avg: [Agent 0: 0.00031437, Agent 1: 0.00020532] ; E(V) Avg: [Agent 0: 0.13101132, Agent 1: 0.08370287]) [Noise stdev: 0.011182530379010779]
Episode 1704/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1449 ; (Q Loss Avg: [Agent 0: 0.00028576, Agent 1: 0.00019540] ; E(V) Avg: [Agent 0: 0.13254879, Agent 1: 0.08349308]) [Noise stdev: 0.011160165318252757]
Episode 1705/5000 (69 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1458 ; (Q Loss Avg: [Agent 0: 0.00028761, Agent 1: 0.00023093] ; E(V) Avg: [Agent 0: 0.13282585, Agent 1: 0.08313842]) [Noise stdev: 0.011137844987616252]
Episode 1706/5000 (67 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1469 ; (Q Loss Avg: [Agent 0: 0.00029568, Agent 1: 0.00020902] ; E(V) Avg: [Agent 0: 0.13168453, Agent 1: 0.08238317]) [Noise stdev: 0.01111556929764102]
Episode 1707/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1469 ; (Q Loss Avg: [Agent 0: 0.00027567, Agent 1: 0.00021355] ; E(V) Avg: [Agent 0: 0.13134348, Agent 1: 0.08398926]) [Noise stdev: 0.011093338159045738]
Episode 1708/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1469 ; (Q Loss Avg: [Agent 0: 0.00027629, Agent 1: 0.00023311] ; E(V) Avg: [Agent 0: 0.13118948, Agent 1: 0.08319913]) [Noise stdev: 0.011071151482727646]
Episode 1709/5000 (92 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1479 ; (Q Loss Avg: [Agent 0: 0.00029385, Agent 1: 0.00020753] ; E(V) Avg: [Agent 0: 0.13144783, Agent 1: 0.08265937]) [Noise stdev: 0.01104900917976219]
Episode 1710/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1479 ; (Q Loss Avg: [Agent 0: 0.00030547, Agent 1: 0.00020950] ; E(V) Avg: [Agent 0: 0.13255964, Agent 1: 0.08343491]) [Noise stdev: 0.011026911161402666]
Episode 1711/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1479 ; (Q Loss Avg: [Agent 0: 0.00027796, Agent 1: 0.00019155] ; E(V) Avg: [Agent 0: 0.13041821, Agent 1: 0.08331907]) [Noise stdev: 0.011004857339079861]
Episode 1712/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1479 ; (Q Loss Avg: [Agent 0: 0.00029886, Agent 1: 0.00019582] ; E(V) Avg: [Agent 0: 0.13183374, Agent 1: 0.08272929]) [Noise stdev: 0.010982847624401701]
Episode 1713/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1478 ; (Q Loss Avg: [Agent 0: 0.00029556, Agent 1: 0.00023308] ; E(V) Avg: [Agent 0: 0.13146783, Agent 1: 0.08328006]) [Noise stdev: 0.010960881929152897]
Episode 1714/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1468 ; (Q Loss Avg: [Agent 0: 0.00031251, Agent 1: 0.00022907] ; E(V) Avg: [Agent 0: 0.12930771, Agent 1: 0.08270254]) [Noise stdev: 0.010938960165294592]
Episode 1715/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1438 ; (Q Loss Avg: [Agent 0: 0.00028897, Agent 1: 0.00020874] ; E(V) Avg: [Agent 0: 0.13062737, Agent 1: 0.08313781]) [Noise stdev: 0.010917082244964002]
Episode 1716/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1438 ; (Q Loss Avg: [Agent 0: 0.00027702, Agent 1: 0.00021311] ; E(V) Avg: [Agent 0: 0.13073709, Agent 1: 0.08208358]) [Noise stdev: 0.010895248080474073]
Episode 1717/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1438 ; (Q Loss Avg: [Agent 0: 0.00032531, Agent 1: 0.00020139] ; E(V) Avg: [Agent 0: 0.13152443, Agent 1: 0.08207165]) [Noise stdev: 0.010873457584313125]
Episode 1718/5000 (71 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1438 ; (Q Loss Avg: [Agent 0: 0.00031029, Agent 1: 0.00020328] ; E(V) Avg: [Agent 0: 0.12968336, Agent 1: 0.08411387]) [Noise stdev: 0.010851710669144499]
Episode 1719/5000 (109 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1458 ; (Q Loss Avg: [Agent 0: 0.00029232, Agent 1: 0.00021221] ; E(V) Avg: [Agent 0: 0.13005545, Agent 1: 0.08284057]) [Noise stdev: 0.01083000724780621]
Episode 1720/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1458 ; (Q Loss Avg: [Agent 0: 0.00026918, Agent 1: 0.00019820] ; E(V) Avg: [Agent 0: 0.12935881, Agent 1: 0.08278487]) [Noise stdev: 0.010808347233310598]
Episode 1721/5000 (41 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1458 ; (Q Loss Avg: [Agent 0: 0.00029095, Agent 1: 0.00019607] ; E(V) Avg: [Agent 0: 0.13062850, Agent 1: 0.08335784]) [Noise stdev: 0.010786730538843976]
Episode 1722/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1457 ; (Q Loss Avg: [Agent 0: 0.00029180, Agent 1: 0.00018894] ; E(V) Avg: [Agent 0: 0.13047210, Agent 1: 0.08333268]) [Noise stdev: 0.010765157077766287]
Episode 1723/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1456 ; (Q Loss Avg: [Agent 0: 0.00029454, Agent 1: 0.00018963] ; E(V) Avg: [Agent 0: 0.13125610, Agent 1: 0.08261600]) [Noise stdev: 0.010743626763610755]
Episode 1724/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1456 ; (Q Loss Avg: [Agent 0: 0.00028979, Agent 1: 0.00019912] ; E(V) Avg: [Agent 0: 0.13054814, Agent 1: 0.08315954]) [Noise stdev: 0.010722139510083533]
Episode 1725/5000 (45 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1456 ; (Q Loss Avg: [Agent 0: 0.00028421, Agent 1: 0.00021256] ; E(V) Avg: [Agent 0: 0.12897592, Agent 1: 0.08343545]) [Noise stdev: 0.010700695231063366]
Episode 1726/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1425 ; (Q Loss Avg: [Agent 0: 0.00032519, Agent 1: 0.00025008] ; E(V) Avg: [Agent 0: 0.12827464, Agent 1: 0.08312130]) [Noise stdev: 0.010679293840601239]
Episode 1727/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1426 ; (Q Loss Avg: [Agent 0: 0.00029788, Agent 1: 0.00020612] ; E(V) Avg: [Agent 0: 0.13067133, Agent 1: 0.08139115]) [Noise stdev: 0.010657935252920036]
Episode 1728/5000 (68 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1436 ; (Q Loss Avg: [Agent 0: 0.00027435, Agent 1: 0.00022837] ; E(V) Avg: [Agent 0: 0.12911042, Agent 1: 0.08371721]) [Noise stdev: 0.010636619382414196]
Episode 1729/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1437 ; (Q Loss Avg: [Agent 0: 0.00029654, Agent 1: 0.00022678] ; E(V) Avg: [Agent 0: 0.12969883, Agent 1: 0.08351521]) [Noise stdev: 0.010615346143649368]
Episode 1730/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1437 ; (Q Loss Avg: [Agent 0: 0.00027449, Agent 1: 0.00017902] ; E(V) Avg: [Agent 0: 0.12896949, Agent 1: 0.08360291]) [Noise stdev: 0.010594115451362069]
Episode 1731/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1436 ; (Q Loss Avg: [Agent 0: 0.00030035, Agent 1: 0.00022547] ; E(V) Avg: [Agent 0: 0.13014676, Agent 1: 0.08349995]) [Noise stdev: 0.010572927220459345]
Episode 1732/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1437 ; (Q Loss Avg: [Agent 0: 0.00025698, Agent 1: 0.00019538] ; E(V) Avg: [Agent 0: 0.12875393, Agent 1: 0.08215899]) [Noise stdev: 0.010551781366018425]
Episode 1733/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1436 ; (Q Loss Avg: [Agent 0: 0.00025410, Agent 1: 0.00021077] ; E(V) Avg: [Agent 0: 0.12936770, Agent 1: 0.08315736]) [Noise stdev: 0.01053067780328639]
Episode 1734/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1436 ; (Q Loss Avg: [Agent 0: 0.00025966, Agent 1: 0.00020470] ; E(V) Avg: [Agent 0: 0.12963762, Agent 1: 0.08231953]) [Noise stdev: 0.010509616447679816]
Episode 1735/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1437 ; (Q Loss Avg: [Agent 0: 0.00027338, Agent 1: 0.00017591] ; E(V) Avg: [Agent 0: 0.12850384, Agent 1: 0.08335021]) [Noise stdev: 0.010488597214784456]
Episode 1736/5000 (57 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1427 ; (Q Loss Avg: [Agent 0: 0.00028935, Agent 1: 0.00019525] ; E(V) Avg: [Agent 0: 0.12932903, Agent 1: 0.08345784]) [Noise stdev: 0.010467620020354888]
Episode 1737/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1427 ; (Q Loss Avg: [Agent 0: 0.00026874, Agent 1: 0.00019858] ; E(V) Avg: [Agent 0: 0.12840203, Agent 1: 0.08240745]) [Noise stdev: 0.010446684780314179]
Episode 1738/5000 (35 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1417 ; (Q Loss Avg: [Agent 0: 0.00027683, Agent 1: 0.00021119] ; E(V) Avg: [Agent 0: 0.12856471, Agent 1: 0.08291397]) [Noise stdev: 0.010425791410753551]
Episode 1739/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1427 ; (Q Loss Avg: [Agent 0: 0.00028200, Agent 1: 0.00020067] ; E(V) Avg: [Agent 0: 0.12869383, Agent 1: 0.08292197]) [Noise stdev: 0.010404939827932044]
Episode 1740/5000 (68 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1438 ; (Q Loss Avg: [Agent 0: 0.00028578, Agent 1: 0.00021530] ; E(V) Avg: [Agent 0: 0.12925529, Agent 1: 0.08299305]) [Noise stdev: 0.01038412994827618]
Episode 1741/5000 (57 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1438 ; (Q Loss Avg: [Agent 0: 0.00029961, Agent 1: 0.00019413] ; E(V) Avg: [Agent 0: 0.12837853, Agent 1: 0.08353369]) [Noise stdev: 0.010363361688379627]
Episode 1742/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1447 ; (Q Loss Avg: [Agent 0: 0.00029298, Agent 1: 0.00019080] ; E(V) Avg: [Agent 0: 0.12890136, Agent 1: 0.08176226]) [Noise stdev: 0.010342634965002867]
Episode 1743/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1427 ; (Q Loss Avg: [Agent 0: 0.00029417, Agent 1: 0.00019363] ; E(V) Avg: [Agent 0: 0.12930350, Agent 1: 0.08313889]) [Noise stdev: 0.01032194969507286]
Episode 1744/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1427 ; (Q Loss Avg: [Agent 0: 0.00029054, Agent 1: 0.00018099] ; E(V) Avg: [Agent 0: 0.12922981, Agent 1: 0.08091187]) [Noise stdev: 0.010301305795682714]
Episode 1745/5000 (71 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1438 ; (Q Loss Avg: [Agent 0: 0.00028948, Agent 1: 0.00020525] ; E(V) Avg: [Agent 0: 0.12838592, Agent 1: 0.08200587]) [Noise stdev: 0.010280703184091348]
Episode 1746/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1438 ; (Q Loss Avg: [Agent 0: 0.00030053, Agent 1: 0.00019348] ; E(V) Avg: [Agent 0: 0.12833505, Agent 1: 0.08190340]) [Noise stdev: 0.010260141777723166]
Episode 1747/5000 (93 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1448 ; (Q Loss Avg: [Agent 0: 0.00031808, Agent 1: 0.00021748] ; E(V) Avg: [Agent 0: 0.12884280, Agent 1: 0.08262925]) [Noise stdev: 0.01023962149416772]
Episode 1748/5000 (30 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1447 ; (Q Loss Avg: [Agent 0: 0.00027375, Agent 1: 0.00021543] ; E(V) Avg: [Agent 0: 0.12854200, Agent 1: 0.08347183]) [Noise stdev: 0.010219142251179385]
Episode 1749/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1457 ; (Q Loss Avg: [Agent 0: 0.00030503, Agent 1: 0.00018836] ; E(V) Avg: [Agent 0: 0.12784735, Agent 1: 0.08259889]) [Noise stdev: 0.010198703966677027]
Episode 1750/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1457 ; (Q Loss Avg: [Agent 0: 0.00028023, Agent 1: 0.00020938] ; E(V) Avg: [Agent 0: 0.12860109, Agent 1: 0.08240813]) [Noise stdev: 0.010178306558743673]
Episode 1751/5000 (68 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1456 ; (Q Loss Avg: [Agent 0: 0.00029902, Agent 1: 0.00021616] ; E(V) Avg: [Agent 0: 0.12925929, Agent 1: 0.08326543]) [Noise stdev: 0.010157949945626184]
Episode 1752/5000 (112 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1477 ; (Q Loss Avg: [Agent 0: 0.00027861, Agent 1: 0.00020659] ; E(V) Avg: [Agent 0: 0.12846881, Agent 1: 0.08339470]) [Noise stdev: 0.010137634045734933]
Episode 1753/5000 (71 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1487 ; (Q Loss Avg: [Agent 0: 0.00029465, Agent 1: 0.00022458] ; E(V) Avg: [Agent 0: 0.12831859, Agent 1: 0.08277797]) [Noise stdev: 0.010117358777643463]
Episode 1754/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1487 ; (Q Loss Avg: [Agent 0: 0.00029864, Agent 1: 0.00019857] ; E(V) Avg: [Agent 0: 0.12843933, Agent 1: 0.08292778]) [Noise stdev: 0.010097124060088176]
Episode 1755/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.1428 ; (Q Loss Avg: [Agent 0: 0.00025362, Agent 1: 0.00022553] ; E(V) Avg: [Agent 0: 0.12659934, Agent 1: 0.08415760]) [Noise stdev: 0.010076929811968]
Episode 1756/5000 (92 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1438 ; (Q Loss Avg: [Agent 0: 0.00027160, Agent 1: 0.00022013] ; E(V) Avg: [Agent 0: 0.12824479, Agent 1: 0.08362929]) [Noise stdev: 0.010056775952344063]
Episode 1757/5000 (72 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1448 ; (Q Loss Avg: [Agent 0: 0.00027773, Agent 1: 0.00021286] ; E(V) Avg: [Agent 0: 0.12799621, Agent 1: 0.08286451]) [Noise stdev: 0.010036662400439376]
Episode 1758/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1438 ; (Q Loss Avg: [Agent 0: 0.00028921, Agent 1: 0.00021082] ; E(V) Avg: [Agent 0: 0.12728202, Agent 1: 0.08170778]) [Noise stdev: 0.010016589075638497]
Episode 1759/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1439 ; (Q Loss Avg: [Agent 0: 0.00027460, Agent 1: 0.00021823] ; E(V) Avg: [Agent 0: 0.12849779, Agent 1: 0.08222045]) [Noise stdev: 0.009996555897487219]
Episode 1760/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1439 ; (Q Loss Avg: [Agent 0: 0.00027325, Agent 1: 0.00021405] ; E(V) Avg: [Agent 0: 0.12882465, Agent 1: 0.08262680]) [Noise stdev: 0.009976562785692245]
Episode 1761/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1439 ; (Q Loss Avg: [Agent 0: 0.00028044, Agent 1: 0.00019087] ; E(V) Avg: [Agent 0: 0.12815344, Agent 1: 0.08291576]) [Noise stdev: 0.00995660966012086]
Episode 1762/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1439 ; (Q Loss Avg: [Agent 0: 0.00028272, Agent 1: 0.00017408] ; E(V) Avg: [Agent 0: 0.12644928, Agent 1: 0.08257018]) [Noise stdev: 0.009936696440800618]
Episode 1763/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1439 ; (Q Loss Avg: [Agent 0: 0.00027324, Agent 1: 0.00021092] ; E(V) Avg: [Agent 0: 0.12781146, Agent 1: 0.08392543]) [Noise stdev: 0.009916823047919018]
Episode 1764/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1439 ; (Q Loss Avg: [Agent 0: 0.00028791, Agent 1: 0.00020269] ; E(V) Avg: [Agent 0: 0.12722865, Agent 1: 0.08333224]) [Noise stdev: 0.009896989401823179]
Episode 1765/5000 (71 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1428 ; (Q Loss Avg: [Agent 0: 0.00026913, Agent 1: 0.00021738] ; E(V) Avg: [Agent 0: 0.12778161, Agent 1: 0.08391940]) [Noise stdev: 0.009877195423019533]
Episode 1766/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1408 ; (Q Loss Avg: [Agent 0: 0.00029089, Agent 1: 0.00018750] ; E(V) Avg: [Agent 0: 0.12803843, Agent 1: 0.08305291]) [Noise stdev: 0.009857441032173493]
Episode 1767/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1408 ; (Q Loss Avg: [Agent 0: 0.00029440, Agent 1: 0.00022640] ; E(V) Avg: [Agent 0: 0.12801157, Agent 1: 0.08402414]) [Noise stdev: 0.009837726150109145]
Episode 1768/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1408 ; (Q Loss Avg: [Agent 0: 0.00032012, Agent 1: 0.00019837] ; E(V) Avg: [Agent 0: 0.12862916, Agent 1: 0.08410470]) [Noise stdev: 0.009818050697808927]
Episode 1769/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1408 ; (Q Loss Avg: [Agent 0: 0.00027973, Agent 1: 0.00020274] ; E(V) Avg: [Agent 0: 0.12788443, Agent 1: 0.08246915]) [Noise stdev: 0.00979841459641331]
Episode 1770/5000 (42 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1408 ; (Q Loss Avg: [Agent 0: 0.00027622, Agent 1: 0.00023743] ; E(V) Avg: [Agent 0: 0.12628771, Agent 1: 0.08314112]) [Noise stdev: 0.009778817767220482]
Episode 1771/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1398 ; (Q Loss Avg: [Agent 0: 0.00028041, Agent 1: 0.00020353] ; E(V) Avg: [Agent 0: 0.12743217, Agent 1: 0.08342855]) [Noise stdev: 0.009759260131686041]
Episode 1772/5000 (32 steps): Episode reward: 0.0900 ; Average reward (last 100): 0.1387 ; (Q Loss Avg: [Agent 0: 0.00026819, Agent 1: 0.00022705] ; E(V) Avg: [Agent 0: 0.12764804, Agent 1: 0.08308969]) [Noise stdev: 0.009739741611422669]
Episode 1773/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1377 ; (Q Loss Avg: [Agent 0: 0.00028519, Agent 1: 0.00022127] ; E(V) Avg: [Agent 0: 0.12809464, Agent 1: 0.08266444]) [Noise stdev: 0.009720262128199824]
Episode 1774/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1377 ; (Q Loss Avg: [Agent 0: 0.00028042, Agent 1: 0.00022609] ; E(V) Avg: [Agent 0: 0.12762746, Agent 1: 0.08243601]) [Noise stdev: 0.009700821603943425]
Episode 1775/5000 (67 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1386 ; (Q Loss Avg: [Agent 0: 0.00025385, Agent 1: 0.00020836] ; E(V) Avg: [Agent 0: 0.12749685, Agent 1: 0.08329125]) [Noise stdev: 0.009681419960735538]
Episode 1776/5000 (72 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1397 ; (Q Loss Avg: [Agent 0: 0.00028659, Agent 1: 0.00022082] ; E(V) Avg: [Agent 0: 0.12721892, Agent 1: 0.08322536]) [Noise stdev: 0.009662057120814067]
Episode 1777/5000 (111 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1407 ; (Q Loss Avg: [Agent 0: 0.00029839, Agent 1: 0.00019787] ; E(V) Avg: [Agent 0: 0.12809343, Agent 1: 0.08312584]) [Noise stdev: 0.00964273300657244]
Episode 1778/5000 (112 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1427 ; (Q Loss Avg: [Agent 0: 0.00027042, Agent 1: 0.00020898] ; E(V) Avg: [Agent 0: 0.12841050, Agent 1: 0.08286779]) [Noise stdev: 0.009623447540559294]
Episode 1779/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1427 ; (Q Loss Avg: [Agent 0: 0.00027115, Agent 1: 0.00019290] ; E(V) Avg: [Agent 0: 0.12764238, Agent 1: 0.08308746]) [Noise stdev: 0.009604200645478175]
Episode 1780/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1427 ; (Q Loss Avg: [Agent 0: 0.00026626, Agent 1: 0.00019192] ; E(V) Avg: [Agent 0: 0.12799972, Agent 1: 0.08209848]) [Noise stdev: 0.009584992244187218]
Episode 1781/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1418 ; (Q Loss Avg: [Agent 0: 0.00029221, Agent 1: 0.00019174] ; E(V) Avg: [Agent 0: 0.12740227, Agent 1: 0.08277000]) [Noise stdev: 0.009565822259698844]
Episode 1782/5000 (54 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1418 ; (Q Loss Avg: [Agent 0: 0.00029011, Agent 1: 0.00018952] ; E(V) Avg: [Agent 0: 0.12680792, Agent 1: 0.08406102]) [Noise stdev: 0.009546690615179446]
Episode 1783/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1418 ; (Q Loss Avg: [Agent 0: 0.00028818, Agent 1: 0.00019536] ; E(V) Avg: [Agent 0: 0.12632866, Agent 1: 0.08293547]) [Noise stdev: 0.009527597233949086]
Episode 1784/5000 (150 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.1448 ; (Q Loss Avg: [Agent 0: 0.00026505, Agent 1: 0.00019651] ; E(V) Avg: [Agent 0: 0.12686223, Agent 1: 0.08407239]) [Noise stdev: 0.009508542039481187]
Episode 1785/5000 (68 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.1427 ; (Q Loss Avg: [Agent 0: 0.00029323, Agent 1: 0.00018609] ; E(V) Avg: [Agent 0: 0.12697469, Agent 1: 0.08365256]) [Noise stdev: 0.009489524955402225]
Episode 1786/5000 (73 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1437 ; (Q Loss Avg: [Agent 0: 0.00027546, Agent 1: 0.00021299] ; E(V) Avg: [Agent 0: 0.12688137, Agent 1: 0.08323599]) [Noise stdev: 0.00947054590549142]
Episode 1787/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1437 ; (Q Loss Avg: [Agent 0: 0.00027103, Agent 1: 0.00018544] ; E(V) Avg: [Agent 0: 0.12713113, Agent 1: 0.08351974]) [Noise stdev: 0.009451604813680438]
Episode 1788/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1388 ; (Q Loss Avg: [Agent 0: 0.00029760, Agent 1: 0.00020862] ; E(V) Avg: [Agent 0: 0.12740697, Agent 1: 0.08350031]) [Noise stdev: 0.009432701604053077]
Episode 1789/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1388 ; (Q Loss Avg: [Agent 0: 0.00026652, Agent 1: 0.00023032] ; E(V) Avg: [Agent 0: 0.12731089, Agent 1: 0.08339925]) [Noise stdev: 0.009413836200844971]
Episode 1790/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1388 ; (Q Loss Avg: [Agent 0: 0.00027180, Agent 1: 0.00020164] ; E(V) Avg: [Agent 0: 0.12566515, Agent 1: 0.08457698]) [Noise stdev: 0.00939500852844328]
Episode 1791/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1388 ; (Q Loss Avg: [Agent 0: 0.00028204, Agent 1: 0.00020083] ; E(V) Avg: [Agent 0: 0.12713040, Agent 1: 0.08286361]) [Noise stdev: 0.009376218511386394]
Episode 1792/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1389 ; (Q Loss Avg: [Agent 0: 0.00028373, Agent 1: 0.00018793] ; E(V) Avg: [Agent 0: 0.12710153, Agent 1: 0.08404557]) [Noise stdev: 0.009357466074363621]
Episode 1793/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1380 ; (Q Loss Avg: [Agent 0: 0.00028339, Agent 1: 0.00022095] ; E(V) Avg: [Agent 0: 0.12683634, Agent 1: 0.08434163]) [Noise stdev: 0.009338751142214894]
Episode 1794/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1371 ; (Q Loss Avg: [Agent 0: 0.00023852, Agent 1: 0.00020307] ; E(V) Avg: [Agent 0: 0.12802068, Agent 1: 0.08259973]) [Noise stdev: 0.009320073639930464]
Episode 1795/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1361 ; (Q Loss Avg: [Agent 0: 0.00027805, Agent 1: 0.00020821] ; E(V) Avg: [Agent 0: 0.12787528, Agent 1: 0.08274694]) [Noise stdev: 0.009301433492650604]
Episode 1796/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1361 ; (Q Loss Avg: [Agent 0: 0.00028572, Agent 1: 0.00021705] ; E(V) Avg: [Agent 0: 0.12731696, Agent 1: 0.08256656]) [Noise stdev: 0.009282830625665303]
Episode 1797/5000 (72 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1362 ; (Q Loss Avg: [Agent 0: 0.00026815, Agent 1: 0.00021618] ; E(V) Avg: [Agent 0: 0.12748739, Agent 1: 0.08298661]) [Noise stdev: 0.009264264964413972]
Episode 1798/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1362 ; (Q Loss Avg: [Agent 0: 0.00027579, Agent 1: 0.00022084] ; E(V) Avg: [Agent 0: 0.12745910, Agent 1: 0.08389641]) [Noise stdev: 0.009245736434485143]
Episode 1799/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1362 ; (Q Loss Avg: [Agent 0: 0.00027884, Agent 1: 0.00021365] ; E(V) Avg: [Agent 0: 0.12676120, Agent 1: 0.08392336]) [Noise stdev: 0.009227244961616173]
Episode 1800/5000 (56 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1352 ; (Q Loss Avg: [Agent 0: 0.00027368, Agent 1: 0.00020855] ; E(V) Avg: [Agent 0: 0.12646030, Agent 1: 0.08318948]) [Noise stdev: 0.009208790471692941]
Episode 1801/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.1333 ; (Q Loss Avg: [Agent 0: 0.00026538, Agent 1: 0.00019815] ; E(V) Avg: [Agent 0: 0.12579122, Agent 1: 0.08459821]) [Noise stdev: 0.009190372890749556]
Episode 1802/5000 (139 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1333 ; (Q Loss Avg: [Agent 0: 0.00027278, Agent 1: 0.00021925] ; E(V) Avg: [Agent 0: 0.12711556, Agent 1: 0.08382537]) [Noise stdev: 0.009171992144968057]
Episode 1803/5000 (15 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.1314 ; (Q Loss Avg: [Agent 0: 0.00027362, Agent 1: 0.00021247] ; E(V) Avg: [Agent 0: 0.12589932, Agent 1: 0.08166047]) [Noise stdev: 0.00915364816067812]
Episode 1804/5000 (51 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1314 ; (Q Loss Avg: [Agent 0: 0.00031072, Agent 1: 0.00019916] ; E(V) Avg: [Agent 0: 0.12627762, Agent 1: 0.08376357]) [Noise stdev: 0.009135340864356764]
Episode 1805/5000 (126 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1325 ; (Q Loss Avg: [Agent 0: 0.00028498, Agent 1: 0.00022616] ; E(V) Avg: [Agent 0: 0.12651835, Agent 1: 0.08420688]) [Noise stdev: 0.00911707018262805]
Episode 1806/5000 (73 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1325 ; (Q Loss Avg: [Agent 0: 0.00029022, Agent 1: 0.00019251] ; E(V) Avg: [Agent 0: 0.12602803, Agent 1: 0.08444485]) [Noise stdev: 0.009098836042262793]
Episode 1807/5000 (109 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1345 ; (Q Loss Avg: [Agent 0: 0.00028639, Agent 1: 0.00020518] ; E(V) Avg: [Agent 0: 0.12700138, Agent 1: 0.08450672]) [Noise stdev: 0.009080638370178267]
Episode 1808/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1345 ; (Q Loss Avg: [Agent 0: 0.00030391, Agent 1: 0.00024138] ; E(V) Avg: [Agent 0: 0.12616051, Agent 1: 0.08466244]) [Noise stdev: 0.00906247709343791]
Episode 1809/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1335 ; (Q Loss Avg: [Agent 0: 0.00031217, Agent 1: 0.00023060] ; E(V) Avg: [Agent 0: 0.12571299, Agent 1: 0.08543505]) [Noise stdev: 0.009044352139251033]
Episode 1810/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1335 ; (Q Loss Avg: [Agent 0: 0.00028574, Agent 1: 0.00021265] ; E(V) Avg: [Agent 0: 0.12542843, Agent 1: 0.08398368]) [Noise stdev: 0.009026263434972532]
Episode 1811/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.1325 ; (Q Loss Avg: [Agent 0: 0.00029079, Agent 1: 0.00025613] ; E(V) Avg: [Agent 0: 0.12685345, Agent 1: 0.08427689]) [Noise stdev: 0.009008210908102587]
Episode 1812/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1326 ; (Q Loss Avg: [Agent 0: 0.00032114, Agent 1: 0.00022553] ; E(V) Avg: [Agent 0: 0.12643650, Agent 1: 0.08419982]) [Noise stdev: 0.008990194486286383]
Episode 1813/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1327 ; (Q Loss Avg: [Agent 0: 0.00026201, Agent 1: 0.00023634] ; E(V) Avg: [Agent 0: 0.12630988, Agent 1: 0.08393753]) [Noise stdev: 0.00897221409731381]
Episode 1814/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1327 ; (Q Loss Avg: [Agent 0: 0.00028126, Agent 1: 0.00021165] ; E(V) Avg: [Agent 0: 0.12696561, Agent 1: 0.08511781]) [Noise stdev: 0.008954269669119183]
Episode 1815/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1317 ; (Q Loss Avg: [Agent 0: 0.00028295, Agent 1: 0.00021104] ; E(V) Avg: [Agent 0: 0.12617278, Agent 1: 0.08350911]) [Noise stdev: 0.008936361129780945]
Episode 1816/5000 (110 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1338 ; (Q Loss Avg: [Agent 0: 0.00028675, Agent 1: 0.00022625] ; E(V) Avg: [Agent 0: 0.12665147, Agent 1: 0.08484003]) [Noise stdev: 0.008918488407521383]
Episode 1817/5000 (51 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1338 ; (Q Loss Avg: [Agent 0: 0.00026414, Agent 1: 0.00020477] ; E(V) Avg: [Agent 0: 0.12733431, Agent 1: 0.08486694]) [Noise stdev: 0.008900651430706341]
Episode 1818/5000 (34 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1328 ; (Q Loss Avg: [Agent 0: 0.00026111, Agent 1: 0.00022660] ; E(V) Avg: [Agent 0: 0.12625985, Agent 1: 0.08494201]) [Noise stdev: 0.008882850127844929]
Episode 1819/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1308 ; (Q Loss Avg: [Agent 0: 0.00028101, Agent 1: 0.00020954] ; E(V) Avg: [Agent 0: 0.12561670, Agent 1: 0.08460724]) [Noise stdev: 0.008865084427589239]
Episode 1820/5000 (90 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1308 ; (Q Loss Avg: [Agent 0: 0.00026881, Agent 1: 0.00021507] ; E(V) Avg: [Agent 0: 0.12697404, Agent 1: 0.08526849]) [Noise stdev: 0.00884735425873406]
Episode 1821/5000 (132 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1328 ; (Q Loss Avg: [Agent 0: 0.00027424, Agent 1: 0.00020931] ; E(V) Avg: [Agent 0: 0.12609383, Agent 1: 0.08530538]) [Noise stdev: 0.008829659550216593]
Episode 1822/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1329 ; (Q Loss Avg: [Agent 0: 0.00027966, Agent 1: 0.00018945] ; E(V) Avg: [Agent 0: 0.12607997, Agent 1: 0.08653208]) [Noise stdev: 0.00881200023111616]
Episode 1823/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1330 ; (Q Loss Avg: [Agent 0: 0.00027172, Agent 1: 0.00020162] ; E(V) Avg: [Agent 0: 0.12584525, Agent 1: 0.08555708]) [Noise stdev: 0.008794376230653928]
Episode 1824/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1330 ; (Q Loss Avg: [Agent 0: 0.00029337, Agent 1: 0.00021684] ; E(V) Avg: [Agent 0: 0.12594165, Agent 1: 0.08570632]) [Noise stdev: 0.00877678747819262]
Episode 1825/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1330 ; (Q Loss Avg: [Agent 0: 0.00027838, Agent 1: 0.00023798] ; E(V) Avg: [Agent 0: 0.12633925, Agent 1: 0.08618264]) [Noise stdev: 0.008759233903236236]
Episode 1826/5000 (91 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1341 ; (Q Loss Avg: [Agent 0: 0.00028029, Agent 1: 0.00020502] ; E(V) Avg: [Agent 0: 0.12711024, Agent 1: 0.08540008]) [Noise stdev: 0.008741715435429764]
Episode 1827/5000 (219 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.1381 ; (Q Loss Avg: [Agent 0: 0.00027178, Agent 1: 0.00021317] ; E(V) Avg: [Agent 0: 0.12688498, Agent 1: 0.08587132]) [Noise stdev: 0.008724232004558904]
Episode 1828/5000 (167 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.1401 ; (Q Loss Avg: [Agent 0: 0.00025651, Agent 1: 0.00022915] ; E(V) Avg: [Agent 0: 0.12670200, Agent 1: 0.08548489]) [Noise stdev: 0.008706783540549786]
Episode 1829/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1401 ; (Q Loss Avg: [Agent 0: 0.00026014, Agent 1: 0.00023702] ; E(V) Avg: [Agent 0: 0.12718046, Agent 1: 0.08688924]) [Noise stdev: 0.008689369973468686]
Episode 1830/5000 (106 steps): Episode reward: 0.2900 ; Average reward (last 100): 0.1420 ; (Q Loss Avg: [Agent 0: 0.00027643, Agent 1: 0.00023187] ; E(V) Avg: [Agent 0: 0.12630327, Agent 1: 0.08531068]) [Noise stdev: 0.00867199123352175]
Episode 1831/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1421 ; (Q Loss Avg: [Agent 0: 0.00028863, Agent 1: 0.00020738] ; E(V) Avg: [Agent 0: 0.12631693, Agent 1: 0.08582762]) [Noise stdev: 0.008654647251054706]
Episode 1832/5000 (187 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.1461 ; (Q Loss Avg: [Agent 0: 0.00026629, Agent 1: 0.00023172] ; E(V) Avg: [Agent 0: 0.12664871, Agent 1: 0.08656731]) [Noise stdev: 0.008637337956552596]
Episode 1833/5000 (128 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1482 ; (Q Loss Avg: [Agent 0: 0.00024982, Agent 1: 0.00020986] ; E(V) Avg: [Agent 0: 0.12629444, Agent 1: 0.08649669]) [Noise stdev: 0.008620063280639492]
Episode 1834/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1482 ; (Q Loss Avg: [Agent 0: 0.00028252, Agent 1: 0.00024009] ; E(V) Avg: [Agent 0: 0.12517853, Agent 1: 0.08563224]) [Noise stdev: 0.008602823154078213]
Episode 1835/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.1472 ; (Q Loss Avg: [Agent 0: 0.00031351, Agent 1: 0.00023773] ; E(V) Avg: [Agent 0: 0.12476063, Agent 1: 0.08680125]) [Noise stdev: 0.008585617507770057]
Episode 1836/5000 (130 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1492 ; (Q Loss Avg: [Agent 0: 0.00027672, Agent 1: 0.00022576] ; E(V) Avg: [Agent 0: 0.12665424, Agent 1: 0.08722289]) [Noise stdev: 0.008568446272754517]
Episode 1837/5000 (104 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1512 ; (Q Loss Avg: [Agent 0: 0.00025564, Agent 1: 0.00021114] ; E(V) Avg: [Agent 0: 0.12715690, Agent 1: 0.08672660]) [Noise stdev: 0.008551309380209008]
Episode 1838/5000 (90 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1522 ; (Q Loss Avg: [Agent 0: 0.00025583, Agent 1: 0.00021777] ; E(V) Avg: [Agent 0: 0.12666512, Agent 1: 0.08730940]) [Noise stdev: 0.00853420676144859]
Episode 1839/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1512 ; (Q Loss Avg: [Agent 0: 0.00026303, Agent 1: 0.00022469] ; E(V) Avg: [Agent 0: 0.12668882, Agent 1: 0.08659294]) [Noise stdev: 0.008517138347925694]
Episode 1840/5000 (224 steps): Episode reward: 0.6000 ; Average reward (last 100): 0.1552 ; (Q Loss Avg: [Agent 0: 0.00027431, Agent 1: 0.00020788] ; E(V) Avg: [Agent 0: 0.12693618, Agent 1: 0.08693827]) [Noise stdev: 0.008500104071229843]
Episode 1841/5000 (170 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.1582 ; (Q Loss Avg: [Agent 0: 0.00025969, Agent 1: 0.00023822] ; E(V) Avg: [Agent 0: 0.12704147, Agent 1: 0.08673003]) [Noise stdev: 0.008483103863087383]
Episode 1842/5000 (206 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.1623 ; (Q Loss Avg: [Agent 0: 0.00028124, Agent 1: 0.00021423] ; E(V) Avg: [Agent 0: 0.12680633, Agent 1: 0.08681574]) [Noise stdev: 0.008466137655361208]
Episode 1843/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.1613 ; (Q Loss Avg: [Agent 0: 0.00025908, Agent 1: 0.00021686] ; E(V) Avg: [Agent 0: 0.12711349, Agent 1: 0.08765633]) [Noise stdev: 0.008449205380050485]
Episode 1844/5000 (90 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1623 ; (Q Loss Avg: [Agent 0: 0.00028232, Agent 1: 0.00021815] ; E(V) Avg: [Agent 0: 0.12717154, Agent 1: 0.08697360]) [Noise stdev: 0.008432306969290385]
Episode 1845/5000 (109 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1633 ; (Q Loss Avg: [Agent 0: 0.00025886, Agent 1: 0.00022469] ; E(V) Avg: [Agent 0: 0.12756654, Agent 1: 0.08703246]) [Noise stdev: 0.008415442355351804]
Episode 1846/5000 (72 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1643 ; (Q Loss Avg: [Agent 0: 0.00024645, Agent 1: 0.00020559] ; E(V) Avg: [Agent 0: 0.12787104, Agent 1: 0.08711297]) [Noise stdev: 0.008398611470641101]
Episode 1847/5000 (106 steps): Episode reward: 0.2900 ; Average reward (last 100): 0.1652 ; (Q Loss Avg: [Agent 0: 0.00024937, Agent 1: 0.00021694] ; E(V) Avg: [Agent 0: 0.12774997, Agent 1: 0.08692309]) [Noise stdev: 0.008381814247699819]
Episode 1848/5000 (336 steps): Episode reward: 0.8900 ; Average reward (last 100): 0.1732 ; (Q Loss Avg: [Agent 0: 0.00025127, Agent 1: 0.00020818] ; E(V) Avg: [Agent 0: 0.12776219, Agent 1: 0.08670555]) [Noise stdev: 0.00836505061920442]
Episode 1849/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1722 ; (Q Loss Avg: [Agent 0: 0.00024378, Agent 1: 0.00022479] ; E(V) Avg: [Agent 0: 0.12840686, Agent 1: 0.08737780]) [Noise stdev: 0.00834832051796601]
Episode 1850/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1722 ; (Q Loss Avg: [Agent 0: 0.00024159, Agent 1: 0.00019811] ; E(V) Avg: [Agent 0: 0.12787499, Agent 1: 0.08843048]) [Noise stdev: 0.008331623876930077]
Episode 1851/5000 (87 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1723 ; (Q Loss Avg: [Agent 0: 0.00025787, Agent 1: 0.00021402] ; E(V) Avg: [Agent 0: 0.12765186, Agent 1: 0.08675754]) [Noise stdev: 0.008314960629176216]
Episode 1852/5000 (147 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.1733 ; (Q Loss Avg: [Agent 0: 0.00025095, Agent 1: 0.00021316] ; E(V) Avg: [Agent 0: 0.12785058, Agent 1: 0.08727847]) [Noise stdev: 0.008298330707917863]
Episode 1853/5000 (106 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1743 ; (Q Loss Avg: [Agent 0: 0.00024906, Agent 1: 0.00021475] ; E(V) Avg: [Agent 0: 0.12738564, Agent 1: 0.08699286]) [Noise stdev: 0.008281734046502027]
Episode 1854/5000 (108 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1763 ; (Q Loss Avg: [Agent 0: 0.00025991, Agent 1: 0.00021108] ; E(V) Avg: [Agent 0: 0.12750875, Agent 1: 0.08712468]) [Noise stdev: 0.008265170578409023]
Episode 1855/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1773 ; (Q Loss Avg: [Agent 0: 0.00025056, Agent 1: 0.00020732] ; E(V) Avg: [Agent 0: 0.12844822, Agent 1: 0.08668433]) [Noise stdev: 0.008248640237252204]
Episode 1856/5000 (108 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1783 ; (Q Loss Avg: [Agent 0: 0.00025232, Agent 1: 0.00020892] ; E(V) Avg: [Agent 0: 0.12777227, Agent 1: 0.08716932]) [Noise stdev: 0.0082321429567777]
Episode 1857/5000 (435 steps): Episode reward: 1.1000 ; Average reward (last 100): 0.1873 ; (Q Loss Avg: [Agent 0: 0.00024367, Agent 1: 0.00020573] ; E(V) Avg: [Agent 0: 0.12807900, Agent 1: 0.08734715]) [Noise stdev: 0.008215678670864144]
Episode 1858/5000 (190 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.1913 ; (Q Loss Avg: [Agent 0: 0.00024063, Agent 1: 0.00021383] ; E(V) Avg: [Agent 0: 0.12838697, Agent 1: 0.08716937]) [Noise stdev: 0.008199247313522416]
Episode 1859/5000 (90 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1923 ; (Q Loss Avg: [Agent 0: 0.00023909, Agent 1: 0.00020470] ; E(V) Avg: [Agent 0: 0.12860877, Agent 1: 0.08765793]) [Noise stdev: 0.008182848818895371]
Episode 1860/5000 (165 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.1953 ; (Q Loss Avg: [Agent 0: 0.00024869, Agent 1: 0.00020853] ; E(V) Avg: [Agent 0: 0.12787651, Agent 1: 0.08693733]) [Noise stdev: 0.00816648312125758]
Episode 1861/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1953 ; (Q Loss Avg: [Agent 0: 0.00022695, Agent 1: 0.00020798] ; E(V) Avg: [Agent 0: 0.12860388, Agent 1: 0.08756390]) [Noise stdev: 0.008150150155015065]
Episode 1862/5000 (128 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1973 ; (Q Loss Avg: [Agent 0: 0.00024635, Agent 1: 0.00020327] ; E(V) Avg: [Agent 0: 0.12834979, Agent 1: 0.08714293]) [Noise stdev: 0.008133849854705034]
Episode 1863/5000 (114 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.1993 ; (Q Loss Avg: [Agent 0: 0.00024446, Agent 1: 0.00019856] ; E(V) Avg: [Agent 0: 0.12850673, Agent 1: 0.08786423]) [Noise stdev: 0.008117582154995624]
Episode 1864/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.1993 ; (Q Loss Avg: [Agent 0: 0.00024142, Agent 1: 0.00019306] ; E(V) Avg: [Agent 0: 0.12752807, Agent 1: 0.08694650]) [Noise stdev: 0.008101346990685633]
Episode 1865/5000 (90 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.1994 ; (Q Loss Avg: [Agent 0: 0.00024472, Agent 1: 0.00020516] ; E(V) Avg: [Agent 0: 0.12903789, Agent 1: 0.08818964]) [Noise stdev: 0.008085144296704263]
Episode 1866/5000 (128 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.2014 ; (Q Loss Avg: [Agent 0: 0.00025616, Agent 1: 0.00022395] ; E(V) Avg: [Agent 0: 0.12850302, Agent 1: 0.08842064]) [Noise stdev: 0.008068974008110854]
Episode 1867/5000 (147 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2044 ; (Q Loss Avg: [Agent 0: 0.00024489, Agent 1: 0.00021412] ; E(V) Avg: [Agent 0: 0.12794865, Agent 1: 0.08807684]) [Noise stdev: 0.008052836060094631]
Episode 1868/5000 (259 steps): Episode reward: 0.7000 ; Average reward (last 100): 0.2104 ; (Q Loss Avg: [Agent 0: 0.00024010, Agent 1: 0.00020931] ; E(V) Avg: [Agent 0: 0.12839067, Agent 1: 0.08855014]) [Noise stdev: 0.008036730387974441]
Episode 1869/5000 (88 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2114 ; (Q Loss Avg: [Agent 0: 0.00022969, Agent 1: 0.00020452] ; E(V) Avg: [Agent 0: 0.12930811, Agent 1: 0.08860358]) [Noise stdev: 0.008020656927198493]
Episode 1870/5000 (188 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.2154 ; (Q Loss Avg: [Agent 0: 0.00024045, Agent 1: 0.00020618] ; E(V) Avg: [Agent 0: 0.12909842, Agent 1: 0.08870500]) [Noise stdev: 0.008004615613344096]
Episode 1871/5000 (165 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2184 ; (Q Loss Avg: [Agent 0: 0.00024402, Agent 1: 0.00020158] ; E(V) Avg: [Agent 0: 0.12900270, Agent 1: 0.08894714]) [Noise stdev: 0.007988606382117408]
Episode 1872/5000 (339 steps): Episode reward: 0.9000 ; Average reward (last 100): 0.2265 ; (Q Loss Avg: [Agent 0: 0.00023928, Agent 1: 0.00019542] ; E(V) Avg: [Agent 0: 0.12920955, Agent 1: 0.08938335]) [Noise stdev: 0.007972629169353172]
Episode 1873/5000 (110 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.2285 ; (Q Loss Avg: [Agent 0: 0.00022767, Agent 1: 0.00021159] ; E(V) Avg: [Agent 0: 0.12961687, Agent 1: 0.08998069]) [Noise stdev: 0.007956683911014466]
Episode 1874/5000 (166 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2315 ; (Q Loss Avg: [Agent 0: 0.00024477, Agent 1: 0.00019610] ; E(V) Avg: [Agent 0: 0.12942880, Agent 1: 0.08967212]) [Noise stdev: 0.007940770543192437]
Episode 1875/5000 (146 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2336 ; (Q Loss Avg: [Agent 0: 0.00022643, Agent 1: 0.00019254] ; E(V) Avg: [Agent 0: 0.12945647, Agent 1: 0.08948744]) [Noise stdev: 0.007924889002106052]
Episode 1876/5000 (182 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2356 ; (Q Loss Avg: [Agent 0: 0.00024157, Agent 1: 0.00019451] ; E(V) Avg: [Agent 0: 0.12945996, Agent 1: 0.08987633]) [Noise stdev: 0.007909039224101841]
Episode 1877/5000 (184 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.2376 ; (Q Loss Avg: [Agent 0: 0.00024896, Agent 1: 0.00019797] ; E(V) Avg: [Agent 0: 0.12977492, Agent 1: 0.08953471]) [Noise stdev: 0.007893221145653637]
Episode 1878/5000 (360 steps): Episode reward: 0.9000 ; Average reward (last 100): 0.2436 ; (Q Loss Avg: [Agent 0: 0.00023510, Agent 1: 0.00019088] ; E(V) Avg: [Agent 0: 0.12962566, Agent 1: 0.09023912]) [Noise stdev: 0.00787743470336233]
Episode 1879/5000 (36 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2436 ; (Q Loss Avg: [Agent 0: 0.00024043, Agent 1: 0.00020921] ; E(V) Avg: [Agent 0: 0.12999423, Agent 1: 0.09058427]) [Noise stdev: 0.007861679833955604]
Episode 1880/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2436 ; (Q Loss Avg: [Agent 0: 0.00021788, Agent 1: 0.00018772] ; E(V) Avg: [Agent 0: 0.12856856, Agent 1: 0.09002845]) [Noise stdev: 0.007845956474287694]
Episode 1881/5000 (243 steps): Episode reward: 0.6000 ; Average reward (last 100): 0.2486 ; (Q Loss Avg: [Agent 0: 0.00021935, Agent 1: 0.00019022] ; E(V) Avg: [Agent 0: 0.13006146, Agent 1: 0.09023808]) [Noise stdev: 0.007830264561339118]
Episode 1882/5000 (225 steps): Episode reward: 0.6000 ; Average reward (last 100): 0.2536 ; (Q Loss Avg: [Agent 0: 0.00022511, Agent 1: 0.00018117] ; E(V) Avg: [Agent 0: 0.13033083, Agent 1: 0.09026409]) [Noise stdev: 0.007814604032216439]
Episode 1883/5000 (90 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2546 ; (Q Loss Avg: [Agent 0: 0.00023047, Agent 1: 0.00018100] ; E(V) Avg: [Agent 0: 0.12993733, Agent 1: 0.09094096]) [Noise stdev: 0.007798974824152006]
Episode 1884/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2516 ; (Q Loss Avg: [Agent 0: 0.00022702, Agent 1: 0.00018384] ; E(V) Avg: [Agent 0: 0.13043354, Agent 1: 0.09057863]) [Noise stdev: 0.007783376874503702]
Episode 1885/5000 (107 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.2527 ; (Q Loss Avg: [Agent 0: 0.00022398, Agent 1: 0.00018077] ; E(V) Avg: [Agent 0: 0.13053968, Agent 1: 0.09089962]) [Noise stdev: 0.007767810120754694]
Episode 1886/5000 (94 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2527 ; (Q Loss Avg: [Agent 0: 0.00023480, Agent 1: 0.00017009] ; E(V) Avg: [Agent 0: 0.12984179, Agent 1: 0.09071433]) [Noise stdev: 0.007752274500513185]
Episode 1887/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2517 ; (Q Loss Avg: [Agent 0: 0.00021943, Agent 1: 0.00018663] ; E(V) Avg: [Agent 0: 0.12940786, Agent 1: 0.09063793]) [Noise stdev: 0.007736769951512158]
Episode 1888/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2527 ; (Q Loss Avg: [Agent 0: 0.00023140, Agent 1: 0.00019983] ; E(V) Avg: [Agent 0: 0.12928696, Agent 1: 0.09039399]) [Noise stdev: 0.007721296411609134]
Episode 1889/5000 (113 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.2547 ; (Q Loss Avg: [Agent 0: 0.00022205, Agent 1: 0.00018691] ; E(V) Avg: [Agent 0: 0.12989446, Agent 1: 0.09069184]) [Noise stdev: 0.007705853818785915]
Episode 1890/5000 (129 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.2567 ; (Q Loss Avg: [Agent 0: 0.00024113, Agent 1: 0.00019305] ; E(V) Avg: [Agent 0: 0.12982235, Agent 1: 0.09049719]) [Noise stdev: 0.007690442111148344]
Episode 1891/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2567 ; (Q Loss Avg: [Agent 0: 0.00021122, Agent 1: 0.00020160] ; E(V) Avg: [Agent 0: 0.13029932, Agent 1: 0.09113742]) [Noise stdev: 0.007675061226926047]
Episode 1892/5000 (183 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.2607 ; (Q Loss Avg: [Agent 0: 0.00023026, Agent 1: 0.00018723] ; E(V) Avg: [Agent 0: 0.13022812, Agent 1: 0.09110123]) [Noise stdev: 0.007659711104472194]
Episode 1893/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2607 ; (Q Loss Avg: [Agent 0: 0.00023602, Agent 1: 0.00017343] ; E(V) Avg: [Agent 0: 0.12961737, Agent 1: 0.09101896]) [Noise stdev: 0.00764439168226325]
Episode 1894/5000 (107 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.2627 ; (Q Loss Avg: [Agent 0: 0.00024206, Agent 1: 0.00017433] ; E(V) Avg: [Agent 0: 0.12963183, Agent 1: 0.09071227]) [Noise stdev: 0.007629102898898723]
Episode 1895/5000 (34 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2627 ; (Q Loss Avg: [Agent 0: 0.00023752, Agent 1: 0.00019152] ; E(V) Avg: [Agent 0: 0.13068376, Agent 1: 0.09115070]) [Noise stdev: 0.007613844693100925]
Episode 1896/5000 (107 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.2647 ; (Q Loss Avg: [Agent 0: 0.00021038, Agent 1: 0.00020850] ; E(V) Avg: [Agent 0: 0.13042646, Agent 1: 0.09174171]) [Noise stdev: 0.007598617003714723]
Episode 1897/5000 (147 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2667 ; (Q Loss Avg: [Agent 0: 0.00021798, Agent 1: 0.00018625] ; E(V) Avg: [Agent 0: 0.12972252, Agent 1: 0.09117765]) [Noise stdev: 0.007583419769707294]
Episode 1898/5000 (164 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2697 ; (Q Loss Avg: [Agent 0: 0.00022606, Agent 1: 0.00018167] ; E(V) Avg: [Agent 0: 0.12998305, Agent 1: 0.09193410]) [Noise stdev: 0.0075682529301678795]
Episode 1899/5000 (147 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2727 ; (Q Loss Avg: [Agent 0: 0.00022730, Agent 1: 0.00018691] ; E(V) Avg: [Agent 0: 0.13010921, Agent 1: 0.09202373]) [Noise stdev: 0.0075531164243075434]
Episode 1900/5000 (164 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2757 ; (Q Loss Avg: [Agent 0: 0.00022293, Agent 1: 0.00018978] ; E(V) Avg: [Agent 0: 0.12925429, Agent 1: 0.09176104]) [Noise stdev: 0.007538010191458928]
Episode 1901/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2767 ; (Q Loss Avg: [Agent 0: 0.00021553, Agent 1: 0.00017559] ; E(V) Avg: [Agent 0: 0.12883415, Agent 1: 0.09050386]) [Noise stdev: 0.00752293417107601]
Episode 1902/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2757 ; (Q Loss Avg: [Agent 0: 0.00022992, Agent 1: 0.00019987] ; E(V) Avg: [Agent 0: 0.12874598, Agent 1: 0.09176690]) [Noise stdev: 0.007507888302733859]
Episode 1903/5000 (51 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2767 ; (Q Loss Avg: [Agent 0: 0.00022277, Agent 1: 0.00020442] ; E(V) Avg: [Agent 0: 0.12905056, Agent 1: 0.09102171]) [Noise stdev: 0.007492872526128391]
Episode 1904/5000 (14 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.2757 ; (Q Loss Avg: [Agent 0: 0.00021368, Agent 1: 0.00018967] ; E(V) Avg: [Agent 0: 0.12960350, Agent 1: 0.09224566]) [Noise stdev: 0.007477886781076134]
Episode 1905/5000 (86 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2747 ; (Q Loss Avg: [Agent 0: 0.00023597, Agent 1: 0.00019931] ; E(V) Avg: [Agent 0: 0.13001223, Agent 1: 0.09230475]) [Noise stdev: 0.0074629310075139815]
Episode 1906/5000 (225 steps): Episode reward: 0.6000 ; Average reward (last 100): 0.2787 ; (Q Loss Avg: [Agent 0: 0.00022024, Agent 1: 0.00019111] ; E(V) Avg: [Agent 0: 0.12982318, Agent 1: 0.09202444]) [Noise stdev: 0.007448005145498954]
Episode 1907/5000 (349 steps): Episode reward: 0.9000 ; Average reward (last 100): 0.2847 ; (Q Loss Avg: [Agent 0: 0.00021422, Agent 1: 0.00017685] ; E(V) Avg: [Agent 0: 0.12965714, Agent 1: 0.09201683]) [Noise stdev: 0.007433109135207956]
Episode 1908/5000 (146 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2877 ; (Q Loss Avg: [Agent 0: 0.00020724, Agent 1: 0.00017783] ; E(V) Avg: [Agent 0: 0.13005868, Agent 1: 0.09172735]) [Noise stdev: 0.00741824291693754]
Episode 1909/5000 (146 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2907 ; (Q Loss Avg: [Agent 0: 0.00021724, Agent 1: 0.00018478] ; E(V) Avg: [Agent 0: 0.12965975, Agent 1: 0.09233716]) [Noise stdev: 0.007403406431103665]
Episode 1910/5000 (139 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2937 ; (Q Loss Avg: [Agent 0: 0.00023001, Agent 1: 0.00018190] ; E(V) Avg: [Agent 0: 0.13088640, Agent 1: 0.09239864]) [Noise stdev: 0.007388599618241458]
Episode 1911/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2957 ; (Q Loss Avg: [Agent 0: 0.00022127, Agent 1: 0.00018727] ; E(V) Avg: [Agent 0: 0.12978935, Agent 1: 0.09255448]) [Noise stdev: 0.007373822419004975]
Episode 1912/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2957 ; (Q Loss Avg: [Agent 0: 0.00023085, Agent 1: 0.00017976] ; E(V) Avg: [Agent 0: 0.13024688, Agent 1: 0.09320817]) [Noise stdev: 0.007359074774166965]
Episode 1913/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2957 ; (Q Loss Avg: [Agent 0: 0.00023054, Agent 1: 0.00019409] ; E(V) Avg: [Agent 0: 0.12985048, Agent 1: 0.09308489]) [Noise stdev: 0.007344356624618631]
Episode 1914/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2957 ; (Q Loss Avg: [Agent 0: 0.00022243, Agent 1: 0.00018909] ; E(V) Avg: [Agent 0: 0.13057973, Agent 1: 0.09308841]) [Noise stdev: 0.007329667911369394]
Episode 1915/5000 (105 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.2977 ; (Q Loss Avg: [Agent 0: 0.00021648, Agent 1: 0.00018412] ; E(V) Avg: [Agent 0: 0.13029948, Agent 1: 0.09305898]) [Noise stdev: 0.007315008575546655]
Episode 1916/5000 (151 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2987 ; (Q Loss Avg: [Agent 0: 0.00022688, Agent 1: 0.00017003] ; E(V) Avg: [Agent 0: 0.12998677, Agent 1: 0.09236835]) [Noise stdev: 0.007300378558395562]
Episode 1917/5000 (146 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.3017 ; (Q Loss Avg: [Agent 0: 0.00020309, Agent 1: 0.00018085] ; E(V) Avg: [Agent 0: 0.13056007, Agent 1: 0.09243738]) [Noise stdev: 0.007285777801278771]
Episode 1918/5000 (86 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.3027 ; (Q Loss Avg: [Agent 0: 0.00022299, Agent 1: 0.00018035] ; E(V) Avg: [Agent 0: 0.12962046, Agent 1: 0.09277928]) [Noise stdev: 0.007271206245676213]
Episode 1919/5000 (68 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.3037 ; (Q Loss Avg: [Agent 0: 0.00022574, Agent 1: 0.00018228] ; E(V) Avg: [Agent 0: 0.12888195, Agent 1: 0.09296589]) [Noise stdev: 0.007256663833184861]
Episode 1920/5000 (106 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.3047 ; (Q Loss Avg: [Agent 0: 0.00021506, Agent 1: 0.00017916] ; E(V) Avg: [Agent 0: 0.12973889, Agent 1: 0.09280158]) [Noise stdev: 0.007242150505518492]
Episode 1921/5000 (107 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.3047 ; (Q Loss Avg: [Agent 0: 0.00020840, Agent 1: 0.00018955] ; E(V) Avg: [Agent 0: 0.12994837, Agent 1: 0.09337988]) [Noise stdev: 0.007227666204507455]
Episode 1922/5000 (94 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.3057 ; (Q Loss Avg: [Agent 0: 0.00019822, Agent 1: 0.00017790] ; E(V) Avg: [Agent 0: 0.12936545, Agent 1: 0.09272019]) [Noise stdev: 0.00721321087209844]
Episode 1923/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3057 ; (Q Loss Avg: [Agent 0: 0.00021041, Agent 1: 0.00017144] ; E(V) Avg: [Agent 0: 0.12991052, Agent 1: 0.09237831]) [Noise stdev: 0.007198784450354243]
Episode 1924/5000 (182 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.3097 ; (Q Loss Avg: [Agent 0: 0.00020840, Agent 1: 0.00017796] ; E(V) Avg: [Agent 0: 0.12994975, Agent 1: 0.09299915]) [Noise stdev: 0.007184386881453534]
Episode 1925/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3097 ; (Q Loss Avg: [Agent 0: 0.00019999, Agent 1: 0.00017811] ; E(V) Avg: [Agent 0: 0.12961286, Agent 1: 0.09312449]) [Noise stdev: 0.0071700181076906275]
Episode 1926/5000 (106 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.3107 ; (Q Loss Avg: [Agent 0: 0.00023245, Agent 1: 0.00017454] ; E(V) Avg: [Agent 0: 0.13021800, Agent 1: 0.09369783]) [Noise stdev: 0.007155678071475246]
Episode 1927/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3067 ; (Q Loss Avg: [Agent 0: 0.00019452, Agent 1: 0.00017412] ; E(V) Avg: [Agent 0: 0.13064671, Agent 1: 0.09272503]) [Noise stdev: 0.007141366715332296]
Episode 1928/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3037 ; (Q Loss Avg: [Agent 0: 0.00019276, Agent 1: 0.00017974] ; E(V) Avg: [Agent 0: 0.12999197, Agent 1: 0.09321786]) [Noise stdev: 0.007127083981901631]
Episode 1929/5000 (220 steps): Episode reward: 0.6000 ; Average reward (last 100): 0.3087 ; (Q Loss Avg: [Agent 0: 0.00020104, Agent 1: 0.00017253] ; E(V) Avg: [Agent 0: 0.12974709, Agent 1: 0.09325972]) [Noise stdev: 0.007112829813937828]
Episode 1930/5000 (105 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.3088 ; (Q Loss Avg: [Agent 0: 0.00019278, Agent 1: 0.00017192] ; E(V) Avg: [Agent 0: 0.12973688, Agent 1: 0.09359512]) [Noise stdev: 0.0070986041543099525]
Episode 1931/5000 (106 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.3108 ; (Q Loss Avg: [Agent 0: 0.00020194, Agent 1: 0.00017167] ; E(V) Avg: [Agent 0: 0.13027006, Agent 1: 0.09382664]) [Noise stdev: 0.0070844069460013325]
Episode 1932/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3068 ; (Q Loss Avg: [Agent 0: 0.00020247, Agent 1: 0.00017568] ; E(V) Avg: [Agent 0: 0.12977726, Agent 1: 0.09358431]) [Noise stdev: 0.00707023813210933]
Episode 1933/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3048 ; (Q Loss Avg: [Agent 0: 0.00019593, Agent 1: 0.00015227] ; E(V) Avg: [Agent 0: 0.13002042, Agent 1: 0.09366507]) [Noise stdev: 0.007056097655845112]
Episode 1934/5000 (49 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3048 ; (Q Loss Avg: [Agent 0: 0.00020426, Agent 1: 0.00018081] ; E(V) Avg: [Agent 0: 0.13078650, Agent 1: 0.09480751]) [Noise stdev: 0.007041985460533421]
Episode 1935/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3058 ; (Q Loss Avg: [Agent 0: 0.00019588, Agent 1: 0.00018350] ; E(V) Avg: [Agent 0: 0.13024026, Agent 1: 0.09335745]) [Noise stdev: 0.007027901489612355]
Episode 1936/5000 (162 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.3068 ; (Q Loss Avg: [Agent 0: 0.00021231, Agent 1: 0.00017619] ; E(V) Avg: [Agent 0: 0.13046595, Agent 1: 0.09342801]) [Noise stdev: 0.00701384568663313]
Episode 1937/5000 (184 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.3088 ; (Q Loss Avg: [Agent 0: 0.00019964, Agent 1: 0.00018047] ; E(V) Avg: [Agent 0: 0.12997121, Agent 1: 0.09471093]) [Noise stdev: 0.006999817995259863]
Episode 1938/5000 (182 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.3118 ; (Q Loss Avg: [Agent 0: 0.00019854, Agent 1: 0.00017313] ; E(V) Avg: [Agent 0: 0.13002847, Agent 1: 0.09420729]) [Noise stdev: 0.006985818359269344]
Episode 1939/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3118 ; (Q Loss Avg: [Agent 0: 0.00020672, Agent 1: 0.00016518] ; E(V) Avg: [Agent 0: 0.13108311, Agent 1: 0.09492666]) [Noise stdev: 0.006971846722550805]
Episode 1940/5000 (89 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.3078 ; (Q Loss Avg: [Agent 0: 0.00020611, Agent 1: 0.00017025] ; E(V) Avg: [Agent 0: 0.13054059, Agent 1: 0.09453767]) [Noise stdev: 0.006957903029105704]
Episode 1941/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3048 ; (Q Loss Avg: [Agent 0: 0.00021124, Agent 1: 0.00017594] ; E(V) Avg: [Agent 0: 0.13041320, Agent 1: 0.09526162]) [Noise stdev: 0.006943987223047492]
Episode 1942/5000 (107 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.3028 ; (Q Loss Avg: [Agent 0: 0.00020600, Agent 1: 0.00017924] ; E(V) Avg: [Agent 0: 0.13079511, Agent 1: 0.09433683]) [Noise stdev: 0.006930099248601398]
Episode 1943/5000 (183 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.3078 ; (Q Loss Avg: [Agent 0: 0.00020155, Agent 1: 0.00016138] ; E(V) Avg: [Agent 0: 0.13031985, Agent 1: 0.09437491]) [Noise stdev: 0.0069162390501041945]
Episode 1944/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3068 ; (Q Loss Avg: [Agent 0: 0.00019089, Agent 1: 0.00016517] ; E(V) Avg: [Agent 0: 0.12990361, Agent 1: 0.09449527]) [Noise stdev: 0.006902406572003986]
Episode 1945/5000 (72 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.3058 ; (Q Loss Avg: [Agent 0: 0.00019469, Agent 1: 0.00017035] ; E(V) Avg: [Agent 0: 0.12966280, Agent 1: 0.09526278]) [Noise stdev: 0.006888601758859978]
Episode 1946/5000 (147 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.3078 ; (Q Loss Avg: [Agent 0: 0.00020363, Agent 1: 0.00017084] ; E(V) Avg: [Agent 0: 0.12995888, Agent 1: 0.09517087]) [Noise stdev: 0.006874824555342258]
Episode 1947/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3059 ; (Q Loss Avg: [Agent 0: 0.00019925, Agent 1: 0.00017148] ; E(V) Avg: [Agent 0: 0.12969325, Agent 1: 0.09492256]) [Noise stdev: 0.006861074906231573]
Episode 1948/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2980 ; (Q Loss Avg: [Agent 0: 0.00019772, Agent 1: 0.00017376] ; E(V) Avg: [Agent 0: 0.12987120, Agent 1: 0.09533212]) [Noise stdev: 0.00684735275641911]
Episode 1949/5000 (109 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.3000 ; (Q Loss Avg: [Agent 0: 0.00020817, Agent 1: 0.00017655] ; E(V) Avg: [Agent 0: 0.13014141, Agent 1: 0.09509893]) [Noise stdev: 0.006833658050906272]
Episode 1950/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3000 ; (Q Loss Avg: [Agent 0: 0.00019082, Agent 1: 0.00016971] ; E(V) Avg: [Agent 0: 0.13033615, Agent 1: 0.09537061]) [Noise stdev: 0.006819990734804459]
Episode 1951/5000 (145 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.3020 ; (Q Loss Avg: [Agent 0: 0.00020734, Agent 1: 0.00018220] ; E(V) Avg: [Agent 0: 0.12988242, Agent 1: 0.09577628]) [Noise stdev: 0.00680635075333485]
Episode 1952/5000 (34 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2990 ; (Q Loss Avg: [Agent 0: 0.00016899, Agent 1: 0.00016325] ; E(V) Avg: [Agent 0: 0.13099204, Agent 1: 0.09618511]) [Noise stdev: 0.00679273805182818]
Episode 1953/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2970 ; (Q Loss Avg: [Agent 0: 0.00018965, Agent 1: 0.00018517] ; E(V) Avg: [Agent 0: 0.12955933, Agent 1: 0.09592242]) [Noise stdev: 0.006779152575724524]
Episode 1954/5000 (88 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2960 ; (Q Loss Avg: [Agent 0: 0.00018597, Agent 1: 0.00017429] ; E(V) Avg: [Agent 0: 0.12977528, Agent 1: 0.09555590]) [Noise stdev: 0.006765594270573075]
Episode 1955/5000 (106 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.2980 ; (Q Loss Avg: [Agent 0: 0.00020797, Agent 1: 0.00016993] ; E(V) Avg: [Agent 0: 0.12962284, Agent 1: 0.09533449]) [Noise stdev: 0.006752063082031929]
Episode 1956/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2960 ; (Q Loss Avg: [Agent 0: 0.00019997, Agent 1: 0.00018276] ; E(V) Avg: [Agent 0: 0.13146165, Agent 1: 0.09579707]) [Noise stdev: 0.006738558955867865]
Episode 1957/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2860 ; (Q Loss Avg: [Agent 0: 0.00018303, Agent 1: 0.00017617] ; E(V) Avg: [Agent 0: 0.12927233, Agent 1: 0.09526319]) [Noise stdev: 0.00672508183795613]
Episode 1958/5000 (144 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2850 ; (Q Loss Avg: [Agent 0: 0.00022513, Agent 1: 0.00017182] ; E(V) Avg: [Agent 0: 0.13074786, Agent 1: 0.09566103]) [Noise stdev: 0.006711631674280218]
Episode 1959/5000 (223 steps): Episode reward: 0.6000 ; Average reward (last 100): 0.2890 ; (Q Loss Avg: [Agent 0: 0.00020588, Agent 1: 0.00017084] ; E(V) Avg: [Agent 0: 0.13050680, Agent 1: 0.09629461]) [Noise stdev: 0.006698208410931657]
Episode 1960/5000 (145 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2890 ; (Q Loss Avg: [Agent 0: 0.00019687, Agent 1: 0.00017417] ; E(V) Avg: [Agent 0: 0.13098480, Agent 1: 0.09601263]) [Noise stdev: 0.006684811994109794]
Episode 1961/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2890 ; (Q Loss Avg: [Agent 0: 0.00019714, Agent 1: 0.00015281] ; E(V) Avg: [Agent 0: 0.13128175, Agent 1: 0.09502328]) [Noise stdev: 0.0066714423701215745]
Episode 1962/5000 (106 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.2890 ; (Q Loss Avg: [Agent 0: 0.00022283, Agent 1: 0.00018415] ; E(V) Avg: [Agent 0: 0.13001500, Agent 1: 0.09541542]) [Noise stdev: 0.006658099485381331]
Episode 1963/5000 (341 steps): Episode reward: 0.9000 ; Average reward (last 100): 0.2950 ; (Q Loss Avg: [Agent 0: 0.00020257, Agent 1: 0.00017140] ; E(V) Avg: [Agent 0: 0.13028617, Agent 1: 0.09664800]) [Noise stdev: 0.0066447832864105685]
Episode 1964/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2950 ; (Q Loss Avg: [Agent 0: 0.00019218, Agent 1: 0.00017386] ; E(V) Avg: [Agent 0: 0.13016292, Agent 1: 0.09689012]) [Noise stdev: 0.006631493719837747]
Episode 1965/5000 (88 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2950 ; (Q Loss Avg: [Agent 0: 0.00019150, Agent 1: 0.00016629] ; E(V) Avg: [Agent 0: 0.13010971, Agent 1: 0.09571984]) [Noise stdev: 0.006618230732398072]
Episode 1966/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2930 ; (Q Loss Avg: [Agent 0: 0.00021010, Agent 1: 0.00018178] ; E(V) Avg: [Agent 0: 0.13007969, Agent 1: 0.09550034]) [Noise stdev: 0.006604994270933276]
Episode 1967/5000 (90 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2910 ; (Q Loss Avg: [Agent 0: 0.00019430, Agent 1: 0.00020173] ; E(V) Avg: [Agent 0: 0.12920021, Agent 1: 0.09704906]) [Noise stdev: 0.00659178428239141]
Episode 1968/5000 (72 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.2859 ; (Q Loss Avg: [Agent 0: 0.00022403, Agent 1: 0.00018038] ; E(V) Avg: [Agent 0: 0.12910415, Agent 1: 0.09664583]) [Noise stdev: 0.006578600713826627]
Episode 1969/5000 (72 steps): Episode reward: 0.1900 ; Average reward (last 100): 0.2858 ; (Q Loss Avg: [Agent 0: 0.00021072, Agent 1: 0.00016905] ; E(V) Avg: [Agent 0: 0.13001141, Agent 1: 0.09634487]) [Noise stdev: 0.006565443512398974]
Episode 1970/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2818 ; (Q Loss Avg: [Agent 0: 0.00020596, Agent 1: 0.00019413] ; E(V) Avg: [Agent 0: 0.12984021, Agent 1: 0.09678934]) [Noise stdev: 0.006552312625374175]
Episode 1971/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2788 ; (Q Loss Avg: [Agent 0: 0.00018390, Agent 1: 0.00016802] ; E(V) Avg: [Agent 0: 0.13152947, Agent 1: 0.09627181]) [Noise stdev: 0.006539208000123427]
Episode 1972/5000 (181 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.2748 ; (Q Loss Avg: [Agent 0: 0.00020131, Agent 1: 0.00017537] ; E(V) Avg: [Agent 0: 0.13007810, Agent 1: 0.09637810]) [Noise stdev: 0.00652612958412318]
Episode 1973/5000 (184 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.2768 ; (Q Loss Avg: [Agent 0: 0.00019710, Agent 1: 0.00017789] ; E(V) Avg: [Agent 0: 0.13039650, Agent 1: 0.09667586]) [Noise stdev: 0.0065130773249549336]
Episode 1974/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2738 ; (Q Loss Avg: [Agent 0: 0.00022234, Agent 1: 0.00016272] ; E(V) Avg: [Agent 0: 0.13026755, Agent 1: 0.09740829]) [Noise stdev: 0.006500051170305023]
Episode 1975/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2708 ; (Q Loss Avg: [Agent 0: 0.00020347, Agent 1: 0.00015794] ; E(V) Avg: [Agent 0: 0.13010887, Agent 1: 0.09715848]) [Noise stdev: 0.006487051067964413]
Episode 1976/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2688 ; (Q Loss Avg: [Agent 0: 0.00019524, Agent 1: 0.00017916] ; E(V) Avg: [Agent 0: 0.12989170, Agent 1: 0.09652085]) [Noise stdev: 0.0064740769658284845]
Episode 1977/5000 (76 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2658 ; (Q Loss Avg: [Agent 0: 0.00019965, Agent 1: 0.00017509] ; E(V) Avg: [Agent 0: 0.13074235, Agent 1: 0.09724437]) [Noise stdev: 0.0064611288118968276]
Episode 1978/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2578 ; (Q Loss Avg: [Agent 0: 0.00017020, Agent 1: 0.00018653] ; E(V) Avg: [Agent 0: 0.13165172, Agent 1: 0.09731180]) [Noise stdev: 0.006448206554273034]
Episode 1979/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2578 ; (Q Loss Avg: [Agent 0: 0.00020174, Agent 1: 0.00016555] ; E(V) Avg: [Agent 0: 0.12955938, Agent 1: 0.09575157]) [Noise stdev: 0.006435310141164489]
Episode 1980/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2588 ; (Q Loss Avg: [Agent 0: 0.00019893, Agent 1: 0.00017282] ; E(V) Avg: [Agent 0: 0.13004519, Agent 1: 0.09709374]) [Noise stdev: 0.00642243952088216]
Episode 1981/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2538 ; (Q Loss Avg: [Agent 0: 0.00023029, Agent 1: 0.00016378] ; E(V) Avg: [Agent 0: 0.12950891, Agent 1: 0.09712635]) [Noise stdev: 0.006409594641840395]
Episode 1982/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2488 ; (Q Loss Avg: [Agent 0: 0.00020317, Agent 1: 0.00016629] ; E(V) Avg: [Agent 0: 0.13045006, Agent 1: 0.09712550]) [Noise stdev: 0.006396775452556715]
Episode 1983/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2478 ; (Q Loss Avg: [Agent 0: 0.00022187, Agent 1: 0.00016800] ; E(V) Avg: [Agent 0: 0.12969814, Agent 1: 0.09668961]) [Noise stdev: 0.006383981901651601]
Episode 1984/5000 (87 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2488 ; (Q Loss Avg: [Agent 0: 0.00019992, Agent 1: 0.00016681] ; E(V) Avg: [Agent 0: 0.12961873, Agent 1: 0.09672962]) [Noise stdev: 0.006371213937848298]
Episode 1985/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.2458 ; (Q Loss Avg: [Agent 0: 0.00019803, Agent 1: 0.00016769] ; E(V) Avg: [Agent 0: 0.12962289, Agent 1: 0.09553388]) [Noise stdev: 0.0063584715099726016]
Episode 1986/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2448 ; (Q Loss Avg: [Agent 0: 0.00019604, Agent 1: 0.00015973] ; E(V) Avg: [Agent 0: 0.12996448, Agent 1: 0.09745132]) [Noise stdev: 0.006345754566952656]
Episode 1987/5000 (16 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.2438 ; (Q Loss Avg: [Agent 0: 0.00018384, Agent 1: 0.00016594] ; E(V) Avg: [Agent 0: 0.12972513, Agent 1: 0.09633471]) [Noise stdev: 0.0063330630578187505]
Episode 1988/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2438 ; (Q Loss Avg: [Agent 0: 0.00020695, Agent 1: 0.00018018] ; E(V) Avg: [Agent 0: 0.12999664, Agent 1: 0.09757083]) [Noise stdev: 0.006320396931703113]
Episode 1989/5000 (51 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2418 ; (Q Loss Avg: [Agent 0: 0.00019279, Agent 1: 0.00017887] ; E(V) Avg: [Agent 0: 0.12986627, Agent 1: 0.09710228]) [Noise stdev: 0.006307756137839707]
Episode 1990/5000 (73 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2398 ; (Q Loss Avg: [Agent 0: 0.00020747, Agent 1: 0.00017991] ; E(V) Avg: [Agent 0: 0.12939415, Agent 1: 0.09784309]) [Noise stdev: 0.006295140625564028]
Episode 1991/5000 (201 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.2438 ; (Q Loss Avg: [Agent 0: 0.00019850, Agent 1: 0.00016167] ; E(V) Avg: [Agent 0: 0.12900553, Agent 1: 0.09767691]) [Noise stdev: 0.0062825503443129]
Episode 1992/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2398 ; (Q Loss Avg: [Agent 0: 0.00022147, Agent 1: 0.00017835] ; E(V) Avg: [Agent 0: 0.12955904, Agent 1: 0.09804069]) [Noise stdev: 0.006269985243624274]
Episode 1993/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2398 ; (Q Loss Avg: [Agent 0: 0.00020443, Agent 1: 0.00017828] ; E(V) Avg: [Agent 0: 0.12924093, Agent 1: 0.09687621]) [Noise stdev: 0.0062574452731370255]
Episode 1994/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2388 ; (Q Loss Avg: [Agent 0: 0.00019726, Agent 1: 0.00016141] ; E(V) Avg: [Agent 0: 0.12935257, Agent 1: 0.09768516]) [Noise stdev: 0.006244930382590752]
Episode 1995/5000 (13 steps): Episode reward: 0.0000 ; Average reward (last 100): 0.2378 ; (Q Loss Avg: [Agent 0: 0.00018757, Agent 1: 0.00017219] ; E(V) Avg: [Agent 0: 0.12957887, Agent 1: 0.09865418]) [Noise stdev: 0.00623244052182557]
Episode 1996/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2358 ; (Q Loss Avg: [Agent 0: 0.00018344, Agent 1: 0.00016689] ; E(V) Avg: [Agent 0: 0.12864568, Agent 1: 0.09740699]) [Noise stdev: 0.006219975640781919]
Episode 1997/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2328 ; (Q Loss Avg: [Agent 0: 0.00019313, Agent 1: 0.00017887] ; E(V) Avg: [Agent 0: 0.12983029, Agent 1: 0.09764729]) [Noise stdev: 0.006207535689500355]
Episode 1998/5000 (279 steps): Episode reward: 0.7000 ; Average reward (last 100): 0.2358 ; (Q Loss Avg: [Agent 0: 0.00019562, Agent 1: 0.00017397] ; E(V) Avg: [Agent 0: 0.12935823, Agent 1: 0.09802127]) [Noise stdev: 0.006195120618121355]
Episode 1999/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2328 ; (Q Loss Avg: [Agent 0: 0.00021392, Agent 1: 0.00015607] ; E(V) Avg: [Agent 0: 0.12993137, Agent 1: 0.09830791]) [Noise stdev: 0.006182730376885112]
Episode 2000/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2298 ; (Q Loss Avg: [Agent 0: 0.00019009, Agent 1: 0.00017093] ; E(V) Avg: [Agent 0: 0.12844936, Agent 1: 0.09828853]) [Noise stdev: 0.006170364916131341]
Episode 2001/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2298 ; (Q Loss Avg: [Agent 0: 0.00019345, Agent 1: 0.00019486] ; E(V) Avg: [Agent 0: 0.12948956, Agent 1: 0.09899821]) [Noise stdev: 0.006158024186299078]
Episode 2002/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2298 ; (Q Loss Avg: [Agent 0: 0.00019117, Agent 1: 0.00016986] ; E(V) Avg: [Agent 0: 0.12916334, Agent 1: 0.09841093]) [Noise stdev: 0.00614570813792648]
Episode 2003/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2298 ; (Q Loss Avg: [Agent 0: 0.00021225, Agent 1: 0.00016859] ; E(V) Avg: [Agent 0: 0.12839415, Agent 1: 0.09893322]) [Noise stdev: 0.006133416721650627]
Episode 2004/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2308 ; (Q Loss Avg: [Agent 0: 0.00019104, Agent 1: 0.00016081] ; E(V) Avg: [Agent 0: 0.12957242, Agent 1: 0.09889243]) [Noise stdev: 0.0061211498882073265]
Episode 2005/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2298 ; (Q Loss Avg: [Agent 0: 0.00020521, Agent 1: 0.00018108] ; E(V) Avg: [Agent 0: 0.12952147, Agent 1: 0.09888364]) [Noise stdev: 0.006108907588430912]
Episode 2006/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2248 ; (Q Loss Avg: [Agent 0: 0.00019488, Agent 1: 0.00015653] ; E(V) Avg: [Agent 0: 0.12923041, Agent 1: 0.09839503]) [Noise stdev: 0.00609668977325405]
Episode 2007/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2178 ; (Q Loss Avg: [Agent 0: 0.00020680, Agent 1: 0.00016889] ; E(V) Avg: [Agent 0: 0.12791402, Agent 1: 0.09861519]) [Noise stdev: 0.006084496393707542]
Episode 2008/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2158 ; (Q Loss Avg: [Agent 0: 0.00020463, Agent 1: 0.00016987] ; E(V) Avg: [Agent 0: 0.12837438, Agent 1: 0.09862152]) [Noise stdev: 0.006072327400920128]
Episode 2009/5000 (33 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2128 ; (Q Loss Avg: [Agent 0: 0.00018278, Agent 1: 0.00017591] ; E(V) Avg: [Agent 0: 0.12926177, Agent 1: 0.09897436]) [Noise stdev: 0.006060182746118288]
Episode 2010/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2098 ; (Q Loss Avg: [Agent 0: 0.00025104, Agent 1: 0.00016887] ; E(V) Avg: [Agent 0: 0.12864819, Agent 1: 0.09813285]) [Noise stdev: 0.006048062380626051]
Episode 2011/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2088 ; (Q Loss Avg: [Agent 0: 0.00020493, Agent 1: 0.00017467] ; E(V) Avg: [Agent 0: 0.12779024, Agent 1: 0.09923664]) [Noise stdev: 0.006035966255864799]
Episode 2012/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2098 ; (Q Loss Avg: [Agent 0: 0.00020773, Agent 1: 0.00018693] ; E(V) Avg: [Agent 0: 0.12897339, Agent 1: 0.09865892]) [Noise stdev: 0.006023894323353069]
Episode 2013/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2098 ; (Q Loss Avg: [Agent 0: 0.00020580, Agent 1: 0.00017238] ; E(V) Avg: [Agent 0: 0.12847788, Agent 1: 0.09861961]) [Noise stdev: 0.006011846534706363]
Episode 2014/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2108 ; (Q Loss Avg: [Agent 0: 0.00021051, Agent 1: 0.00017233] ; E(V) Avg: [Agent 0: 0.12850562, Agent 1: 0.09952561]) [Noise stdev: 0.00599982284163695]
Episode 2015/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2088 ; (Q Loss Avg: [Agent 0: 0.00018893, Agent 1: 0.00016837] ; E(V) Avg: [Agent 0: 0.12949503, Agent 1: 0.09844882]) [Noise stdev: 0.005987823195953677]
Episode 2016/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2068 ; (Q Loss Avg: [Agent 0: 0.00019185, Agent 1: 0.00017265] ; E(V) Avg: [Agent 0: 0.12912915, Agent 1: 0.09859447]) [Noise stdev: 0.00597584754956177]
Episode 2017/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2038 ; (Q Loss Avg: [Agent 0: 0.00022385, Agent 1: 0.00017839] ; E(V) Avg: [Agent 0: 0.12946937, Agent 1: 0.09909060]) [Noise stdev: 0.005963895854462646]
Episode 2018/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2028 ; (Q Loss Avg: [Agent 0: 0.00022495, Agent 1: 0.00016618] ; E(V) Avg: [Agent 0: 0.12874293, Agent 1: 0.09898803]) [Noise stdev: 0.005951968062753721]
Episode 2019/5000 (623 steps): Episode reward: 1.6000 ; Average reward (last 100): 0.2168 ; (Q Loss Avg: [Agent 0: 0.00019375, Agent 1: 0.00017449] ; E(V) Avg: [Agent 0: 0.12833411, Agent 1: 0.09900705]) [Noise stdev: 0.005940064126628213]
Episode 2020/5000 (127 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.2168 ; (Q Loss Avg: [Agent 0: 0.00018911, Agent 1: 0.00017960] ; E(V) Avg: [Agent 0: 0.12786780, Agent 1: 0.09968386]) [Noise stdev: 0.0059281839983749566]
Episode 2021/5000 (392 steps): Episode reward: 1.0000 ; Average reward (last 100): 0.2238 ; (Q Loss Avg: [Agent 0: 0.00019185, Agent 1: 0.00018379] ; E(V) Avg: [Agent 0: 0.12836681, Agent 1: 0.09981906]) [Noise stdev: 0.005916327630378207]
Episode 2022/5000 (165 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.2258 ; (Q Loss Avg: [Agent 0: 0.00019889, Agent 1: 0.00017814] ; E(V) Avg: [Agent 0: 0.12770074, Agent 1: 0.10024648]) [Noise stdev: 0.005904494975117451]
Episode 2023/5000 (318 steps): Episode reward: 0.8000 ; Average reward (last 100): 0.2328 ; (Q Loss Avg: [Agent 0: 0.00018856, Agent 1: 0.00016928] ; E(V) Avg: [Agent 0: 0.12776108, Agent 1: 0.10010492]) [Noise stdev: 0.005892685985167216]
Episode 2024/5000 (65 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2298 ; (Q Loss Avg: [Agent 0: 0.00017479, Agent 1: 0.00017863] ; E(V) Avg: [Agent 0: 0.12768788, Agent 1: 0.10068672]) [Noise stdev: 0.005880900613196882]
Episode 2025/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2298 ; (Q Loss Avg: [Agent 0: 0.00020911, Agent 1: 0.00017215] ; E(V) Avg: [Agent 0: 0.12810470, Agent 1: 0.10018433]) [Noise stdev: 0.005869138811970488]
Episode 2026/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2288 ; (Q Loss Avg: [Agent 0: 0.00018358, Agent 1: 0.00018300] ; E(V) Avg: [Agent 0: 0.12796420, Agent 1: 0.09974796]) [Noise stdev: 0.005857400534346547]
Episode 2027/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2288 ; (Q Loss Avg: [Agent 0: 0.00016965, Agent 1: 0.00017478] ; E(V) Avg: [Agent 0: 0.12632481, Agent 1: 0.09986068]) [Noise stdev: 0.005845685733277853]
Episode 2028/5000 (799 steps): Episode reward: 2.1000 ; Average reward (last 100): 0.2488 ; (Q Loss Avg: [Agent 0: 0.00019586, Agent 1: 0.00017218] ; E(V) Avg: [Agent 0: 0.12790430, Agent 1: 0.10097183]) [Noise stdev: 0.005833994361811297]
Episode 2029/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2438 ; (Q Loss Avg: [Agent 0: 0.00022308, Agent 1: 0.00017953] ; E(V) Avg: [Agent 0: 0.12713296, Agent 1: 0.10204430]) [Noise stdev: 0.0058223263730876745]
Episode 2030/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2418 ; (Q Loss Avg: [Agent 0: 0.00019701, Agent 1: 0.00019145] ; E(V) Avg: [Agent 0: 0.12756980, Agent 1: 0.10216555]) [Noise stdev: 0.005810681720341499]
Episode 2031/5000 (51 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2398 ; (Q Loss Avg: [Agent 0: 0.00019031, Agent 1: 0.00017981] ; E(V) Avg: [Agent 0: 0.12733198, Agent 1: 0.10159458]) [Noise stdev: 0.005799060356900816]
Episode 2032/5000 (433 steps): Episode reward: 1.1000 ; Average reward (last 100): 0.2498 ; (Q Loss Avg: [Agent 0: 0.00019481, Agent 1: 0.00017620] ; E(V) Avg: [Agent 0: 0.12745195, Agent 1: 0.10183917]) [Noise stdev: 0.0057874622361870146]
Episode 2033/5000 (68 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2508 ; (Q Loss Avg: [Agent 0: 0.00017693, Agent 1: 0.00018055] ; E(V) Avg: [Agent 0: 0.12788096, Agent 1: 0.10178525]) [Noise stdev: 0.005775887311714641]
Episode 2034/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2518 ; (Q Loss Avg: [Agent 0: 0.00017195, Agent 1: 0.00016676] ; E(V) Avg: [Agent 0: 0.12749665, Agent 1: 0.10186379]) [Noise stdev: 0.0057643355370912116]
Episode 2035/5000 (532 steps): Episode reward: 1.4000 ; Average reward (last 100): 0.2648 ; (Q Loss Avg: [Agent 0: 0.00018433, Agent 1: 0.00017305] ; E(V) Avg: [Agent 0: 0.12807036, Agent 1: 0.10248327]) [Noise stdev: 0.005752806866017029]
Episode 2036/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2628 ; (Q Loss Avg: [Agent 0: 0.00019330, Agent 1: 0.00017895] ; E(V) Avg: [Agent 0: 0.12787036, Agent 1: 0.10260160]) [Noise stdev: 0.005741301252284995]
Episode 2037/5000 (1001 steps): Episode reward: 2.6000 ; Average reward (last 100): 0.2838 ; (Q Loss Avg: [Agent 0: 0.00018399, Agent 1: 0.00016966] ; E(V) Avg: [Agent 0: 0.12783106, Agent 1: 0.10326149]) [Noise stdev: 0.005729818649780425]
Episode 2038/5000 (20 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2798 ; (Q Loss Avg: [Agent 0: 0.00022551, Agent 1: 0.00016617] ; E(V) Avg: [Agent 0: 0.12650378, Agent 1: 0.10504184]) [Noise stdev: 0.0057183590124808645]
Episode 2039/5000 (643 steps): Episode reward: 1.7000 ; Average reward (last 100): 0.2958 ; (Q Loss Avg: [Agent 0: 0.00018691, Agent 1: 0.00017747] ; E(V) Avg: [Agent 0: 0.12760901, Agent 1: 0.10456734]) [Noise stdev: 0.005706922294455903]
Episode 2040/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2958 ; (Q Loss Avg: [Agent 0: 0.00019600, Agent 1: 0.00017734] ; E(V) Avg: [Agent 0: 0.12778510, Agent 1: 0.10498405]) [Noise stdev: 0.005695508449866991]
Episode 2041/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2968 ; (Q Loss Avg: [Agent 0: 0.00018511, Agent 1: 0.00016055] ; E(V) Avg: [Agent 0: 0.12737299, Agent 1: 0.10534576]) [Noise stdev: 0.005684117432967257]
Episode 2042/5000 (223 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.2988 ; (Q Loss Avg: [Agent 0: 0.00018484, Agent 1: 0.00016736] ; E(V) Avg: [Agent 0: 0.12790425, Agent 1: 0.10505298]) [Noise stdev: 0.005672749198101323]
Episode 2043/5000 (51 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2948 ; (Q Loss Avg: [Agent 0: 0.00017198, Agent 1: 0.00016670] ; E(V) Avg: [Agent 0: 0.12824217, Agent 1: 0.10573682]) [Noise stdev: 0.00566140369970512]
Episode 2044/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2958 ; (Q Loss Avg: [Agent 0: 0.00018279, Agent 1: 0.00016957] ; E(V) Avg: [Agent 0: 0.12782253, Agent 1: 0.10520387]) [Noise stdev: 0.005650080892305709]
Episode 2045/5000 (89 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2958 ; (Q Loss Avg: [Agent 0: 0.00018008, Agent 1: 0.00016197] ; E(V) Avg: [Agent 0: 0.12798863, Agent 1: 0.10489922]) [Noise stdev: 0.005638780730521098]
Episode 2046/5000 (90 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2938 ; (Q Loss Avg: [Agent 0: 0.00018914, Agent 1: 0.00017325] ; E(V) Avg: [Agent 0: 0.12772913, Agent 1: 0.10481632]) [Noise stdev: 0.005627503169060056]
Episode 2047/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2938 ; (Q Loss Avg: [Agent 0: 0.00017501, Agent 1: 0.00018035] ; E(V) Avg: [Agent 0: 0.12778795, Agent 1: 0.10611642]) [Noise stdev: 0.005616248162721936]
Episode 2048/5000 (89 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2948 ; (Q Loss Avg: [Agent 0: 0.00017939, Agent 1: 0.00018293] ; E(V) Avg: [Agent 0: 0.12821579, Agent 1: 0.10578613]) [Noise stdev: 0.0056050156663964915]
Episode 2049/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2928 ; (Q Loss Avg: [Agent 0: 0.00018914, Agent 1: 0.00016309] ; E(V) Avg: [Agent 0: 0.12706173, Agent 1: 0.10474782]) [Noise stdev: 0.005593805635063698]
Episode 2050/5000 (279 steps): Episode reward: 0.7000 ; Average reward (last 100): 0.2988 ; (Q Loss Avg: [Agent 0: 0.00018409, Agent 1: 0.00016417] ; E(V) Avg: [Agent 0: 0.12786235, Agent 1: 0.10557566]) [Noise stdev: 0.0055826180237935705]
Episode 2051/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2958 ; (Q Loss Avg: [Agent 0: 0.00018403, Agent 1: 0.00017103] ; E(V) Avg: [Agent 0: 0.12689146, Agent 1: 0.10649748]) [Noise stdev: 0.005571452787745983]
Episode 2052/5000 (53 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.2958 ; (Q Loss Avg: [Agent 0: 0.00018653, Agent 1: 0.00015879] ; E(V) Avg: [Agent 0: 0.12767104, Agent 1: 0.10491498]) [Noise stdev: 0.005560309882170491]
Episode 2053/5000 (68 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.2968 ; (Q Loss Avg: [Agent 0: 0.00018212, Agent 1: 0.00016017] ; E(V) Avg: [Agent 0: 0.12779510, Agent 1: 0.10643814]) [Noise stdev: 0.00554918926240615]
Episode 2054/5000 (209 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.2998 ; (Q Loss Avg: [Agent 0: 0.00019145, Agent 1: 0.00018047] ; E(V) Avg: [Agent 0: 0.12841057, Agent 1: 0.10598740]) [Noise stdev: 0.005538090883881338]
Episode 2055/5000 (183 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.3018 ; (Q Loss Avg: [Agent 0: 0.00018327, Agent 1: 0.00017165] ; E(V) Avg: [Agent 0: 0.12827689, Agent 1: 0.10619827]) [Noise stdev: 0.005527014702113575]
Episode 2056/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3018 ; (Q Loss Avg: [Agent 0: 0.00017782, Agent 1: 0.00018702] ; E(V) Avg: [Agent 0: 0.12849173, Agent 1: 0.10675773]) [Noise stdev: 0.005515960672709348]
Episode 2057/5000 (563 steps): Episode reward: 1.5000 ; Average reward (last 100): 0.3158 ; (Q Loss Avg: [Agent 0: 0.00018956, Agent 1: 0.00017261] ; E(V) Avg: [Agent 0: 0.12837419, Agent 1: 0.10648905]) [Noise stdev: 0.005504928751363929]
Episode 2058/5000 (46 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3128 ; (Q Loss Avg: [Agent 0: 0.00018397, Agent 1: 0.00019522] ; E(V) Avg: [Agent 0: 0.12932847, Agent 1: 0.10674322]) [Noise stdev: 0.005493918893861201]
Episode 2059/5000 (354 steps): Episode reward: 0.9000 ; Average reward (last 100): 0.3158 ; (Q Loss Avg: [Agent 0: 0.00018322, Agent 1: 0.00016398] ; E(V) Avg: [Agent 0: 0.12855057, Agent 1: 0.10708847]) [Noise stdev: 0.005482931056073478]
Episode 2060/5000 (127 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.3148 ; (Q Loss Avg: [Agent 0: 0.00017426, Agent 1: 0.00017259] ; E(V) Avg: [Agent 0: 0.12819035, Agent 1: 0.10714762]) [Noise stdev: 0.005471965193961331]
Episode 2061/5000 (68 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.3158 ; (Q Loss Avg: [Agent 0: 0.00017381, Agent 1: 0.00017980] ; E(V) Avg: [Agent 0: 0.12770627, Agent 1: 0.10741371]) [Noise stdev: 0.005461021263573409]
Episode 2062/5000 (144 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.3168 ; (Q Loss Avg: [Agent 0: 0.00018481, Agent 1: 0.00017481] ; E(V) Avg: [Agent 0: 0.12800253, Agent 1: 0.10784327]) [Noise stdev: 0.005450099221046262]
Episode 2063/5000 (340 steps): Episode reward: 0.9000 ; Average reward (last 100): 0.3168 ; (Q Loss Avg: [Agent 0: 0.00017660, Agent 1: 0.00017710] ; E(V) Avg: [Agent 0: 0.12856035, Agent 1: 0.10794952]) [Noise stdev: 0.005439199022604169]
Episode 2064/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.3178 ; (Q Loss Avg: [Agent 0: 0.00017947, Agent 1: 0.00015955] ; E(V) Avg: [Agent 0: 0.12928562, Agent 1: 0.10747480]) [Noise stdev: 0.0054283206245589605]
Episode 2065/5000 (29 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3168 ; (Q Loss Avg: [Agent 0: 0.00018293, Agent 1: 0.00016424] ; E(V) Avg: [Agent 0: 0.12841612, Agent 1: 0.10790857]) [Noise stdev: 0.005417463983309842]
Episode 2066/5000 (185 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.3208 ; (Q Loss Avg: [Agent 0: 0.00017600, Agent 1: 0.00016964] ; E(V) Avg: [Agent 0: 0.12783151, Agent 1: 0.10830302]) [Noise stdev: 0.005406629055343223]
Episode 2067/5000 (167 steps): Episode reward: 0.4000 ; Average reward (last 100): 0.3228 ; (Q Loss Avg: [Agent 0: 0.00016875, Agent 1: 0.00017661] ; E(V) Avg: [Agent 0: 0.12782889, Agent 1: 0.10783186]) [Noise stdev: 0.005395815797232536]
Episode 2068/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3219 ; (Q Loss Avg: [Agent 0: 0.00015381, Agent 1: 0.00017852] ; E(V) Avg: [Agent 0: 0.12755158, Agent 1: 0.10777792]) [Noise stdev: 0.005385024165638071]
Episode 2069/5000 (107 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.3230 ; (Q Loss Avg: [Agent 0: 0.00017872, Agent 1: 0.00016151] ; E(V) Avg: [Agent 0: 0.12809617, Agent 1: 0.10783334]) [Noise stdev: 0.0053742541173067955]
Episode 2070/5000 (72 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.3240 ; (Q Loss Avg: [Agent 0: 0.00017125, Agent 1: 0.00015878] ; E(V) Avg: [Agent 0: 0.12882952, Agent 1: 0.10747601]) [Noise stdev: 0.005363505609072182]
Episode 2071/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3240 ; (Q Loss Avg: [Agent 0: 0.00019884, Agent 1: 0.00016220] ; E(V) Avg: [Agent 0: 0.12725205, Agent 1: 0.10862957]) [Noise stdev: 0.005352778597854037]
Episode 2072/5000 (55 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3200 ; (Q Loss Avg: [Agent 0: 0.00020116, Agent 1: 0.00016883] ; E(V) Avg: [Agent 0: 0.12761957, Agent 1: 0.10820028]) [Noise stdev: 0.005342073040658329]
Episode 2073/5000 (302 steps): Episode reward: 0.8000 ; Average reward (last 100): 0.3230 ; (Q Loss Avg: [Agent 0: 0.00018593, Agent 1: 0.00016561] ; E(V) Avg: [Agent 0: 0.12800025, Agent 1: 0.10833412]) [Noise stdev: 0.0053313888945770126]
Episode 2074/5000 (207 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.3270 ; (Q Loss Avg: [Agent 0: 0.00017971, Agent 1: 0.00017312] ; E(V) Avg: [Agent 0: 0.12806435, Agent 1: 0.10889044]) [Noise stdev: 0.0053207261167878585]
Episode 2075/5000 (87 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.3280 ; (Q Loss Avg: [Agent 0: 0.00016648, Agent 1: 0.00015347] ; E(V) Avg: [Agent 0: 0.12801133, Agent 1: 0.10926489]) [Noise stdev: 0.005310084664554283]
Episode 2076/5000 (129 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.3290 ; (Q Loss Avg: [Agent 0: 0.00018658, Agent 1: 0.00018181] ; E(V) Avg: [Agent 0: 0.12828844, Agent 1: 0.10877984]) [Noise stdev: 0.0052994644952251745]
Episode 2077/5000 (299 steps): Episode reward: 0.8000 ; Average reward (last 100): 0.3350 ; (Q Loss Avg: [Agent 0: 0.00018106, Agent 1: 0.00017403] ; E(V) Avg: [Agent 0: 0.12855078, Agent 1: 0.10925639]) [Noise stdev: 0.005288865566234724]
Episode 2078/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3350 ; (Q Loss Avg: [Agent 0: 0.00016708, Agent 1: 0.00017153] ; E(V) Avg: [Agent 0: 0.12849498, Agent 1: 0.10916271]) [Noise stdev: 0.005278287835102255]
Episode 2079/5000 (32 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3350 ; (Q Loss Avg: [Agent 0: 0.00017567, Agent 1: 0.00017888] ; E(V) Avg: [Agent 0: 0.12851618, Agent 1: 0.10943972]) [Noise stdev: 0.005267731259432051]
Episode 2080/5000 (127 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.3360 ; (Q Loss Avg: [Agent 0: 0.00016017, Agent 1: 0.00018081] ; E(V) Avg: [Agent 0: 0.12868969, Agent 1: 0.10969199]) [Noise stdev: 0.005257195796913187]
Episode 2081/5000 (483 steps): Episode reward: 1.2900 ; Average reward (last 100): 0.3479 ; (Q Loss Avg: [Agent 0: 0.00017569, Agent 1: 0.00017400] ; E(V) Avg: [Agent 0: 0.12863097, Agent 1: 0.10922133]) [Noise stdev: 0.00524668140531936]
Episode 2082/5000 (30 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3479 ; (Q Loss Avg: [Agent 0: 0.00020030, Agent 1: 0.00017649] ; E(V) Avg: [Agent 0: 0.12909576, Agent 1: 0.10940867]) [Noise stdev: 0.0052361880425087215]
Episode 2083/5000 (69 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.3489 ; (Q Loss Avg: [Agent 0: 0.00019535, Agent 1: 0.00016223] ; E(V) Avg: [Agent 0: 0.12847919, Agent 1: 0.10950892]) [Noise stdev: 0.005225715666423704]
Episode 2084/5000 (201 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.3519 ; (Q Loss Avg: [Agent 0: 0.00017892, Agent 1: 0.00016432] ; E(V) Avg: [Agent 0: 0.12884269, Agent 1: 0.11002265]) [Noise stdev: 0.005215264235090856]
Episode 2085/5000 (31 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.3529 ; (Q Loss Avg: [Agent 0: 0.00018601, Agent 1: 0.00016701] ; E(V) Avg: [Agent 0: 0.12858690, Agent 1: 0.11016633]) [Noise stdev: 0.005204833706620675]
Episode 2086/5000 (605 steps): Episode reward: 1.6000 ; Average reward (last 100): 0.3679 ; (Q Loss Avg: [Agent 0: 0.00018144, Agent 1: 0.00016392] ; E(V) Avg: [Agent 0: 0.12890711, Agent 1: 0.11045281]) [Noise stdev: 0.005194424039207434]
Episode 2087/5000 (852 steps): Episode reward: 2.2000 ; Average reward (last 100): 0.3899 ; (Q Loss Avg: [Agent 0: 0.00017799, Agent 1: 0.00016862] ; E(V) Avg: [Agent 0: 0.12938473, Agent 1: 0.11097267]) [Noise stdev: 0.005184035191129019]
Episode 2088/5000 (508 steps): Episode reward: 1.3000 ; Average reward (last 100): 0.4009 ; (Q Loss Avg: [Agent 0: 0.00017970, Agent 1: 0.00016637] ; E(V) Avg: [Agent 0: 0.12958012, Agent 1: 0.11163355]) [Noise stdev: 0.0051736671207467605]
Episode 2089/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.4019 ; (Q Loss Avg: [Agent 0: 0.00016337, Agent 1: 0.00016556] ; E(V) Avg: [Agent 0: 0.13024598, Agent 1: 0.11251221]) [Noise stdev: 0.005163319786505267]
Episode 2090/5000 (105 steps): Episode reward: 0.2900 ; Average reward (last 100): 0.4038 ; (Q Loss Avg: [Agent 0: 0.00017377, Agent 1: 0.00019139] ; E(V) Avg: [Agent 0: 0.13039999, Agent 1: 0.11184650]) [Noise stdev: 0.005152993146932256]
Episode 2091/5000 (281 steps): Episode reward: 0.7000 ; Average reward (last 100): 0.4058 ; (Q Loss Avg: [Agent 0: 0.00018712, Agent 1: 0.00017384] ; E(V) Avg: [Agent 0: 0.13007912, Agent 1: 0.11215782]) [Noise stdev: 0.005142687160638391]
Episode 2092/5000 (512 steps): Episode reward: 1.3000 ; Average reward (last 100): 0.4178 ; (Q Loss Avg: [Agent 0: 0.00017489, Agent 1: 0.00017174] ; E(V) Avg: [Agent 0: 0.13003899, Agent 1: 0.11203678]) [Noise stdev: 0.005132401786317115]
Episode 2093/5000 (52 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.4178 ; (Q Loss Avg: [Agent 0: 0.00016867, Agent 1: 0.00016416] ; E(V) Avg: [Agent 0: 0.13071282, Agent 1: 0.11269727]) [Noise stdev: 0.005122136982744481]
Episode 2094/5000 (319 steps): Episode reward: 0.8000 ; Average reward (last 100): 0.4238 ; (Q Loss Avg: [Agent 0: 0.00018598, Agent 1: 0.00016842] ; E(V) Avg: [Agent 0: 0.13078297, Agent 1: 0.11271290]) [Noise stdev: 0.005111892708778992]
Episode 2095/5000 (184 steps): Episode reward: 0.5000 ; Average reward (last 100): 0.4288 ; (Q Loss Avg: [Agent 0: 0.00018089, Agent 1: 0.00016826] ; E(V) Avg: [Agent 0: 0.13111837, Agent 1: 0.11300064]) [Noise stdev: 0.0051016689233614335]
Episode 2096/5000 (588 steps): Episode reward: 1.5000 ; Average reward (last 100): 0.4428 ; (Q Loss Avg: [Agent 0: 0.00017776, Agent 1: 0.00016939] ; E(V) Avg: [Agent 0: 0.13104635, Agent 1: 0.11309805]) [Noise stdev: 0.00509146558551471]
Episode 2097/5000 (449 steps): Episode reward: 1.2000 ; Average reward (last 100): 0.4538 ; (Q Loss Avg: [Agent 0: 0.00017811, Agent 1: 0.00017288] ; E(V) Avg: [Agent 0: 0.13190838, Agent 1: 0.11360407]) [Noise stdev: 0.005081282654343681]
Episode 2098/5000 (51 steps): Episode reward: 0.1000 ; Average reward (last 100): 0.4478 ; (Q Loss Avg: [Agent 0: 0.00016540, Agent 1: 0.00016059] ; E(V) Avg: [Agent 0: 0.13222970, Agent 1: 0.11372324]) [Noise stdev: 0.005071120089034994]
Episode 2099/5000 (91 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.4488 ; (Q Loss Avg: [Agent 0: 0.00016694, Agent 1: 0.00017302] ; E(V) Avg: [Agent 0: 0.13235350, Agent 1: 0.11419218]) [Noise stdev: 0.005060977848856924]
Episode 2100/5000 (128 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.4508 ; (Q Loss Avg: [Agent 0: 0.00018997, Agent 1: 0.00017809] ; E(V) Avg: [Agent 0: 0.13199734, Agent 1: 0.11403796]) [Noise stdev: 0.005050855893159211]
Episode 2101/5000 (949 steps): Episode reward: 2.5000 ; Average reward (last 100): 0.4748 ; (Q Loss Avg: [Agent 0: 0.00017039, Agent 1: 0.00016933] ; E(V) Avg: [Agent 0: 0.13258662, Agent 1: 0.11449152]) [Noise stdev: 0.005040754181372892]
Episode 2102/5000 (262 steps): Episode reward: 0.7000 ; Average reward (last 100): 0.4798 ; (Q Loss Avg: [Agent 0: 0.00017169, Agent 1: 0.00016306] ; E(V) Avg: [Agent 0: 0.13352226, Agent 1: 0.11482736]) [Noise stdev: 0.005030672673010147]
Episode 2103/5000 (70 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.4808 ; (Q Loss Avg: [Agent 0: 0.00017886, Agent 1: 0.00018831] ; E(V) Avg: [Agent 0: 0.13324034, Agent 1: 0.11463205]) [Noise stdev: 0.005020611327664126]
Episode 2104/5000 (108 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.4828 ; (Q Loss Avg: [Agent 0: 0.00017063, Agent 1: 0.00017577] ; E(V) Avg: [Agent 0: 0.13358024, Agent 1: 0.11564714]) [Noise stdev: 0.005010570105008798]
Episode 2105/5000 (107 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.4848 ; (Q Loss Avg: [Agent 0: 0.00017205, Agent 1: 0.00015958] ; E(V) Avg: [Agent 0: 0.13347088, Agent 1: 0.11545501]) [Noise stdev: 0.00500054896479878]
Episode 2106/5000 (89 steps): Episode reward: 0.2000 ; Average reward (last 100): 0.4858 ; (Q Loss Avg: [Agent 0: 0.00017220, Agent 1: 0.00016236] ; E(V) Avg: [Agent 0: 0.13377342, Agent 1: 0.11543467]) [Noise stdev: 0.004990547866869183]
Episode 2107/5000 (375 steps): Episode reward: 1.0000 ; Average reward (last 100): 0.4938 ; (Q Loss Avg: [Agent 0: 0.00017790, Agent 1: 0.00017232] ; E(V) Avg: [Agent 0: 0.13392548, Agent 1: 0.11548088]) [Noise stdev: 0.004980566771135444]
Episode 2108/5000 (108 steps): Episode reward: 0.3000 ; Average reward (last 100): 0.4948 ; (Q Loss Avg: [Agent 0: 0.00017838, Agent 1: 0.00018618] ; E(V) Avg: [Agent 0: 0.13422649, Agent 1: 0.11563022]) [Noise stdev: 0.004970605637593173]
Episode 2109/5000 (1001 steps): Episode reward: 2.6000 ; Average reward (last 100): 0.5198 ; (Q Loss Avg: [Agent 0: 0.00017391, Agent 1: 0.00016950] ; E(V) Avg: [Agent 0: 0.13459222, Agent 1: 0.11606811]) [Noise stdev: 0.004960664426317987]
Environment solved at 2109
